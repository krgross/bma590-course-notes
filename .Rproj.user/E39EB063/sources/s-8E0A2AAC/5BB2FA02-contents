# Introduction to likelihood

## A likelihood with a single parameter: Horse-kick data

First import the data.  Note that the pathname used here is specific to the file directory that was used to create this file.  The pathname that you use will likely differ.
```{r}
horse <- read.table("data/horse.txt", header = TRUE)
```
Ask for a `summary` of the data to make sure the data have been imported correctly.
```{r}
summary(horse)
```
We can also learn about the data by asking to see the first few records using the `head` command
```{r}
head(horse)
```
or we can see the last few records using the `tail` command:
```{r}
tail(horse)
```
Let's plot a histogram of the values:
```{r}
hist(horse$deaths,
     breaks = seq(from = min(horse$deaths) - 0.5, 
                  to = max(horse$deaths) + 0.5, 
                  by = 1))
```

### Calculate and plot the log-likelihood function

Create a function that calculates the log-likelihood for a value of $\lambda$:

```{r}
horse.ll <- function(lambda){
  
  ll.vals <- dpois(x = horse$deaths, lambda = lambda, log = TRUE)
  ll <- sum(ll.vals)
  
  return(ll)
}
```
We can use this function to calculate the log-likelihood for any value of $\lambda$, such as $\lambda = 1$:
```{r}
horse.ll(1)
```

Let's calculate the log-likelihood for many values of $\lambda$, in preparation for making a plot.  We'll use a loop here, and not worry about vectorization.

```{r}
# create a vector of lambda values using the 'seq'uence command
lambda.vals <- seq(from = 0.1, to = 2.0, by = 0.01)  

# create an empty vector to store the values of the log-likelihood
ll.vals <- double(length = length(lambda.vals))  

# use a loop to find the log-likelihood for each value in lambda.vals
for (i.lambda in 1:length(lambda.vals)) {
  
  ll.vals[i.lambda] <- horse.ll(lambda.vals[i.lambda])
}
```

Now plot the log-likelihood values vs.\ the values of $\lambda$:
```{r}
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "log likelihood", type = "l")
abline(v = 0.7, col = "red")
```

### Find the MLE numerically using 'optimize'

Note that Bolker's book illustrates numerical optimization using the 'optim' function.  The R documentation recommends using 'optimize' for one-dimensional optimization, and 'optim' for optimizing a function in several dimensions.  So, we will use 'optimize' here.

```{r}
horse.mle <- optimize(f = horse.ll, interval = c(0.1, 2), maximum = TRUE)
```

The 'optimize' function returns a 'list'.  A list is an R object that contains components of different types.  We can see all the components of this list by printing it.
```{r}
horse.mle 
```
So the numerically calculated MLE is $\hat{lambda} \approx 0.7$.  The 'objective' component of \texttt{horse.mle} gives the value of the log-likelihood at that point.

## Myxomatosis data

The myxomatosis data are in Bolker's library 'emdbook'.  First load the library.  If the library is not found, you will first have to download and install the library on your computer, using the Packages tab in RStudio.
```{r}
library(emdbook)
data(MyxoTiter_sum)
```
Inspect the data to make sure they have been imported correctly.
```{r}
summary(MyxoTiter_sum)
head(MyxoTiter_sum)
```
Extract the subset of the data that corresponds to the ``grade 1'' viral strain.
```{r}
myxo <- subset(MyxoTiter_sum, grade == 1)
summary(myxo)
```
Out of curiosity, let's make a scatterplot of the titer vs.\ the day
```{r}
with(myxo, plot(titer ~ day))
```
For the sake of this example, we will ignore the apparent (and unsurprising) relationship between titer and day, and instead will consider only the titer data.  We will regard these data as a random sample from a normal distribution.  For the sake of illustration, we will estimate the mean and variance of the normal distribution using the \texttt{optim} function in R.

First, we write a function to calculate the log likelihood.
```{r}
myxo.ll <- function(m, v){

  ll.vals <- dnorm(myxo$titer, mean = m, sd = sqrt(v), log = TRUE)
  ll <- sum(ll.vals)
  
  return(ll)
}
```
Note that R's function for the pdf of a normal distribution --- \texttt{dnorm} --- is parameterized by the mean and standard deviation (SD) of the normal distribution.  Although it would be just as easy to find the MLE of the standard deviation $\sigma$, for the sake of illustration, we will seek the MLE of the variance, $\sigma^2$.\footnote{It turns out that, if we write the MLE of the standard deviation as $\hat{\sigma}$ and the MLE of the variance as $\hat{\sigma}^2$, then $\hat{\sigma} = \sqrt{\hat{\sigma}^2}$.  This is an example of the {\em invarance property} of MLEs.}

We can use our function to calculate the likelihood for any choice of mean and variance.  For example, let's try $\mu = 6$ and $\sigma^2 = 1$.
```{r}
myxo.ll(m = 6, v = 1)
```
We want to maximize the likelihood using \texttt{optim}.  Unfortuantely, \texttt{optim} is a little finicky.  To use \texttt{optim}, we have to re-write our function \texttt{myxo.ll} so that the parameters to be estimated are passed to the function as a single vector.  Also, by default, \texttt{optim} performs minimization instead of maximization.  We can change this behavior when we call \texttt{optim}.  Alternatively, we can just re-define the function to return the negative log likelihood.  
```{r}
myxo.neg.ll <- function(pars){

  m <- pars[1]
  v <- pars[2]
  
  ll.vals <- dnorm(myxo$titer, mean = m, sd = sqrt(v), log = TRUE)
  ll <- sum(ll.vals)
  
  return(-ll)
}
```
Now we can use \texttt{optim}:
```{r}
myxo.mle <- optim(par = c(7, 1),  # starting values, just a ballpark guess 
                  fn  = myxo.neg.ll)
myxo.mle
```
Note that the MLE of the variance is
\[
\hat{\sigma}^2 = \frac{\sum_i (x_i - \bar{x})}{n}.
\]
Let's verify this by calculating the same quantity at the command line:
```{r}
residuals <- with(myxo, titer - mean(titer))
ss <- sum(residuals^2)
n <- length(myxo$titer)
var.mle <- ss / n
var.mle
```

Compare this to the answer given by \texttt{optim}, and to the more usual calculation of
```{r}
var.usual <- ss / (n - 1)
var.usual
var(myxo$titer)
```

Make a contour plot of the likelihood surface.
```{r}
m.vals <- seq(from = 6, to = 8, by = 0.05)
v.vals <- seq(from = 0.3, to = 2.5, by = 0.05)

ll.vals <- matrix(nrow = length(m.vals), ncol = length(v.vals))

for (i.m in 1:length(m.vals)) {
  for(i.v in 1:length(v.vals)) {
    ll.vals[i.m, i.v] <- myxo.ll(m = m.vals[i.m], v = v.vals[i.v])
  }
}

contour(x = m.vals, y = v.vals, z = ll.vals, nlevels = 100,
        xlab = "mean", ylab = "variance")

# show the MLE
points(x = myxo.mle$par[1], y = myxo.mle$par[2], col = "red")
```
