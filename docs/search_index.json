[["maximum-likelihood-estimation.html", "Computing companion for BMA / ST 590, Fall 2021 Chapter 1 Maximum likelihood estimation 1.1 A very simple example with a single observation 1.2 Horse-kick data 1.3 Myxomatosis data 1.4 Tadpole data", " Computing companion for BMA / ST 590, Fall 2021 Kevin Gross 2021-08-30 Chapter 1 Maximum likelihood estimation 1.1 A very simple example with a single observation Suppose we observe a single observation from a Poisson distribution. Suppose that observation is \\(X=2\\). We can use the dpois function to evaluate the likelihood for this single observation. For example, we can evaluate the likelihood at \\(\\lambda = 1.5\\): dpois(x = 2, lambda = 1.5) ## [1] 0.2510214 Or we could evaluate the likelihood at \\(\\lambda = 2\\) or \\(\\lambda = 2.5\\): dpois(x = 2, lambda = c(2, 2.5)) ## [1] 0.2706706 0.2565156 Now lets evaluate the likelihood at a sequence of \\(\\lambda\\) values: lambda.vals &lt;- seq(from = 0, to = 5, by = 0.01) my.lhood &lt;- dpois(x = 2, lambda = lambda.vals) plot(lambda.vals, my.lhood, xlab = expression(lambda), ylab = &quot;Likelihood&quot;, type = &quot;l&quot;) We might guess that the likelihood is maximized at \\(\\lambda = 2\\). Wed be right. plot(lambda.vals, my.lhood, xlab = expression(lambda), ylab = &quot;Likelihood&quot;, type = &quot;l&quot;) abline(v = 2, col = &quot;red&quot;) 1.2 Horse-kick data Most real data sets contain more than a single observation. Here is a data set that we can use to illustrate maximum likelihood estimation with a single parameter. Famously, Ladislaus van Bortkewitsch (1868 - 1931) published how many members of the Prussiam army were killed by horse kicks in each of 20 years, for each of 14 army corps. As a caveat, these data are often used to illustrate the Poisson distribution, as we will use them. They match the Poisson distribution more neatly than we might expect for most data sets. First import the data. Note that the path name used here is specific to the file directory that was used to create this file. The path name that you use will likely differ. horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) Ask for a summary of the data to make sure the data have been imported correctly. summary(horse) ## year corps deaths ## Min. :1875 Length:280 Min. :0.0 ## 1st Qu.:1880 Class :character 1st Qu.:0.0 ## Median :1884 Mode :character Median :0.0 ## Mean :1884 Mean :0.7 ## 3rd Qu.:1889 3rd Qu.:1.0 ## Max. :1894 Max. :4.0 We can also learn about the data by asking to see the first few records using the head command head(horse) ## year corps deaths ## 1 1875 GC 0 ## 2 1876 GC 2 ## 3 1877 GC 2 ## 4 1878 GC 1 ## 5 1879 GC 0 ## 6 1880 GC 0 or we can see the last few records using the tail command: tail(horse) ## year corps deaths ## 275 1889 C15 2 ## 276 1890 C15 2 ## 277 1891 C15 0 ## 278 1892 C15 0 ## 279 1893 C15 0 ## 280 1894 C15 0 Another useful function to keep in mind is the `str function which tells you about the [str]ucture of an R object: str(horse) ## &#39;data.frame&#39;: 280 obs. of 3 variables: ## $ year : int 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 ... ## $ corps : chr &quot;GC&quot; &quot;GC&quot; &quot;GC&quot; &quot;GC&quot; ... ## $ deaths: int 0 2 2 1 0 0 1 1 0 3 ... Lets plot a histogram of the values: hist(horse$deaths, breaks = seq(from = min(horse$deaths) - 0.5, to = max(horse$deaths) + 0.5, by = 1)) 1.2.1 Calculate and plot the log-likelihood function Create a function that calculates the log-likelihood for a value of \\(\\lambda\\): horse.ll &lt;- function(my.lambda){ ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) sum(ll.vals) } We can use this function to calculate the log-likelihood for any value of \\(\\lambda\\), such as \\(\\lambda = 1\\): horse.ll(1) ## [1] -328.2462 Lets calculate the log-likelihood for many values of \\(\\lambda\\), in preparation for making a plot. Well use a loop here, and not worry about vectorization. # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.01, to = 2.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.ll(lambda.vals[i.lambda]) } Now plot the log-likelihood values vs. the values of \\(\\lambda\\): plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;log likelihood&quot;, type = &quot;l&quot;) abline(v = 0.7, col = &quot;red&quot;) 1.2.2 Find the MLE numerically using optimize Bolkers book illustrates numerical optimization using the optim function. The R documentation recommends using optimize for one-dimensional optimization, and optim for optimizing a function in several dimensions. So, we will use optimize here. We will enclose the entire call to optimize in parentheses so that the output is dumped to the command line in addition to being stored as horse.mle. (horse.mle &lt;- optimize(f = horse.ll, interval = c(0.1, 2), maximum = TRUE)) ## $maximum ## [1] 0.7000088 ## ## $objective ## [1] -314.1545 The optimize function returns a list. A list is an R object that contains components of different types. The numerically calculated MLE is \\(\\hat{\\lambda} \\approx 0.7\\). The objective component of gives the value of the log-likelihood at that point. 1.3 Myxomatosis data The myxomatosis data are in Bolkers library emdbook. First load the library. If the library is not found, you will first have to download and install the library on your computer, using the Packages tab in RStudio. The call to data loads the particular myxomatosis data set that we want into memory. library(emdbook) data(MyxoTiter_sum) Inspect the data to make sure they have been imported correctly. summary(MyxoTiter_sum) ## grade day titer ## Min. :1.000 Min. : 2.000 Min. :1.958 ## 1st Qu.:3.000 1st Qu.: 4.000 1st Qu.:5.400 ## Median :4.000 Median : 8.000 Median :6.612 ## Mean :3.604 Mean : 9.564 Mean :6.331 ## 3rd Qu.:5.000 3rd Qu.:13.000 3rd Qu.:7.489 ## Max. :5.000 Max. :28.000 Max. :9.021 head(MyxoTiter_sum) ## grade day titer ## 1 1 2 5.207 ## 2 1 2 5.734 ## 3 1 2 6.613 ## 4 1 3 5.997 ## 5 1 3 6.612 ## 6 1 3 6.810 Extract the subset of the data that corresponds to the grade 1 viral strain. myxo &lt;- subset(MyxoTiter_sum, grade == 1) summary(myxo) ## grade day titer ## Min. :1 Min. :2.000 Min. :4.196 ## 1st Qu.:1 1st Qu.:3.500 1st Qu.:6.556 ## Median :1 Median :5.000 Median :7.112 ## Mean :1 Mean :5.037 Mean :6.924 ## 3rd Qu.:1 3rd Qu.:6.000 3rd Qu.:7.543 ## Max. :1 Max. :9.000 Max. :8.499 Out of curiosity, lets make a scatterplot of the titer vs. the day with(myxo, plot(titer ~ day)) For the sake of this example, we will ignore the apparent (and unsurprising) relationship between titer and day, and instead will consider only the titer data. We will regard these data as a random sample from a normal distribution. For the sake of illustration, we will estimate the mean and variance of the normal distribution using the optim function in R. First, we write a function to calculate the log likelihood. myxo.ll &lt;- function(m, v){ ll.vals &lt;- dnorm(myxo$titer, mean = m, sd = sqrt(v), log = TRUE) sum(ll.vals) } Note that Rs function for the pdf of a normal distribution  dnorm  is parameterized by the mean and standard deviation (SD) of the normal distribution. Although it would be just as easy to find the MLE of the standard deviation \\(\\sigma\\), for the sake of illustration, we will seek the MLE of the variance, \\(\\sigma^2\\). (It turns out that, if we write the MLE of the standard deviation as \\(\\hat{\\sigma}\\) and the MLE of the variance as \\(\\hat{\\sigma}^2\\), then \\(\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}\\). This is an example of the {} of MLEs.) We can use our function to calculate the likelihood for any choice of mean and variance. For example, lets try \\(\\mu = 6\\) and \\(\\sigma^2 = 1\\). myxo.ll(m = 6, v = 1) ## [1] -47.91229 We want to maximize the likelihood using optim. Unfortuantely, optim is a little finicky. To use optim, we have to re-write our function myxo.ll so that the parameters to be estimated are passed to the function as a single vector. Also, by default, optim performs minimization instead of maximization. We can change this behavior when we call optim. Alternatively, we can just re-define the function to return the negative log likelihood. myxo.neg.ll &lt;- function(pars){ m &lt;- pars[1] v &lt;- pars[2] ll.vals &lt;- dnorm(myxo$titer, mean = m, sd = sqrt(v), log = TRUE) -sum(ll.vals) } Now we can use optim: (myxo.mle &lt;- optim(par = c(7, 1), # starting values, just a ballpark guess fn = myxo.neg.ll)) ## $par ## [1] 6.9241029 0.8571471 ## ## $value ## [1] 36.23228 ## ## $counts ## function gradient ## 55 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Note that the MLE of the variance is \\[ \\hat{\\sigma}^2 = \\frac{\\sum_i (x_i - \\bar{x})}{n}. \\] Lets verify this by calculating the same quantity at the command line: residuals &lt;- with(myxo, titer - mean(titer)) ss &lt;- sum(residuals^2) n &lt;- length(myxo$titer) ss / n ## [1] 0.8572684 Compare this to the answer given by var, and to the more usual calculation of the variance as \\[ s^2 = \\frac{\\sum_i (x_i - \\bar{x})}{n-1}. \\] (var.usual &lt;- ss / (n - 1)) ## [1] 0.8902403 var(myxo$titer) ## [1] 0.8902403 One main take-home of this example is that when we use maximum likelihood to estimate variances for normally distributed data, the MLE is biased low. In other words, it underestimates the true variance. When we study hierarchical models later in the semester, we will regularly find ourselves estimating variances for normally distributed effects, and will have to deal with the consequences of the fact that the MLEs of these variances are biased low. For models with 2 parameters, we can visualize the likelihood surface with a contour plot. To do so, the first step is to define a lattice of values at which we want to calculate the log-likelihood. Well do so by defining vectors for \\(\\mu\\) and \\(\\sigma^2\\): m.vals &lt;- seq(from = 6, to = 8, by = 0.05) v.vals &lt;- seq(from = 0.3, to = 2.5, by = 0.05) Here is some fancy R code that shows this lattice. Dont worry about how this plot is created, as it isnt critical for what follows. plot(rep(m.vals, length(v.vals)), rep(v.vals, rep(length(m.vals), length(v.vals))), xlab = expression(mu), ylab = expression(sigma^2)) Now we will define the matrix that will store the values of the log-likelihood for each combination of \\(\\mu\\) and \\(\\sigma^2\\) in the lattice shown above. ll.vals &lt;- matrix(nrow = length(m.vals), ncol = length(v.vals)) Next, we will write a nested loop that cycles through the lattice points, calculates the log-likelihood for each, and stores the value of the log likelihood in the matrix ll.vals that we just created. for (i.m in 1:length(m.vals)) { for(i.v in 1:length(v.vals)) { ll.vals[i.m, i.v] &lt;- myxo.ll(m = m.vals[i.m], v = v.vals[i.v]) } } Now we will use the contour function to build the contour plot, and then add a red dot for the MLE. contour(x = m.vals, y = v.vals, z = ll.vals, nlevels = 100, xlab = expression(mu), ylab = expression(sigma^2)) # show the MLE points(x = myxo.mle$par[1], y = myxo.mle$par[2], col = &quot;red&quot;) 1.4 Tadpole data Finally, well take a look at the data from the functional response experiment of Vonesh &amp; Bolker (2005), described in section 6.3.1.1 of Bolkers book. This is another example of using likelihood to estimate parameters in a two-parameter model. This example differs from the previous two examples because we wont assume that the data constitute a simple random sample from some known distribution like the Gaussian or Poisson distribution. Instead, well build a somewhat more customized model for these data that incorporates some ecological ideas. This process of building a customized model is more typical of how one would analyze a real data set. Well start by using the rm command to clean up the workspace. rm(list = ls()) First, well read in the data and explore them in various ways. library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) summary(frog) ## Initial Killed ## Min. : 5.00 Min. : 1.00 ## 1st Qu.: 13.75 1st Qu.: 5.75 ## Median : 25.00 Median :10.00 ## Mean : 38.12 Mean :13.25 ## 3rd Qu.: 56.25 3rd Qu.:18.75 ## Max. :100.00 Max. :35.00 head(frog) ## Initial Killed ## 1 5 1 ## 2 5 2 ## 3 10 5 ## 4 10 6 ## 5 15 10 ## 6 15 9 plot(Killed ~ Initial, data = frog) Following Bolker, well assume that the number of individuals killed takes a binomial distribution, where the number of trials equals the initial tadpole density, and the probability that a tadpole is killed is given by the expression \\[ p_i = \\dfrac{a}{1 + a h N_i} \\] The two parameters to estimate are \\(a\\), which we interpret as the attack rate when the prey density is low, and \\(h\\), which is the handling time. Well first construct the negative log-likelihood function. # negative log-likelihood, for use with optim frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } Now well find the MLE using optim (frog.mle &lt;- optim(par = c(0.5, 1/40), fn = frog.neg.ll)) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced ## $par ## [1] 0.52592567 0.01660454 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 59 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL # note the warnings a.mle &lt;- frog.mle$par[1] h.mle &lt;- frog.mle$par[2] Well plot the data and overlay a fitted line. # add a line to our plot to show the fitted curve plot(Killed ~ Initial, data = frog) init.values &lt;- with(frog, seq(from = min(Initial), to = max(Initial), length = 100)) pred.values &lt;- a.mle * init.values / (1 + a.mle * h.mle * init.values) lines(x = init.values, y = pred.values, col = &quot;red&quot;) Finally, well plot the likelihood contours. # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) Note that, in contrast to the Myxomatosis data, here the likelihood contours form regions whose major axes are not parallel to the parameter axes. Well reflect on the implications of this shape in the next section. "],["beyond-the-mle-confidence-regions-and-hypothesis-tests-using-the-likelihood-function.html", "Chapter 2 Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function 2.1 Confidence intervals for single parameters 2.2 Confidence regions, profile likelihoods, and associated univariate intervals 2.3 Locally quadratic approximations to confidence intervals and regions 2.4 Comparing models: Likelihood ratio test and AIC", " Chapter 2 Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function Likelihood can be used for more than simply isolating the MLE. The likelihood can also be used to generate confidence intervals for single parameters, or confidence regions for several parameters. Well start by using the horse-kick data to see how to generate a confidence interval for a single parameter, and then move on to considering models with more than one parameter. 2.1 Confidence intervals for single parameters First well read in the data and recreate the negative log likelihood function. ################# ## Preparation ################ # read in the data horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) horse.neg.ll &lt;- function(my.lambda) { ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) -1 * sum(ll.vals) } # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.5, to = 1.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.neg.ll(lambda.vals[i.lambda]) } plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) Now well find the a (asymptotic) 95% confidence interval for \\(\\lambda\\) directly. cutoff.ll &lt;- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 # recreate the plot and add a line plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) # use uniroot to find the confidence bounds precisely my.function &lt;- function(my.lambda){ horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda) } (lower &lt;- uniroot(f = my.function, interval = c(0.6, 0.7))) ## $root ## [1] 0.6065198 ## ## $f.root ## [1] -3.556854e-05 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 (upper &lt;- uniroot(f = my.function, interval = c(0.7, 0.9))) ## $root ## [1] 0.8026265 ## ## $f.root ## [1] -0.0001007316 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 As an alternative programming style, we could have defined the objective function on the fly, and not bothered to create my.function. (lower &lt;- uniroot(f = function(x) horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(x) , interval = c(0.6, 0.7))) ## $root ## [1] 0.6065198 ## ## $f.root ## [1] -3.556854e-05 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 Lets recreate the plot and add vertical lines to indicate the confidence interval. plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = c(lower$root, upper$root), col = &quot;red&quot;) # clean up the workspace rm(list = ls()) Thus, the 95% CI for \\(\\lambda\\) is \\((0.607, 0.803)\\). 2.2 Confidence regions, profile likelihoods, and associated univariate intervals With a 2-parameter model, we can plot a confidence region directly. First some housekeeping to get started: library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } (frog.mle &lt;- optim(par = c(0.5, 1/60), fn = frog.neg.ll)) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced ## $par ## [1] 0.52585566 0.01660104 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 61 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle &lt;- frog.mle$par[1] h.mle &lt;- frog.mle$par[2] # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) Equipped with the contour plot, graphing the appropriate confidence region is straightforward. cut.off &lt;- frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2) # recreate the plot and add a line for the 95% confidence region contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) contour(x = a.vals, y = h.vals, z = ll.vals, levels = cut.off, add = TRUE, col = &quot;red&quot;, lwd = 2) However, there are several drawbacks to confidence regions. First, while a two-dimensional confidence region can be readily visualized, it is hard to summarize or describe. Second, and more importantly, most models have more than two parameters. In these models, a confidence region would have more than 2 dimensions, and thus would be impractical to visualize. Thus it is helpful, or even essential, to be able to generate univariate confidence intervals for single parameters from high-dimensional likelihoods. One approach to doing so is to calculate the so-called profile likelihood for a given parameter, and then to derive the univariate interval from this profile likelihood. We will illustrate this approach by computing a profile-based confidence interval for the attack rate \\(a\\) in the tadpole data. # profile log-likelihood function for the attack rate a profile.ll &lt;- function(my.a) { # Calculate the minimum log likelihood value for a given value of a, the attack rate my.ll &lt;- function(h) frog.neg.ll(c(my.a, h)) my.profile &lt;- optimize(f = my.ll, interval = c(0, 0.03), maximum = FALSE) my.profile$objective } # plot the profile likelihood vs. a # not necessary for finding the CI, but useful for understanding a.values &lt;- seq(from = 0.3, to = 0.8, by = 0.01) a.profile &lt;- double(length = length(a.values)) for (i in 1:length(a.values)) { a.profile[i] &lt;- profile.ll(a.values[i]) } plot(x = a.values, y = a.profile, xlab = &quot;a&quot;, ylab = &quot;negative log-likelihood&quot;, type = &quot;l&quot;) Now well follow the same steps as before to compute the profile-based 95% CI. # Now follow the same steps as before to find the profile 95% CI cut.off &lt;- profile.ll(a.mle) + qchisq(0.95, df = 1) / 2 (lower &lt;- uniroot(f = function(x) cut.off - profile.ll(x) , interval = c(0.3, a.mle))) ## $root ## [1] 0.4024268 ## ## $f.root ## [1] -0.0001303772 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 (upper &lt;- uniroot(f = function(x) cut.off - profile.ll(x) , interval = c(a.mle, 0.8))) ## $root ## [1] 0.6824678 ## ## $f.root ## [1] -9.763258e-06 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 plot(x = a.values, y = a.profile, xlab = &quot;a&quot;, ylab = &quot;negative log-likelihood&quot;, type = &quot;l&quot;) abline(v = c(lower$root, upper$root), col = &quot;blue&quot;) abline(h = cut.off, col = &quot;blue&quot;, lty = &quot;dashed&quot;) So, the 95% profile CI for \\(a\\) is (0.402, 0.682). 2.3 Locally quadratic approximations to confidence intervals and regions Likelihood profiling provides a straightforward way to summarize a high-dimensional confidence region by univariate confidence intervals. However, these profile intervals can still involve quite a bit of computation. Further, they are not able to capture possible correlations among parameter estimates, which are revealed in (two-dimensional) confidence regions. (Recall the shape of the joint confidence region for the parameters \\(a\\) and \\(h\\) in the tadpole data.) Locally quadratic approximations provide a way to approximate the (already approximate) univariate confidence intervals and bivariate confidence regions using only information about the curvature of the likelihood surface at the MLE. Well first start by revisiting the horse-kick data again. Of course, with the more precise \\(\\chi^2\\) based confidence interval in hand, there is no reason to seek an approximation. But doing so allows us to illustrate the calculations involved, and to see how well the approximation fares in this case. First some housekeepign to read the data into memory, etc. # clean up rm(list = ls()) # read in the data horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) horse.neg.ll &lt;- function(my.lambda) { ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) -1 * sum(ll.vals) } # use uniroot to find the confidence bounds precisely my.function &lt;- function(my.lambda){ horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda) } lower &lt;- uniroot(f = my.function, interval = c(0.6, 0.7)) upper &lt;- uniroot(f = my.function, interval = c(0.7, 0.9)) Now we will proceed to use a locally quadratic approximation to the negative log likelihood. ## this function finds the second derivative at the MLE by finite differences second.deriv &lt;- function(delta.l) { (horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2 } (horse.D2 &lt;- second.deriv(1e-04)) ## [1] 400 # see how the answer changes if we change delta second.deriv(1e-05) ## [1] 400.0003 Lets compare this answer to the answer obtained by numDeriv::hessian. numDeriv::hessian(func = horse.neg.ll, x = 0.7) ## [,1] ## [1,] 400 The approximate standard error of \\(\\hat{\\lambda}\\) is the square root of the inverse of the second derivative of the likelihood function. (lambda.se &lt;- sqrt(1 / horse.D2)) ## [1] 0.05 Now we can approximate the 95% confidence interval by using critical values from a standard normal distribution. (lower.approx &lt;- 0.7 - qnorm(.975) * lambda.se) ## [1] 0.6020018 (upper.approx &lt;- 0.7 + qnorm(.975) * lambda.se) ## [1] 0.7979982 Compare the approximation to the exact values lower$root ## [1] 0.6065198 upper$root ## [1] 0.8026265 Make a plot # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.5, to = 1.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.neg.ll(lambda.vals[i.lambda]) } plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) ################################### ## Now find the confidence interval, and plot it #################################### cutoff.ll &lt;- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 # add a line to the plot abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = c(lower$root, upper$root), col = &quot;red&quot;) abline(v = c(lower.approx, upper.approx), col = &quot;blue&quot;) legend(x = 0.65, y = 326, leg = c(&quot;exact&quot;, &quot;approximate&quot;), pch = 16, col = c(&quot;red&quot;, &quot;blue&quot;), bty = &quot;n&quot;) # clean up rm(list = ls()) Notice that the full \\(\\chi^2\\)-based confidence intervals capture the asymmetry in the information about \\(\\lambda\\). The intervals based on the quadratic approximation are symmetric. Now, use the quadratic approximation to find standard errors for \\(\\hat{a}\\) and \\(\\hat{h}\\) in the tadpole predation data. The first part is preparatory work from old classes. library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } frog.mle &lt;- optim(par = c(0.5, 1/60), fn = frog.neg.ll) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced (a.mle &lt;- frog.mle$par[1]) ## [1] 0.5258557 (h.mle &lt;- frog.mle$par[2]) ## [1] 0.01660104 Now find the hessian: (D2 &lt;- numDeriv::hessian(func = frog.neg.ll, x = c(a.mle, h.mle))) ## [,1] [,2] ## [1,] 616.5606 -7394.263 ## [2,] -7394.2628 130640.685 The matrix inverse of the hessian is the variance-covariance matrix of the parameters. Note that R uses the function solve to find the inverse of a matrix. # invert to get var-cov matrix (var.matrix &lt;- solve(D2)) ## [,1] [,2] ## [1,] 0.0050493492 2.857932e-04 ## [2,] 0.0002857932 2.383048e-05 We can use the handy cov2cor function to convert the variance matrix into a correlation matrix: cov2cor(var.matrix) ## [,1] [,2] ## [1,] 1.0000000 0.8238872 ## [2,] 0.8238872 1.0000000 Note the large correlation between \\(\\hat{a}\\) and \\(\\hat{h}\\). Compare with Figure 6.13 of Bolker. The standard errors of \\(\\hat{a}\\) and \\(\\hat{h}\\) are the square roots of the diagaonal elements of the variance-covariance matrix. (a.se &lt;- sqrt(var.matrix[1, 1])) ## [1] 0.07105877 (h.se &lt;- sqrt(var.matrix[2, 2])) ## [1] 0.004881647 Note the large correlation between \\(\\hat{a}\\) and \\(\\hat{h}\\). Lets use the (approximate) standard error of \\(\\hat{a}\\) to calculate an (approximate) 95% confidence interval: (ci.approx &lt;- a.mle + qnorm(c(0.025, .975)) * a.se) ## [1] 0.3865830 0.6651283 Recall that the 95% confidence interval we calculated by the profile likelihood was \\((0.402, 0.682)\\). So the quadratic approximation has gotten the width of the interval more or less correct, but it has fared less at capturing the asymmetry of the interval. 2.4 Comparing models: Likelihood ratio test and AIC Obtaining a parsimonious statistical description of data often requires arbitrating between competing model fits. Likelihood provides two tools for comparing models: likelihood ratio tests (LRTs) and information criteria. Of the latter, the best known information criterion is due to Akaike, and takes the name AIC. (Akaike didnt name AIC after himself; he used AIC to refer to An information criterion. In his honor, the acronym is now largely taken to stand for Akaikes information criterion.) LRTs and information criteria have complementary strengths and weaknesses. LRTs are direct, head-to-head comparisons of nested models. By nested, we mean that one model can be obtained as a special case of the other. The reduced, or less flexible (and thus more parsimonious) model plays the role of the null hypothesis, and the full, or more flexible (and thus less parsimonious) model plays the role of the alternative hypothesis. The LRT then formally evaluates whether the improvement in fit offered by the full model is statistically significant, that is, greater than what we would expect merely by chance. On the other hand, information criteria provide a penalized goodness-of-fit measure that can be used to compare many models at once. Information criteria produce a ranking of model fits, and thus a best-fitting model. The downside to information criteria is that there are no hard and fast guidelines to determine when one model provides a significantly better fit than another. The properties of information criteria are also less well understood than the properties of LRTs. To illustrate both, we will use the study of cone production by fir trees studied in \\(\\S\\) 6.6 of Bolker. These data are originally from work by Dodd and Silvertown. The data are much richer than we will examine here. Like Bolker, we will focus on whether the relationship between tree size (as measured by diameter at breast height, or dbh) and the number of cones produces differs between populations that have experienced wave-like die-offs and those that have not. First some preparatory work to import and assemble the data: require(emdbook) data(&quot;FirDBHFec&quot;) # give the data a simpler name fir &lt;- FirDBHFec rm(FirDBHFec) fir &lt;- fir[, c(&quot;WAVE_NON&quot;, &quot;DBH&quot;, &quot;TOTCONES&quot;)] # select just the variables we want summary(fir) ## WAVE_NON DBH TOTCONES ## n:166 Min. : 3.200 Min. : 0.0 ## w:205 1st Qu.: 6.400 1st Qu.: 14.0 ## Median : 7.600 Median : 36.0 ## Mean : 8.169 Mean : 49.9 ## 3rd Qu.: 9.700 3rd Qu.: 66.0 ## Max. :17.400 Max. :297.0 ## NA&#39;s :26 NA&#39;s :114 names(fir) &lt;- c(&quot;wave&quot;, &quot;dbh&quot;, &quot;cones&quot;) # rename the variables # get rid of the incomplete records fir &lt;- na.omit(fir) par(mfrow = c(1, 2)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;w&quot;)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;non-wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;n&quot;)) # any non-integral responses? with(fir, table(cones == round(cones))) # illustrate the use of &#39;with&#39; ## ## FALSE TRUE ## 6 236 # round the non-integral values fir$cones &lt;- round(fir$cones) # check with(fir, table(cones == round(cones))) ## ## TRUE ## 242 Like Bolker, we will assume that the average number of cones produced (\\(\\mu\\)) has a power-law relationship with tree dbh (\\(x\\)). We will also assume that the actual number of cones produced (\\(Y\\)) takes a negative binomial distribution with size-dependent mean and overdispersion parameter \\(k\\). That is, our model is \\[\\begin{align*} \\mu(x) &amp; = a x ^ b \\\\ Y &amp; \\sim \\mbox{NB}(\\mu(x), k) \\end{align*}\\] To head in a slightly different direction from Bolker, we will compare two models. In the first, or reduced, model the same parameters will prevail for both wave and non-wave populations. Thus this model has three parameters: \\(a\\), \\(b\\), and \\(k\\). In the second, or full, model, we will allow the \\(a\\) and \\(b\\) parameters to differ between the wave and non-wave populations. (We will continue to assume a common \\(k\\) for both population types.) Using subscripts on \\(a\\) and \\(b\\) to distinguish population types, the full model then has 5 parameters: \\(a_w\\), \\(a_n\\), \\(b_w\\), \\(b_n\\), and \\(k\\). Well fit the reduced model first. To do so, well use the dnbinom function in R, in which the \\(k\\) parameter is located in the formal argument size. fir.neg.ll &lt;- function(parms, x, y){ a &lt;- parms[1] b &lt;- parms[2] k &lt;- parms[3] my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k, mu = my.mu, log = TRUE) neg.ll &lt;- -1 * sum(ll.values) return(neg.ll) } Note a subtle difference here. In preparation for fitting this same model to different subsets of the data, the function fir.neg.ll has formal arguments that receive the values of the \\(x\\) and \\(y\\) variables. In the call to optim, we can supply those additional values as subsequent arguments in the optim function, as illustrated below. # fit reduced model (fir.reduced &lt;- optim(f = fir.neg.ll, par = c(a = 1, b = 1, k = 1), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## 0.3041425 2.3190142 1.5033525 ## ## $value ## [1] 1136.015 ## ## $counts ## function gradient ## 134 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle &lt;- fir.reduced$par[1] b.mle &lt;- fir.reduced$par[2] k.mle &lt;- fir.reduced$par[3] Make a plot of the reduced model fit, with both populations pooled together: dbh.vals &lt;- seq(from = min(fir$dbh), to = max(fir$dbh), length = 100) fit.vals &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals[i] &lt;- a.mle * dbh.vals[i] ^ b.mle } par(mfrow = c(1, 1)) # don&#39;t break the next figure into two panels with(fir, plot(cones ~ dbh)) # plot the data points lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) Now fit the full model with separate values of \\(a\\) and \\(b\\) for each population: fir.neg.ll.full &lt;- function(parms) { a.w &lt;- parms[1] b.w &lt;- parms[2] a.n &lt;- parms[3] b.n &lt;- parms[4] k &lt;- parms[5] wave &lt;- subset(fir, wave == &quot;w&quot;) nonwave &lt;- subset(fir, wave == &quot;n&quot;) # note how we call fir.neg.ll here, but each time only # passing a subset of the data neg.ll.wave &lt;- fir.neg.ll(parms = c(a = a.w, b = b.w, k = k), x = wave$dbh, y = wave$cones) neg.ll.nonwave &lt;- fir.neg.ll(parms = c(a = a.n, b = b.n, k = k), x = nonwave$dbh, y = nonwave$cones) total.ll &lt;- neg.ll.wave + neg.ll.nonwave return(total.ll) } (fir.full &lt;- optim(f = fir.neg.ll.full, par = c(a.w = 1, b.w = 1, a.n = 1, b.n = 1, k = 1))) ## $par ## a.w b.w a.n b.n k ## 0.4136414 2.1417941 0.2874122 2.3550753 1.5083974 ## ## $value ## [1] 1135.677 ## ## $counts ## function gradient ## 502 NA ## ## $convergence ## [1] 1 ## ## $message ## NULL Lets make a plot to show the different fits. a.w.mle &lt;- fir.full$par[1] b.w.mle &lt;- fir.full$par[2] a.n.mle &lt;- fir.full$par[3] b.n.mle &lt;- fir.full$par[4] par(mfrow = c(1, 2)) # wave populations fit.vals.wave &lt;- fit.vals.non &lt;- double(length = length(dbh.vals)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;w&quot;)) for (i in seq(along = dbh.vals)) { fit.vals.wave[i] &lt;- a.w.mle * dbh.vals[i] ^ b.w.mle } lines(fit.vals.wave ~ dbh.vals, col = &quot;blue&quot;) # non-wave populations plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;non-wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;n&quot;)) for (i in seq(along = dbh.vals)) { fit.vals.non[i] &lt;- a.n.mle * dbh.vals[i] ^ b.n.mle } lines(fit.vals.non ~ dbh.vals, col = &quot;red&quot;) Note that to compute the negative log likelihood for the full model, we compute the negative log likelihood for each population separately, and then sum the two negative log likelihoods. We can see the justification for doing so by writing out the log likelihood function explicitly: \\[\\begin{eqnarray*} \\ln L(a_w, a_n, b_w, b_n, k; \\mathbf{y}) &amp; = &amp; \\ln \\prod_{i \\in \\left\\{w, n \\right\\}} \\prod_{j=1}^{n_i} f(y_{ij}; a_w, a_n, b_w, b_n, k) \\\\ &amp; = &amp; \\sum_{i \\in \\left\\{w, n \\right\\}} \\sum_{j=1}^{n_i} \\ln f(y_{ij}; a_w, a_n, b_w, b_n, k) \\\\ &amp; = &amp; \\sum_{j=1}^{n_w} \\ln f(y_{w,j}; a_w, b_w, k) + \\sum_{j=1}^{n_n} \\ln f(y_{2, n}; a_n, b_n, k) \\end{eqnarray*}\\] Now conduct the likelihood ratio test: (lrt.stat &lt;- 2 * (fir.reduced$value - fir.full$value)) # compute the likelihood ratio test statistic ## [1] 0.6762567 (lrt.pvalue &lt;- pchisq(q = lrt.stat, df = 2, lower.tail = FALSE)) # calculate the p-vlaue ## [1] 0.7131037 The LRT suggests that the full model does not provide a significantly better fit than the reduced model (\\(\\chi^2_2 = 0.676\\), \\(p=0.71\\)). In other words, there is no evidence that the two population types have different relationships between tree size and avearage fecundity. Now compare AIC values for the two models. Because we have already done the LRT, this AIC comparison is for illustration. (aic.reduced &lt;- 2 * fir.reduced$value + 2 * 3) ## [1] 2278.03 (aic.full &lt;- 2 * fir.full$value + 2 * 5) ## [1] 2281.354 (delta.aic &lt;- aic.full - aic.reduced) ## [1] 3.323743 The reduced model is AIC-best, although the \\(\\Delta AIC\\) is only moderately large. We can also fit a Poisson model to these data. Because we have ruled out the need for different models for the two population type, we fit a Poisson model to the data with the two populations pooled together. fir.neg.ll.pois &lt;- function(parms, x, y){ a &lt;- parms[1] b &lt;- parms[2] my.mu &lt;- a * x^b ll.values &lt;- dpois(y, lambda = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.pois &lt;- optim(f = fir.neg.ll.pois, par = c(a = 1, b = 1), x = fir$dbh, y = fir$cones)) ## $par ## a b ## 0.2613297 2.3883860 ## ## $value ## [1] 3161.832 ## ## $counts ## function gradient ## 115 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle.pois &lt;- fir.pois$par[1] b.mle.pois &lt;- fir.pois$par[2] Calculate the AIC for this model: # calculate AIC (aic.pois &lt;- 2 * fir.pois$value + 2 * 2) ## [1] 6327.664 Whoa! The AIC suggests the negative binomial model is an overwhelmingly better fit. Finally, make a plot to compare the two fits: with(fir, plot(cones ~ dbh)) lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) # plot the fit from the NegBin model ## calculate and plot the fit for the Poisson model fit.vals.pois &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals.pois[i] &lt;- a.mle.pois * dbh.vals[i] ^ b.mle.pois } lines(fit.vals.pois ~ dbh.vals, col = &quot;red&quot;) legend(x = 4, y = 280, leg = c(&quot;Neg Bin&quot;, &quot;Poisson&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, bty = &quot;n&quot;) "]]
