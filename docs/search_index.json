[["bayesian-computation.html", "Chapter 3 Bayesian computation 3.1 Computations with conjugate priors 3.2 JAGS in R", " Chapter 3 Bayesian computation This chapter of the computing companion will focus solely on the computing aspects of Bayesian computation in R. See the course notes or relevant sections of Bolker for the underlying theory. The landscape of computing tools available to fit Bayesian models is fluid. Here, we will look at three tools currently available: R2jags, which is based on the JAGS (Just Another Gibbs Sampler) platform, rstan, which is based on the computer program Stan (itself based on Hamiltonian Monte Carlo, or HMC), and the recent rstanarm, which seeks to put much of the computational details in the background. (The arm portion of the name rstanarm is an acronym for applied regression modeling.) Throughout, we will be working with two data sets: the horse kick data (again), and a data set that details how the rate at which a cricket chirps depends on the air temperature. The horse kick data are useful in this context because a Gamma distribution is a conjugate prior for Poisson data. Thus, if we use a Gamma prior, then we know the posterior exactly. Therefore, we can compare the approximations provided by stochastic sampling schemes to the known posterior. The cricket data set will be used as an example of a simple linear regression, even though the data hint that the actual relationship between temperature and the rate of chirping is nonlinear. 3.1 Computations with conjugate priors Suppose that we observe an iid random sample \\(X_1, \\ldots, X_n\\) from a Poisson distribution with unknown parameter \\(\\lambda\\). (This is the setting for the horse-kick data.) If we place a Gamma prior with shape parameter \\(a\\) and rate parameter \\(r\\) on \\(\\lambda\\), then the posterior distribution is also Gamma with shape parameter \\(a + \\sum_n X_n\\) and rate parameter \\(r + n\\). In other words, \\[\\begin{align*} \\lambda &amp; \\sim \\mbox{Gamma}(a, r) \\\\ X_1, \\ldots, X_n &amp; \\sim \\mbox{Pois}(\\lambda) \\\\ \\lambda | X_1, \\ldots, X_n &amp; \\sim \\mbox{Gamma}(a + \\sum_n X_n, r + n) \\\\ \\end{align*}\\] In the horse kick data, \\(\\sum_n x_n = 196\\) and \\(n = 280\\). Suppose we start with the vague Gamma prior \\(a=.01\\), \\(r = .01\\) on \\(\\lambda\\). This prior has mean \\(a/r = 1\\) and variance \\(a/r^2 = 100\\). The posterior distribution for \\(\\lambda\\) is then a Gamma with shape parameter \\(a = 196.01\\) and rate parameter \\(280.01\\). We can plot it: horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE, stringsAsFactors = TRUE) l.vals &lt;- seq(from = 0, to = 2, length = 200) plot(l.vals, dgamma(l.vals, shape = 196.01, rate = 280.01), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;&quot;) lines(l.vals, dgamma(l.vals, shape = .01, rate = .01), lty = &quot;dashed&quot;) abline(v = 0.7, col = &quot;red&quot;) legend(&quot;topleft&quot;, leg = c(&quot;prior&quot;, &quot;posterior&quot;), lty = c(&quot;dashed&quot;, &quot;solid&quot;)) The red line shows the MLE, which is displaced slightly from the posterior mode. As a point estimate, we might consider any of the following. The posterior mean can be found exactly as \\(a/r\\) = 0.70001. Alternatively, we might consider the posterior median qgamma(0.5, shape = 196.01, rate = 280.01) ## [1] 0.6988206 Finally, we might conisder the posterior mode: optimize(f = function(x) dgamma(x, shape = 196.01, rate = 280.01), interval = c(0.5, 1), maximum = TRUE) ## $maximum ## [1] 0.6964383 ## ## $objective ## [1] 7.995941 To find a 95% confidence interval, we might consider the central 95% interval: qgamma(c(0.025, 0.975), shape = 196.01, rate = 280.01) ## [1] 0.6054387 0.8013454 A 95% highest posterior density (HPD) interval takes a bit more work: diff.in.pdf &lt;- function(x){ upper &lt;- qgamma(p = x, shape = 196.01, rate = 280.01) lower &lt;- qgamma(p = x - .95, shape = 196.01, rate = 280.01) dgamma(upper, shape = 196.01, rate = 280.01) - dgamma(lower, shape = 196.01, rate = 280.01) } (upper.qtile &lt;- uniroot(diff.in.pdf, interval = c(0.95, 1))$root) ## [1] 0.9722176 (hpd.ci &lt;- qgamma(p = c(upper.qtile - .95, upper.qtile), shape = 196.01, rate = 280.01)) ## [1] 0.6031732 0.7988576 As an illustration, note that if we had begun with a more informative prior  say, a gamma distribution with shape parameter \\(a = 50\\) and rate parameter = \\(100\\)  then the posterior would have been more of a compromise between the prior and the information in the data: plot(l.vals, dgamma(l.vals, shape = 196 + 50, rate = 100 + 280), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;&quot;) lines(l.vals, dgamma(l.vals, shape = 50, rate = 100), lty = &quot;dashed&quot;) abline(v = 0.7, col = &quot;red&quot;) legend(&quot;topleft&quot;, leg = c(&quot;prior&quot;, &quot;posterior&quot;), lty = c(&quot;dashed&quot;, &quot;solid&quot;)) 3.2 JAGS in R All of the computational tools that we will examine in this section involve some form of stochastic sampling from the posterior. This computing companion will largely use the default settings, though in real practice the analyst will often have to do considerable work adjusting the settings to obtain a satisfactory approximation. Well use JAGS through R, using the library r2jags. Here is JAGS code to approximate the posterior to \\(\\lambda\\) for the horse kick data, using the vague prior. require(R2jags) ## Loading required package: R2jags ## Warning: package &#39;R2jags&#39; was built under R version 4.1.1 ## Loading required package: rjags ## Warning: package &#39;rjags&#39; was built under R version 4.1.1 ## Loading required package: coda ## Linked to JAGS 4.3.0 ## Loaded modules: basemod,bugs ## ## Attaching package: &#39;R2jags&#39; ## The following object is masked from &#39;package:coda&#39;: ## ## traceplot horse.model &lt;- function() { for (j in 1:J) { # J = 280, number of data points y[j] ~ dpois (lambda) # data model: the likelihood } lambda ~ dgamma (0.01, 0.01) # prior # note that BUGS / JAGS parameterizes # gamma by shape, rate } jags.data &lt;- list(y = horse$deaths, J = length(horse$deaths)) jags.params &lt;- c(&quot;lambda&quot;) jags.inits &lt;- function(){ list(&quot;lambda&quot; = rgamma(0.01, 0.01)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = horse.model, n.chains = 3, n.iter = 5000) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 280 ## Unobserved stochastic nodes: 1 ## Total graph size: 283 ## ## Initializing model Lets take a look at some summary statistics of the fit print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpumPenH/model1e302a1a490b.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## lambda 0.700 0.050 0.605 0.665 0.699 0.733 0.800 1.001 3800 ## deviance 629.306 1.405 628.310 628.413 628.770 629.640 633.274 1.005 1900 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 1.0 and DIC = 630.3 ## DIC is an estimate of expected predictive error (lower deviance is better). The Rhat values suggest that our chains have converged, as we might hope for such a simple model. We can generate a trace plot to inspect convergence visually, but beware that visual assessment of convergence is prone to error: traceplot(jagsfit) We can also use the lattice package to construct smoothed estimates of the posterior density: require(lattice) ## Loading required package: lattice jagsfit.mcmc &lt;- as.mcmc(jagsfit) densityplot(jagsfit.mcmc) For a more involved example, lets take a look at the simple regression fit to the cricket data. First, well make a plot of the data and fit a SLR model by least squares. cricket &lt;- read.table(&quot;data/cricket.txt&quot;, header = TRUE) cricket.slr &lt;- lm(chirps ~ temperature, data = cricket) summary(cricket.slr) ## ## Call: ## lm(formula = chirps ~ temperature, data = cricket) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.56009 -0.57930 0.03129 0.59020 1.53259 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.30914 3.10858 -0.099 0.922299 ## temperature 0.21193 0.03871 5.475 0.000107 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9715 on 13 degrees of freedom ## Multiple R-squared: 0.6975, Adjusted R-squared: 0.6742 ## F-statistic: 29.97 on 1 and 13 DF, p-value: 0.0001067 plot(chirps ~ temperature, data = cricket) abline(cricket.slr) Now well fit the same model in JAGS, using vague priors for all model parameters cricket.model &lt;- function() { for (j in 1:J) { # J = number of data points y[j] ~ dnorm (mu[j], tau) # data model: the likelihood # note that BUGS / JAGS uses precision # instead of variance mu[j] &lt;- b0 + b1 * x[j] # compute the mean for each observation } b0 ~ dnorm (0.0, 1E-6) # prior for intercept b1 ~ dnorm (0.0, 1E-6) # prior for slope tau ~ dgamma (0.01, 0.01) # prior for tau # note that BUGS / JAGS parameterizes # gamma by shape, rate sigma &lt;- pow(tau, -1/2) # the SD of the residaul errors } jags.data &lt;- list(y = cricket$chirps, x = cricket$temperature, J = nrow(cricket)) jags.params &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;sigma&quot;) jags.inits &lt;- function(){ list(&quot;b0&quot; = rnorm(1), &quot;b1&quot; = rnorm(1), &quot;tau&quot; = runif(1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = cricket.model, n.chains = 3, n.iter = 5000) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 3 ## Total graph size: 70 ## ## Initializing model print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpaALYhr/modele205b6f45ab.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## b0 -0.267 3.350 -6.904 -2.385 -0.289 1.802 6.401 1.001 3800 ## b1 0.211 0.042 0.128 0.185 0.212 0.238 0.293 1.001 3800 ## sigma 1.037 0.229 0.704 0.877 1.000 1.155 1.596 1.001 3800 ## tau 1.056 0.421 0.392 0.749 1.000 1.300 2.019 1.001 3800 ## deviance 42.940 2.773 39.796 40.925 42.243 44.144 50.139 1.002 2200 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.8 and DIC = 46.8 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) The traces for the intercept arent great, but we havent centered the predictor either. In the usual way, the slope and intercept are strongly negatively correlated in the posterior. We can visualize this posterior correlation: library(hexbin) ## Warning: package &#39;hexbin&#39; was built under R version 4.1.1 library(RColorBrewer) rf &lt;- colorRampPalette(rev(brewer.pal(11, &#39;Spectral&#39;))) with(jagsfit$BUGSoutput$sims.list, hexbinplot(b0 ~ b1, colramp = rf)) We could make life easier on ourselves by centering the predictor and trying again: cricket$temp.ctr &lt;- cricket$temperature - mean(cricket$temperature) jags.data &lt;- list(y = cricket$chirps, x = cricket$temp.ctr, J = nrow(cricket)) jags.params &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;sigma&quot;) jags.inits &lt;- function(){ list(&quot;b0&quot; = rnorm(1), &quot;b1&quot; = rnorm(1), &quot;tau&quot; = runif(1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = cricket.model, n.chains = 3, n.iter = 5000) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 3 ## Total graph size: 70 ## ## Initializing model print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpaALYhr/modele20767654a1.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## b0 16.656 0.279 16.097 16.484 16.653 16.830 17.217 1.001 3800 ## b1 0.212 0.042 0.130 0.185 0.211 0.238 0.297 1.002 1200 ## sigma 1.038 0.224 0.712 0.879 1.002 1.158 1.570 1.001 3800 ## tau 1.050 0.415 0.406 0.745 0.996 1.295 1.974 1.001 3800 ## deviance 42.933 2.711 39.823 40.945 42.249 44.175 50.133 1.001 3800 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.7 and DIC = 46.6 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) The posteriors for the intercept and slope are now uncorrelated: library(hexbin) library(RColorBrewer) rf &lt;- colorRampPalette(rev(brewer.pal(11, &#39;Spectral&#39;))) with(jagsfit$BUGSoutput$sims.list, hexbinplot(b0 ~ b1, colramp = rf)) "]]
