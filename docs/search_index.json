[["index.html", "BMA / ST 590 computing companion Chapter 1 Maximum likelihood estimation 1.1 Mathematical basics 1.2 Horse-kick data 1.3 Pulse rate data 1.4 Tadpole data", " BMA / ST 590 computing companion Kevin Gross 2023-08-30 Chapter 1 Maximum likelihood estimation The likelihood function is the mathematical object that underlies many of the methods that we will study in this course. In this chapter, we will study the properties of the likelihood function for some simple models and data sets. We will see that the likelihood can be used to generate parameter estimates and associated measures of uncertainty (e.g., standard errors and confidence intervals). For most of the methods that we study later in this course, we will use software in which someone else has written code to analyze the likelihood function; thus we won’t have to worry about coding the likelihood function ourselves. However, it is helpful to know how to derive and analyze a likelihood function when needed, because likelihood analysis is flexible and can often be applied in specialized situations where code for a specific analysis may not already exist. 1.1 Mathematical basics The mathematical expression for a likelihood function is identical to the mathematical expression that one would use to find the probability mass or density associated with a particular value of a random variable. For example, suppose that we have a very simple data set that consists of only one observation from a Poisson distribution. Let \\(X\\) denote the value of the single data point, and let \\(\\lambda\\) denote the parameter of the Poisson distribution. In a probability class, we learn that we can find the probability mass associated with any particular value of \\(X\\) using the formula \\[ \\mathrm{Pr}\\!\\left\\{X=x; \\lambda\\right\\} = \\dfrac{e^{-\\lambda} \\lambda^x}{x!}. \\] In R, we can access this probability mass function using the dpois function. For example, if we wanted to find the probability mass associated with \\(X=1\\) when \\(\\lambda = 1.5\\), we could use dpois(x = 1, lambda = 1.5) ## [1] 0.3346952 In likelihood analysis, we use the same mathematical expression for the probability mass function (pmf) of a data set, but we change our perspective. Instead of regarding the parameter as a known quantity and computing the probability associated with various possible values for the data, in likelihood analysis we regard the data as the known quantity and evaluate the same mathematical expression for different parameter values. In notation, this logic translates into an expression that we can write as \\[ \\ell\\!\\left(\\lambda; x\\right) = \\mathrm{Pr}\\!\\left\\{X=x; \\lambda\\right\\}. \\] where we have used \\(\\ell\\!\\left(\\lambda; x\\right)\\) to denote the likelihood function for \\(\\lambda\\) when we have a data set with value \\(x\\). The expression above seems strange, because nothing seems to be happening; we are simply taking the same mathematical expression and calling it two different things depending on the context. But that is all there is to it, at least with regard to constructing the likelihood function. Because a likelihood function uses the same mathematical formulas as a probability mass function, we can use the same functions that R provides for computing probability masses for discretely valued data (or probability densities for continuously valued data) to compute the a likelihood function. Let’s return to our simple example of observing a single observation from a Poisson distribution. Suppose that observation is \\(X=2\\). We can use the dpois function to evaluate the likelihood for this single observation. For example, we can evaluate the likelihood at \\(\\lambda = 1.5\\): dpois(x = 2, lambda = 1.5) ## [1] 0.2510214 Or we could evaluate the likelihood at \\(\\lambda = 2\\) or \\(\\lambda = 2.5\\): dpois(x = 2, lambda = c(2, 2.5)) ## [1] 0.2706706 0.2565156 Now let’s evaluate the likelihood at a sequence of \\(\\lambda\\) values: my.lhood &lt;- function(lambda) dpois(x = 2, lambda = lambda) curve(my.lhood, from = 0, to = 5, xlab = expression(lambda), ylab = &quot;Likelihood&quot;) We might guess that the likelihood is maximized at \\(\\lambda = 2\\). We’d be right, as the plot below suggests. curve(my.lhood, from = 0, to = 5, xlab = expression(lambda), ylab = &quot;Likelihood&quot;) abline(v = 2, col = &quot;red&quot;) 1.2 Horse-kick data Most real data sets contain more than a single observation. Here is a data set that we can use to illustrate maximum likelihood estimation with a single parameter. Famously, Ladislaus van Bortkewitsch (1868 – 1931) published how many members of the Prussian army were killed by horse kicks in each of 20 years, for each of 14 army corps. In this analysis, we will ignore both the temporal structure and the grouping among corps and treat the data as just a simple random sample1 from a Poisson distribution with \\(n=280\\) data points. As a caveat, these data are often used to illustrate the Poisson distribution, as we will use them. They match the Poisson distribution more neatly than we might expect for most data sets. First import the data. Note that the path name used here is specific to the file directory that was used to create this file. The path name that you use will likely differ. horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) Ask for a summary of the data to make sure the data have been imported correctly. summary(horse) ## year corps deaths ## Min. :1875 Length:280 Min. :0.0 ## 1st Qu.:1880 Class :character 1st Qu.:0.0 ## Median :1884 Mode :character Median :0.0 ## Mean :1884 Mean :0.7 ## 3rd Qu.:1889 3rd Qu.:1.0 ## Max. :1894 Max. :4.0 We can also learn about the data by asking to see the first few records using the head command head(horse) ## year corps deaths ## 1 1875 GC 0 ## 2 1876 GC 2 ## 3 1877 GC 2 ## 4 1878 GC 1 ## 5 1879 GC 0 ## 6 1880 GC 0 or we can see the last few records using the tail command: tail(horse) ## year corps deaths ## 275 1889 C15 2 ## 276 1890 C15 2 ## 277 1891 C15 0 ## 278 1892 C15 0 ## 279 1893 C15 0 ## 280 1894 C15 0 Another useful function to keep in mind is the str function which tells you about the [str]ucture of an R object: str(horse) ## &#39;data.frame&#39;: 280 obs. of 3 variables: ## $ year : int 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 ... ## $ corps : chr &quot;GC&quot; &quot;GC&quot; &quot;GC&quot; &quot;GC&quot; ... ## $ deaths: int 0 2 2 1 0 0 1 1 0 3 ... Let’s plot a histogram of the values: hist(horse$deaths, breaks = seq(from = min(horse$deaths) - 0.5, to = max(horse$deaths) + 0.5, by = 1)) 1.2.1 Calculate and plot the log-likelihood function The first step in likelihood analysis is to construct the likelihood function. The likelihood function is given by the same mathematical expression as the expression for the joint probability mass function of the data. This joint pmf should follow from our probability model for the data. In this case, we will assume that the data are an iid sample from a Poisson distribution with parameter \\(\\lambda\\). Denoting the random sample as \\(X_1, X_2, \\ldots, X_n\\), we might write this model as \\[ X_i \\stackrel{\\text{iid}}{\\sim} \\mathrm{Pois}(\\lambda). \\] To make the notation a bit easier, we’ll write the entire data set as a vector \\(\\mathbf{X} = \\left[ X_1 \\; X_2 \\; \\cdots \\; X_n\\right]^T\\), where we use uppercase \\(\\mathbf{X}\\) to denote the unobserved random vector and lowercase \\(\\mathbf{x}\\) to denote a single realization of \\(\\mathbf{X}\\). The likelihood function is then given by \\[\\begin{align*} \\ell(\\lambda; \\mathbf{x}) &amp; = \\mathrm{Pr}\\!\\left\\{\\mathbf{X} = \\mathbf{x}; \\lambda\\right\\} \\\\ &amp; = \\mathrm{Pr}\\!\\left\\{X_1 = x_1, X_2 = x_2, \\ldots X_n = x_n; \\lambda\\right\\} \\\\ &amp; = \\mathrm{Pr}\\!\\left\\{X_1 = x_1; \\lambda\\right\\} \\times \\mathrm{Pr}\\!\\left\\{X_2 = x_2; \\lambda\\right\\} \\times \\cdots \\times \\mathrm{Pr}\\!\\left\\{X_n = x_n; \\lambda\\right\\} \\\\ &amp; = \\prod_{i=1}^n \\mathrm{Pr}\\!\\left\\{X_i = x_i; \\lambda\\right\\}. \\end{align*}\\] The third equality above follows from the independence of the data points. To prevent numerical underflow, we’ll work on the log-likelihood instead of the likelihood itself. Throughout these notes, we’ll use lowercase \\(\\ell = \\ln \\ell\\) to denote the log likelihood. Note that when we use the log likelihood, the product of the marginal pmfs above becomes a sum: \\[\\begin{align*} \\ell(\\lambda; \\mathbf{x}) &amp; = \\ln \\prod_{i=1}^n \\mathrm{Pr}\\!\\left\\{X_i = x_i; \\lambda\\right\\} \\\\ &amp; = \\sum_{i=1}^n \\ln \\mathrm{Pr}\\!\\left\\{X_i = x_i; \\lambda\\right\\} \\end{align*}\\] Let’s create a function that calculates the log-likelihood for a value of \\(\\lambda\\): horse.ll &lt;- function(my.lambda){ ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) sum(ll.vals) } We can use this function to calculate the log-likelihood for any value of \\(\\lambda\\), such as \\(\\lambda = 1\\): horse.ll(1) ## [1] -328.2462 Let’s calculate the log-likelihood for many values of \\(\\lambda\\), in preparation for making a plot. We’ll use a loop here, and not worry about vectorization. # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.01, to = 2.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.ll(lambda.vals[i.lambda]) } Now plot the log-likelihood values vs. the values of \\(\\lambda\\): plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;log likelihood&quot;, type = &quot;l&quot;) abline(v = 0.7, col = &quot;red&quot;) 1.2.2 Find the MLE numerically using ‘optimize’ Bolker’s book illustrates numerical optimization using the optim function. The R documentation recommends using optimize for one-dimensional optimization, and optim for optimizing a function in several dimensions. So, we will use optimize here. We will enclose the entire call to optimize in parentheses so that the output is dumped to the command line in addition to being stored as horse.mle. (horse.mle &lt;- optimize(f = horse.ll, interval = c(0.1, 2), maximum = TRUE)) ## $maximum ## [1] 0.7000088 ## ## $objective ## [1] -314.1545 The optimize function returns a ‘list’. A list is an R object that contains components of different types. The numerically calculated MLE is \\(\\hat{\\lambda} \\approx 0.7\\). The ‘objective’ component of gives the value of the log-likelihood at that point. 1.3 Pulse rate data The data set pulse.csv contains the heights (in cm) and resting pulse rates (in beats per minute) of 43 graduate students at NCSU. We will use the pulse-rate data as an example to illustrate estimating the mean and variance of a Gaussian distribution from a simple random sample. The purpose of this example is two-fold: first, to illustrate maximum-likelihood estimation with more than one parameter, and second, to illustrate an important result about the MLE of the variance for normally distributed data. pulse &lt;- read.csv(&quot;data/pulse.csv&quot;, head = T) Inspect the data to make sure they have been imported correctly. summary(pulse) ## height rate ## Min. :152.0 Min. : 52 ## 1st Qu.:163.0 1st Qu.: 66 ## Median :168.0 Median : 72 ## Mean :168.2 Mean : 72 ## 3rd Qu.:173.0 3rd Qu.: 78 ## Max. :185.0 Max. :100 head(pulse) ## height rate ## 1 152 68 ## 2 173 68 ## 3 165 82 ## 4 160 60 ## 5 168 74 ## 6 170 80 For the sake of illustration, we will estimate the mean and variance of the normal distribution using the optim function in R. First, we write a function to calculate the log likelihood. pulse.ll &lt;- function(m, v){ ll.vals &lt;- dnorm(pulse$rate, mean = m, sd = sqrt(v), log = TRUE) sum(ll.vals) } Note that R’s function for the pdf of a normal distribution — dnorm — is parameterized by the mean and standard deviation (SD) of the normal distribution. Although it would be just as easy to find the MLE of the standard deviation \\(\\sigma\\), for the sake of illustration, we will seek the MLE of the variance, \\(\\sigma^2\\). (It turns out that, if we write the MLE of the standard deviation as \\(\\hat{\\sigma}\\) and the MLE of the variance as \\(\\hat{\\sigma}^2\\), then \\(\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}\\). This is an example of the {} of MLEs.) We can use our function to calculate the likelihood for any choice of mean and variance. For example, let’s try \\(\\mu = 60\\) and \\(\\sigma^2 = 100\\). pulse.ll(m = 60, v = 100) ## [1] -189.6155 We want to maximize the likelihood using optim. Unfortuantely, optim is a little finicky. To use optim, we have to re-write our function pulse.ll so that the parameters to be estimated are passed to the function as a single vector. Also, by default, optim performs minimization instead of maximization. We can change this behavior when we call optim. Alternatively, we can just re-define the function to return the negative log likelihood. pulse.neg.ll &lt;- function(pars){ m &lt;- pars[1] v &lt;- pars[2] ll.vals &lt;- dnorm(pulse$rate, mean = m, sd = sqrt(v), log = TRUE) -sum(ll.vals) } Now we can use optim: (pulse.mle &lt;- optim(par = c(60, 100), # starting values, just a ballpark guess fn = pulse.neg.ll)) ## $par ## [1] 71.99897 93.65332 ## ## $value ## [1] 158.6099 ## ## $counts ## function gradient ## 51 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Note that the MLE of the variance is \\[ \\hat{\\sigma}^2 = \\frac{\\sum_i (x_i - \\bar{x})}{n}. \\] Let’s verify this by calculating the same quantity at the command line: residuals &lt;- with(pulse, rate - mean(rate)) ss &lt;- sum(residuals^2) n &lt;- nrow(pulse) ss / n ## [1] 93.62791 Compare this to the answer given by var, and to the more usual calculation of the variance as \\[ s^2 = \\frac{\\sum_i (x_i - \\bar{x})}{n-1}. \\] (var.usual &lt;- ss / (n - 1)) ## [1] 95.85714 var(pulse$rate) ## [1] 95.85714 One main take-home of this example is that when we use maximum likelihood to estimate variances for normally distributed data, the MLE is biased low. In other words, it underestimates the true variance. When we study hierarchical models later in the semester, we will regularly find ourselves estimating variances for normally distributed effects, and will have to deal with the consequences of the fact that the MLEs of these variances are biased low. For models with 2 parameters, we can visualize the likelihood surface with a contour plot. To do so, the first step is to define a lattice of values at which we want to calculate the log-likelihood. We’ll do so by defining vectors for \\(\\mu\\) and \\(\\sigma^2\\): m.vals &lt;- seq(from = 60, to = 80, by = 0.5) v.vals &lt;- seq(from = 70, to = 125, by = 0.5) Now we will define the matrix that will store the values of the log-likelihood for each combination of \\(\\mu\\) and \\(\\sigma^2\\) in the lattice shown above. ll.vals &lt;- matrix(nrow = length(m.vals), ncol = length(v.vals)) Next, we will write a nested loop that cycles through the lattice points, calculates the log-likelihood for each, and stores the value of the log likelihood in the matrix ll.vals that we just created. for (i.m in 1:length(m.vals)) { for(i.v in 1:length(v.vals)) { ll.vals[i.m, i.v] &lt;- pulse.ll(m = m.vals[i.m], v = v.vals[i.v]) } } Now we will use the contour function to build the contour plot, and then add a red dot for the MLE. contour(x = m.vals, y = v.vals, z = ll.vals, nlevels = 100, xlab = expression(mu), ylab = expression(sigma^2)) # show the MLE points(x = pulse.mle$par[1], y = pulse.mle$par[2], col = &quot;red&quot;) 1.4 Tadpole data Finally, we’ll take a look at the data from the functional response experiment of Vonesh and Bolker (2005), described in section 6.3.1.1 of Bolker’s book. This is another example of using likelihood to estimate parameters in a two-parameter model. This example differs from the previous two examples because we won’t assume that the data constitute a simple random sample from some known distribution like the Gaussian or Poisson distribution. Instead, we’ll build a somewhat more customized model for these data that incorporates some ecological ideas. This process of building a customized model is more typical of how one would analyze a “real” data set. We’ll start by using the rm command to clean up the workspace. rm(list = ls()) First, we’ll read in the data and explore them in various ways. library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) summary(frog) ## Initial Killed ## Min. : 5.00 Min. : 1.00 ## 1st Qu.: 13.75 1st Qu.: 5.75 ## Median : 25.00 Median :10.00 ## Mean : 38.12 Mean :13.25 ## 3rd Qu.: 56.25 3rd Qu.:18.75 ## Max. :100.00 Max. :35.00 head(frog) ## Initial Killed ## 1 5 1 ## 2 5 2 ## 3 10 5 ## 4 10 6 ## 5 15 10 ## 6 15 9 plot(Killed ~ Initial, data = frog) Following Bolker, we’ll assume that the number of individuals killed takes a binomial distribution, where the number of trials equals the initial tadpole density, and the probability that a tadpole is killed is given by the expression \\[ p_i = \\dfrac{a}{1 + a h N_i}. \\] The two parameters to estimate are \\(a\\), which we interpret as the attack rate when the prey density is low, and \\(h\\), which is the handling time. This model is motivated by the so-called “Type II” functional response of predator-prey ecology, in which the prey consumption rate saturates as prey density grows. In this case, using the Type II functional curve for these data is a pedagogical simplification; as Vonesh and Bolker (2005) observe, Holling’s functional responses give the predation rate when the prey density is constant. However, this experiment ran for two weeks and prey densities declined over the course of the experiment. A more appropriate analysis, and one that Vonesh and Bolker (2005) pursue in their paper, takes account of the declining prey densities. For the purposes of this example, though, we’ll ignore this aspect of the analysis (as Bolker (2008) does) and fit the data assuming that the probability of predation is given by the Type II functional response. If we write the number of individuals killed in each trial as \\(Y_i\\), The full model can then be written as \\[\\begin{align*} Y_i &amp; \\sim \\mathrm{Binom}\\left(p_i, N_i \\right) \\\\ p_i &amp; = \\dfrac{a}{1 + a h N_i}. \\end{align*}\\] We’ll first construct the negative log-likelihood function. # negative log-likelihood, for use with optim frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } Now we’ll find the MLE using optim (frog.mle &lt;- optim(par = c(0.5, 1/40), fn = frog.neg.ll)) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced ## $par ## [1] 0.52592567 0.01660454 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 59 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Why did this code produce warnings? Should we care? Let’s extract the MLEs and add a fitted to our data plot. We’ll plot the data and overlay a fitted line. a.mle &lt;- frog.mle$par[1] h.mle &lt;- frog.mle$par[2] # add a line to our plot to show the fitted curve plot(Killed ~ Initial, data = frog) init.values &lt;- with(frog, seq(from = min(Initial), to = max(Initial), length = 100)) pred.values &lt;- a.mle * init.values / (1 + a.mle * h.mle * init.values) lines(x = init.values, y = pred.values, col = &quot;red&quot;) Finally, we’ll plot the likelihood contours. # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) Note that, in contrast to the pulse-rate data, here the likelihood contours form regions whose major axes are not parallel to the parameter axes. We’ll reflect on the implications of this shape in the next section. Bibliography "],["beyond-the-mle-confidence-regions-and-hypothesis-tests-using-the-likelihood-function.html", "Chapter 2 Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function 2.1 Confidence intervals for single parameters 2.2 Confidence regions, profile likelihoods, and associated univariate intervals 2.3 Locally quadratic approximations to confidence intervals and regions 2.4 Comparing models: Likelihood ratio test and AIC 2.5 Transformable constraints 2.6 The negative binomial distriution, revisited", " Chapter 2 Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function Likelihood can be used for more than simply isolating the MLE. The likelihood can also be used to generate confidence intervals for single parameters, or confidence regions for several parameters. We’ll start by using the horse-kick data to see how to generate a confidence interval for a single parameter, and then move on to considering models with more than one parameter. 2.1 Confidence intervals for single parameters First we’ll read in the data and recreate the negative log likelihood function. horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) horse.neg.ll &lt;- function(my.lambda) { ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) -1 * sum(ll.vals) } # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.5, to = 1.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.neg.ll(lambda.vals[i.lambda]) } plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) To find an asymptotic confidence interval for \\(\\lambda\\) with confidence level \\(100 \\times (1-\\alpha)\\%\\), we want to find all the values of \\(\\lambda\\) for which the negative log-likelihood is no greater than \\(\\frac{1}{2}\\chi^2_1(1-\\alpha)\\) larger than the negative log-likelihood at the MLE.2 (In other words, if we think about the negative log likelihood as quantifying the “badness of fit”, as Bolker suggests, then we want to find all values of \\(\\lambda\\) that give a fit that is no more than \\(\\frac{1}{2}\\chi^2_1(1-\\alpha)\\) worse than the fit at the MLE.) By \\(\\chi^2_1(1-\\alpha)\\), we mean \\(1-\\alpha\\) quantile of a \\(\\chi^2_1\\) distribution, which can be found with the function qchisq in R. The code below uses the function uniroot to find the upper and lower bounds of a 95% CI for \\(\\lambda\\). cutoff.ll &lt;- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 # recreate the plot and add a line plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) # use uniroot to find the confidence bounds precisely my.function &lt;- function(my.lambda){ horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda) } (lower &lt;- uniroot(f = my.function, interval = c(0.6, 0.7))) ## $root ## [1] 0.6065198 ## ## $f.root ## [1] -3.556854e-05 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 (upper &lt;- uniroot(f = my.function, interval = c(0.7, 0.9))) ## $root ## [1] 0.8026265 ## ## $f.root ## [1] -0.0001007316 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 As an alternative programming style, we could have defined the objective function on the fly without bothering to create my.function. (lower &lt;- uniroot(f = function(x) horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(x) , interval = c(0.6, 0.7))) ## $root ## [1] 0.6065198 ## ## $f.root ## [1] -3.556854e-05 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 Let’s recreate the plot and add vertical lines to indicate the confidence interval. plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = c(lower$root, upper$root), col = &quot;red&quot;) # clean up the workspace rm(list = ls()) Thus, the 95% CI for \\(\\lambda\\) is \\((0.607, 0.803)\\). There are two important caveats about the CIs constructed from the likelihood function in this way. First, the coverage is asymptotic, which means that the actual coverage is only guaranteed to match the nominal coverage (e.g., the 95% value) in the limit as the volume of data grows large. As Bolker (p. 194) notes, though, analysts use these asymptotic CIs “very freely”. Secondly, the CI is only valid if the MLE lies in the interior of its range of allowable values. (In other words, the CI isn’t valid if the MLE lies at the edge of the parameter’s allowable values.) We’ll have to worry about this most when constructing likelihood-based CIs for variances. To foreshadow, in mixed models we sometimes encounter a variance whose MLE is 0 — its smallest allowable value. In those case, we’ll have to modify the method detailed here to get a valid CI. 2.2 Confidence regions, profile likelihoods, and associated univariate intervals With a 2-parameter model, we can plot a confidence region directly. First some housekeeping to get started: library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } (frog.mle &lt;- optim(par = c(0.5, 1/60), fn = frog.neg.ll)) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced ## $par ## [1] 0.52585566 0.01660104 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 61 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle &lt;- frog.mle$par[1] h.mle &lt;- frog.mle$par[2] # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) Equipped with the contour plot, graphing the appropriate confidence region is straightforward. cut.off &lt;- frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2) # recreate the plot and add a line for the 95% confidence region contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) contour(x = a.vals, y = h.vals, z = ll.vals, levels = cut.off, add = TRUE, col = &quot;red&quot;, lwd = 2) However, there are several drawbacks to confidence regions. First, while a two-dimensional confidence region can be readily visualized, it is hard to summarize or describe. Second, and more importantly, most models have more than two parameters. In these models, a confidence region would have more than 2 dimensions, and thus would be impractical to visualize. Thus it is helpful, or even essential, to be able to generate univariate confidence intervals for single parameters from high-dimensional likelihoods. One approach to doing so is to calculate the so-called profile likelihood for a given parameter, and then to derive the univariate interval from this profile likelihood. We will illustrate this approach by computing a profile-based confidence interval for the attack rate \\(a\\) in the tadpole data. # profile log-likelihood function for the attack rate a profile.ll &lt;- function(my.a) { # Calculate the minimum log likelihood value for a given value of a, the attack rate my.ll &lt;- function(h) frog.neg.ll(c(my.a, h)) my.profile &lt;- optimize(f = my.ll, interval = c(0, 0.03), maximum = FALSE) my.profile$objective } # plot the profile likelihood vs. a # not necessary for finding the CI, but useful for understanding a.values &lt;- seq(from = 0.3, to = 0.8, by = 0.01) a.profile &lt;- double(length = length(a.values)) for (i in 1:length(a.values)) { a.profile[i] &lt;- profile.ll(a.values[i]) } plot(x = a.values, y = a.profile, xlab = &quot;a&quot;, ylab = &quot;negative log-likelihood&quot;, type = &quot;l&quot;) Now we’ll follow the same steps as before to compute the profile-based 95% CI. # Now follow the same steps as before to find the profile 95% CI cut.off &lt;- profile.ll(a.mle) + qchisq(0.95, df = 1) / 2 (lower &lt;- uniroot(f = function(x) cut.off - profile.ll(x) , interval = c(0.3, a.mle))) ## $root ## [1] 0.4024268 ## ## $f.root ## [1] -0.0001303772 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 (upper &lt;- uniroot(f = function(x) cut.off - profile.ll(x) , interval = c(a.mle, 0.8))) ## $root ## [1] 0.6824678 ## ## $f.root ## [1] -9.763258e-06 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 plot(x = a.values, y = a.profile, xlab = &quot;a&quot;, ylab = &quot;negative log-likelihood&quot;, type = &quot;l&quot;) abline(v = c(lower$root, upper$root), col = &quot;blue&quot;) abline(h = cut.off, col = &quot;blue&quot;, lty = &quot;dashed&quot;) So, the 95% profile CI for \\(a\\) is (0.402, 0.682). 2.3 Locally quadratic approximations to confidence intervals and regions Likelihood profiling provides a straightforward way to summarize a high-dimensional confidence region by univariate confidence intervals. However, these profile intervals can still involve quite a bit of computation. Further, they are not able to capture possible correlations among parameter estimates, which are revealed in (two-dimensional) confidence regions. (Recall the shape of the joint confidence region for the parameters \\(a\\) and \\(h\\) in the tadpole data.) Locally quadratic approximations provide a way to approximate the (already approximate) univariate confidence intervals and bivariate confidence regions using only information about the curvature of the likelihood surface at the MLE. We’ll first start by revisiting the horse-kick data again. Of course, with the more precise \\(\\chi^2\\) based confidence interval in hand, there is no reason to seek an approximation. But doing so allows us to illustrate the calculations involved, and to see how well the approximation fares in this case. First some housekeepign to read the data into memory, etc. # clean up rm(list = ls()) # read in the data horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) horse.neg.ll &lt;- function(my.lambda) { ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) -1 * sum(ll.vals) } # use uniroot to find the confidence bounds precisely my.function &lt;- function(my.lambda){ horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda) } lower &lt;- uniroot(f = my.function, interval = c(0.6, 0.7)) upper &lt;- uniroot(f = my.function, interval = c(0.7, 0.9)) Now we will proceed to use a locally quadratic approximation to the negative log likelihood. ## this function finds the second derivative at the MLE by finite differences second.deriv &lt;- function(delta.l) { (horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2 } (horse.D2 &lt;- second.deriv(1e-04)) ## [1] 400 # see how the answer changes if we change delta second.deriv(1e-05) ## [1] 400.0003 Let’s compare this answer to the answer obtained by numDeriv::hessian. numDeriv::hessian(func = horse.neg.ll, x = 0.7) ## [,1] ## [1,] 400 The approximate standard error of \\(\\hat{\\lambda}\\) is the square root of the inverse of the second derivative of the likelihood function. (lambda.se &lt;- sqrt(1 / horse.D2)) ## [1] 0.05 Now we can approximate the 95% confidence interval by using critical values from a standard normal distribution. (lower.approx &lt;- 0.7 - qnorm(.975) * lambda.se) ## [1] 0.6020018 (upper.approx &lt;- 0.7 + qnorm(.975) * lambda.se) ## [1] 0.7979982 Compare the approximation to the “exact” values lower$root ## [1] 0.6065198 upper$root ## [1] 0.8026265 Make a plot # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.5, to = 1.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.neg.ll(lambda.vals[i.lambda]) } plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) ################################### ## Now find the confidence interval, and plot it #################################### cutoff.ll &lt;- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 # add a line to the plot abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = c(lower$root, upper$root), col = &quot;red&quot;) abline(v = c(lower.approx, upper.approx), col = &quot;blue&quot;) legend(x = 0.65, y = 326, leg = c(&quot;exact&quot;, &quot;approximate&quot;), pch = 16, col = c(&quot;red&quot;, &quot;blue&quot;), bty = &quot;n&quot;) # clean up rm(list = ls()) Notice that the full \\(\\chi^2\\)-based confidence intervals capture the asymmetry in the information about \\(\\lambda\\). The intervals based on the quadratic approximation are symmetric. Now, use the quadratic approximation to find standard errors for \\(\\hat{a}\\) and \\(\\hat{h}\\) in the tadpole predation data. The first part is preparatory work from old classes. library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } frog.mle &lt;- optim(par = c(0.5, 1/60), fn = frog.neg.ll) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced (a.mle &lt;- frog.mle$par[1]) ## [1] 0.5258557 (h.mle &lt;- frog.mle$par[2]) ## [1] 0.01660104 Now find the hessian: (D2 &lt;- numDeriv::hessian(func = frog.neg.ll, x = c(a.mle, h.mle))) ## [,1] [,2] ## [1,] 616.5606 -7394.263 ## [2,] -7394.2628 130640.685 The matrix inverse of the hessian is the variance-covariance matrix of the parameters. Note that R uses the function solve to find the inverse of a matrix. # invert to get var-cov matrix (var.matrix &lt;- solve(D2)) ## [,1] [,2] ## [1,] 0.0050493492 2.857932e-04 ## [2,] 0.0002857932 2.383048e-05 We can use the handy cov2cor function to convert the variance matrix into a correlation matrix: cov2cor(var.matrix) ## [,1] [,2] ## [1,] 1.0000000 0.8238872 ## [2,] 0.8238872 1.0000000 Note the large correlation between \\(\\hat{a}\\) and \\(\\hat{h}\\). Compare with Figure 6.13 of Bolker. The standard errors of \\(\\hat{a}\\) and \\(\\hat{h}\\) are the square roots of the diagaonal elements of the variance-covariance matrix. (a.se &lt;- sqrt(var.matrix[1, 1])) ## [1] 0.07105877 (h.se &lt;- sqrt(var.matrix[2, 2])) ## [1] 0.004881647 Note the large correlation between \\(\\hat{a}\\) and \\(\\hat{h}\\). Let’s use the (approximate) standard error of \\(\\hat{a}\\) to calculate an (approximate) 95% confidence interval: (ci.approx &lt;- a.mle + qnorm(c(0.025, .975)) * a.se) ## [1] 0.3865830 0.6651283 Recall that the 95% confidence interval we calculated by the profile likelihood was \\((0.402, 0.682)\\). So the quadratic approximation has gotten the width of the interval more or less correct, but it has fared less at capturing the asymmetry of the interval. 2.4 Comparing models: Likelihood ratio test and AIC Obtaining a parsimonious statistical description of data often requires arbitrating between competing model fits. Likelihood provides two tools for comparing models: likelihood ratio tests (LRTs) and information criteria. Of the latter, the best known information criterion is due to Akaike, and takes the name AIC. (Akaike didn’t name AIC after himself; he used AIC to refer to “An information criterion”. In his honor, the acronym is now largely taken to stand for “Akaike’s information criterion”.) LRTs and information criteria have complementary strengths and weaknesses. LRTs are direct, head-to-head comparisons of nested models. By “nested”, we mean that one model can be obtained as a special case of the other. The “reduced”, or less flexible (and thus more parsimonious) model plays the role of the null hypothesis, and the “full”, or more flexible (and thus less parsimonious) model plays the role of the alternative hypothesis. The LRT then formally evaluates whether the improvement in fit offered by the full model is statistically significant, that is, greater than what we would expect merely by chance. On the other hand, information criteria provide a penalized goodness-of-fit measure that can be used to compare many models at once. Information criteria produce a ranking of model fits, and thus a best-fitting model. The downside to information criteria is that there are no hard and fast guidelines to determine when one model provides a significantly better fit than another. The properties of information criteria are also less well understood than the properties of LRTs. To illustrate both, we will use the study of cone production by fir trees studied in \\(\\S\\) 6.6 of Bolker. These data are originally from work by Dodd and Silvertown. The data are much richer than we will examine here. Like Bolker, we will focus on whether the relationship between tree size (as measured by diameter at breast height, or dbh) and the number of cones produces differs between populations that “have experienced wave-like die-offs” and those that have not. First some preparatory work to import and assemble the data: require(emdbook) data(&quot;FirDBHFec&quot;) # give the data a simpler name fir &lt;- FirDBHFec rm(FirDBHFec) fir &lt;- fir[, c(&quot;WAVE_NON&quot;, &quot;DBH&quot;, &quot;TOTCONES&quot;)] # select just the variables we want summary(fir) ## WAVE_NON DBH TOTCONES ## n:166 Min. : 3.200 Min. : 0.0 ## w:205 1st Qu.: 6.400 1st Qu.: 14.0 ## Median : 7.600 Median : 36.0 ## Mean : 8.169 Mean : 49.9 ## 3rd Qu.: 9.700 3rd Qu.: 66.0 ## Max. :17.400 Max. :297.0 ## NA&#39;s :26 NA&#39;s :114 names(fir) &lt;- c(&quot;wave&quot;, &quot;dbh&quot;, &quot;cones&quot;) # rename the variables # get rid of the incomplete records fir &lt;- na.omit(fir) par(mfrow = c(1, 2)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;w&quot;)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;non-wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;n&quot;)) # any non-integral responses? with(fir, table(cones == round(cones))) # illustrate the use of &#39;with&#39; ## ## FALSE TRUE ## 6 236 # round the non-integral values fir$cones &lt;- round(fir$cones) # check with(fir, table(cones == round(cones))) ## ## TRUE ## 242 Like Bolker, we will assume that the average number of cones produced (\\(\\mu\\)) has a power-law relationship with tree dbh (\\(x\\)). We will also assume that the actual number of cones produced (\\(Y\\)) takes a negative binomial distribution with size-dependent mean and overdispersion parameter \\(k\\). That is, our model is \\[\\begin{align*} \\mu(x) &amp; = a x ^ b \\\\ Y &amp; \\sim \\mbox{NB}(\\mu(x), k) \\end{align*}\\] To head in a slightly different direction from Bolker, we will compare two models. In the first, or reduced, model the same parameters will prevail for both wave and non-wave populations. Thus this model has three parameters: \\(a\\), \\(b\\), and \\(k\\). In the second, or full, model, we will allow the \\(a\\) and \\(b\\) parameters to differ between the wave and non-wave populations. (We will continue to assume a common \\(k\\) for both population types.) Using subscripts on \\(a\\) and \\(b\\) to distinguish population types, the full model then has 5 parameters: \\(a_w\\), \\(a_n\\), \\(b_w\\), \\(b_n\\), and \\(k\\). We’ll fit the reduced model first. To do so, we’ll use the dnbinom function in R, in which the \\(k\\) parameter is located in the formal argument “size”. fir.neg.ll &lt;- function(parms, x, y){ a &lt;- parms[1] b &lt;- parms[2] k &lt;- parms[3] my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k, mu = my.mu, log = TRUE) neg.ll &lt;- -1 * sum(ll.values) return(neg.ll) } Note a subtle difference here. In preparation for fitting this same model to different subsets of the data, the function fir.neg.ll has formal arguments that receive the values of the \\(x\\) and \\(y\\) variables. In the call to optim, we can supply those additional values as subsequent arguments in the optim function, as illustrated below. # fit reduced model (fir.reduced &lt;- optim(f = fir.neg.ll, par = c(a = 1, b = 1, k = 1), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## 0.3041425 2.3190142 1.5033525 ## ## $value ## [1] 1136.015 ## ## $counts ## function gradient ## 134 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle &lt;- fir.reduced$par[1] b.mle &lt;- fir.reduced$par[2] k.mle &lt;- fir.reduced$par[3] Make a plot of the reduced model fit, with both populations pooled together: dbh.vals &lt;- seq(from = min(fir$dbh), to = max(fir$dbh), length = 100) fit.vals &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals[i] &lt;- a.mle * dbh.vals[i] ^ b.mle } par(mfrow = c(1, 1)) # don&#39;t break the next figure into two panels with(fir, plot(cones ~ dbh)) # plot the data points lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) Now fit the full model with separate values of \\(a\\) and \\(b\\) for each population: fir.neg.ll.full &lt;- function(parms) { a.w &lt;- parms[1] b.w &lt;- parms[2] a.n &lt;- parms[3] b.n &lt;- parms[4] k &lt;- parms[5] wave &lt;- subset(fir, wave == &quot;w&quot;) nonwave &lt;- subset(fir, wave == &quot;n&quot;) # note how we call fir.neg.ll here, but each time only # passing a subset of the data neg.ll.wave &lt;- fir.neg.ll(parms = c(a = a.w, b = b.w, k = k), x = wave$dbh, y = wave$cones) neg.ll.nonwave &lt;- fir.neg.ll(parms = c(a = a.n, b = b.n, k = k), x = nonwave$dbh, y = nonwave$cones) total.ll &lt;- neg.ll.wave + neg.ll.nonwave return(total.ll) } (fir.full &lt;- optim(f = fir.neg.ll.full, par = c(a.w = 1, b.w = 1, a.n = 1, b.n = 1, k = 1))) ## $par ## a.w b.w a.n b.n k ## 0.4136414 2.1417941 0.2874122 2.3550753 1.5083974 ## ## $value ## [1] 1135.677 ## ## $counts ## function gradient ## 502 NA ## ## $convergence ## [1] 1 ## ## $message ## NULL Let’s make a plot to show the different fits. a.w.mle &lt;- fir.full$par[1] b.w.mle &lt;- fir.full$par[2] a.n.mle &lt;- fir.full$par[3] b.n.mle &lt;- fir.full$par[4] par(mfrow = c(1, 2)) # wave populations fit.vals.wave &lt;- fit.vals.non &lt;- double(length = length(dbh.vals)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;w&quot;)) for (i in seq(along = dbh.vals)) { fit.vals.wave[i] &lt;- a.w.mle * dbh.vals[i] ^ b.w.mle } lines(fit.vals.wave ~ dbh.vals, col = &quot;blue&quot;) # non-wave populations plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;non-wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;n&quot;)) for (i in seq(along = dbh.vals)) { fit.vals.non[i] &lt;- a.n.mle * dbh.vals[i] ^ b.n.mle } lines(fit.vals.non ~ dbh.vals, col = &quot;red&quot;) Note that to compute the negative log likelihood for the full model, we compute the negative log likelihood for each population separately, and then sum the two negative log likelihoods. We can see the justification for doing so by writing out the log likelihood function explicitly: \\[\\begin{eqnarray*} \\ln L(a_w, a_n, b_w, b_n, k; \\mathbf{y}) &amp; = &amp; \\ln \\prod_{i \\in \\left\\{w, n \\right\\}} \\prod_{j=1}^{n_i} f(y_{ij}; a_w, a_n, b_w, b_n, k) \\\\ &amp; = &amp; \\sum_{i \\in \\left\\{w, n \\right\\}} \\sum_{j=1}^{n_i} \\ln f(y_{ij}; a_w, a_n, b_w, b_n, k) \\\\ &amp; = &amp; \\sum_{j=1}^{n_w} \\ln f(y_{w,j}; a_w, b_w, k) + \\sum_{j=1}^{n_n} \\ln f(y_{2, n}; a_n, b_n, k) \\end{eqnarray*}\\] Now conduct the likelihood ratio test: (lrt.stat &lt;- 2 * (fir.reduced$value - fir.full$value)) # compute the likelihood ratio test statistic ## [1] 0.6762567 (lrt.pvalue &lt;- pchisq(q = lrt.stat, df = 2, lower.tail = FALSE)) # calculate the p-vlaue ## [1] 0.7131037 The LRT suggests that the full model does not provide a significantly better fit than the reduced model (\\(\\chi^2_2 = 0.676\\), \\(p=0.71\\)). In other words, there is no evidence that the two population types have different relationships between tree size and avearage fecundity. Now compare AIC values for the two models. Because we have already done the LRT, this AIC comparison is for illustration. (aic.reduced &lt;- 2 * fir.reduced$value + 2 * 3) ## [1] 2278.03 (aic.full &lt;- 2 * fir.full$value + 2 * 5) ## [1] 2281.354 (delta.aic &lt;- aic.full - aic.reduced) ## [1] 3.323743 The reduced model is AIC-best, although the \\(\\Delta AIC\\) is only moderately large. We can also fit a Poisson model to these data. Because we have ruled out the need for different models for the two population type, we fit a Poisson model to the data with the two populations pooled together. fir.neg.ll.pois &lt;- function(parms, x, y){ a &lt;- parms[1] b &lt;- parms[2] my.mu &lt;- a * x^b ll.values &lt;- dpois(y, lambda = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.pois &lt;- optim(f = fir.neg.ll.pois, par = c(a = 1, b = 1), x = fir$dbh, y = fir$cones)) ## $par ## a b ## 0.2613297 2.3883860 ## ## $value ## [1] 3161.832 ## ## $counts ## function gradient ## 115 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle.pois &lt;- fir.pois$par[1] b.mle.pois &lt;- fir.pois$par[2] Calculate the AIC for this model: # calculate AIC (aic.pois &lt;- 2 * fir.pois$value + 2 * 2) ## [1] 6327.664 Whoa! The AIC suggests the negative binomial model is an overwhelmingly better fit. Finally, make a plot to compare the two fits: with(fir, plot(cones ~ dbh)) lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) # plot the fit from the NegBin model ## calculate and plot the fit for the Poisson model fit.vals.pois &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals.pois[i] &lt;- a.mle.pois * dbh.vals[i] ^ b.mle.pois } lines(fit.vals.pois ~ dbh.vals, col = &quot;red&quot;) legend(x = 4, y = 280, leg = c(&quot;Neg Bin&quot;, &quot;Poisson&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, bty = &quot;n&quot;) 2.5 Transformable constraints So far, we have not thought much about the numerical optimization routines that R uses to find MLEs. If time allowed, we really should think more deeply about how these routines work. Indeed, Bolker devotes an entire chapter (his chapter 7) to numerical optimization. Because time is short, we won’t go that deeply into understanding these methods now, although Bolker’s chapter is worth a read if you are so inclined. There is one topic that deserves more of our attention, which is the issue of constriants on the allowable parameter space. (Bolker touches on this in his \\(\\S\\) 7.4.5.) Many times, we write down models with parameters that only make biological sense in a certain range. For example, in the fir data, we know that the parameter \\(a\\) (the average cone production for trees of size \\(x = 1\\)) must be positive. We also know that \\(k\\), the overdispersion parameter in the negative binomial model, must also be positive. However, most numerical optimization routines are not terribly well suited to optimizing over a constrained space. (The presence of constraints is one of the reasons why it is important to initiate numerical optimization routines with reasonalbe starting values.) One exception is the “L-BFGS-B” method, available in optim, which will permit so-called rectangular constraints. An alternative approach that will work with any numerical optimization scheme is to transform the constraints away. That is, transform the parameterization to a new scale that is unconstrained. Because of the invariance principle of MLEs, these transformations won’t change the MLEs that we eventually find, as long as the MLEs are not on the edge of the original, constrained space. To illustrate, consider the fir data again, and consider the negative-binomial model fit to the entire data set, ignoring differences between wave vs. non-wave populations. To transform away the constraints on \\(a\\) and \\(k\\), re-parameterize the model in terms of the logs of both parameters. That is, define \\[\\begin{align*} a^* &amp; = \\ln (a) \\\\ k^* &amp; = \\ln (k) \\\\ \\end{align*}\\] so that the model is now \\[\\begin{align*} \\mu(x) &amp; = \\exp(a^*) \\times x ^ b \\\\ Y &amp; \\sim \\mbox{NB}(\\mu(x), \\exp(k^*)) \\end{align*}\\] Fitting proceeds in the usual way: fir.neg.ll &lt;- function(parms, x, y){ a &lt;- exp(parms[1]) b &lt;- parms[2] k &lt;- exp(parms[3]) my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k, mu = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.reduced &lt;- optim(f = fir.neg.ll, par = c(a = 0, b = 1, k = 0), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## -1.1914367 2.3195050 0.4074672 ## ## $value ## [1] 1136.015 ## ## $counts ## function gradient ## 158 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Back-transforming to the original scale recovers the previous MLEs: (a.mle &lt;- exp(fir.reduced$par[1])) ## a ## 0.3037845 (b.mle &lt;- fir.reduced$par[2]) ## b ## 2.319505 (k.mle &lt;- exp(fir.reduced$par[3])) ## k ## 1.503006 The constraint issue also explains why we received warnings from R when we first found the MLEs for the tadpole predation data in Section 2.2. Another example that one frequently encounters in ecology are parameters that are constrained to lie between 0 and 1, such as a survival probability. A logit (or log odds) transformation will eliminate the constraints on a parameter that lies between 0 and 1. 2.6 The negative binomial distriution, revisited The negative binomial distribution is a funny distribution that is frequently misunderstood by ecologists. In ecology, the negative binomial distribution is typically parameterized by the distribution’s mean (which we typically write as \\(\\mu\\)) and the “overdispersion parameter”, almost always written as \\(k\\). In this parameterization, if \\(X\\) has a negative binomial distribution with mean \\(\\mu\\) and overdispersion parameter \\(k\\), then the variance of \\(X\\) is \\[ Var(X) = \\mu + \\frac{\\mu^2}{k} \\] Thus, for fixed \\(\\mu\\), the variance increases as \\(k\\) decreases. As \\(k\\) gets large, the variance approaches \\(\\mu\\), and the negative binomial distribution approaches a Poisson distribution. There are a few occasions in ecology where the overdispersion parameter \\(k\\) has a mechanistic interpretation. In all other cases, though, \\(k\\) is merely a phenomenological descriptor that captures the relationship between the mean and variance for one particular value of \\(\\mu\\). The error that most ecologists make is to assume that a single value of \\(k\\) should prevail across several values of \\(\\mu\\). If \\(k\\) is phenomenological, there is no reason that \\(k\\) should remain fixed as \\(\\mu\\) changes. The fit to the fir data exemplifies this error, as so far we have assumed that one value of \\(k\\) must prevail across all sizes of trees. By assuming that \\(k\\) is fixed, we impose a relationship on the data where the variance must increase quadratically as the mean increases. This may be a reasonable model for the relationship between the variance and the mean, or it may not be. Instead of assuming \\(k\\) constant, another equally viable approach might be to assume that \\(k\\) is a linear function of \\(\\mu\\). In other words, We might set \\(k = \\kappa \\mu\\) for some value of \\(\\kappa\\). In this case, for a given mean \\(\\mu\\), the variance would be \\(\\mu + \\frac{\\mu^2}{\\kappa \\mu} = \\mu \\left(1 + \\frac{1}{\\kappa}\\right)\\), so that the variance would increase linearly as the mean increases. We can try fitting this alternative model to the fir tree data, again pooling wave and non-wave populations together. fir.alt.neg.ll &lt;- function(parms, x, y){ a &lt;- exp(parms[1]) b &lt;- parms[2] k &lt;- exp(parms[3]) my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k * my.mu, mu = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.alt &lt;- optim(f = fir.alt.neg.ll, par = c(a = 0, b = 1, k = 0), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## -1.008373 2.243545 -3.278311 ## ## $value ## [1] 1128.403 ## ## $counts ## function gradient ## 182 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL (a.mle.alt &lt;- exp(fir.alt$par[1])) ## a ## 0.3648121 (b.mle.alt &lt;- fir.alt$par[2]) ## b ## 2.243545 (k.mle.alt &lt;- exp(fir.alt$par[3])) ## k ## 0.03769186 If we compare the fits graphically, the alternative model doesn’t generate a dramatically different fit for the relationship between the average cone production and tree size: fit.vals.alt &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals.alt[i] &lt;- a.mle.alt * dbh.vals[i] ^ b.mle.alt } with(fir, plot(cones ~ dbh)) lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) lines(fit.vals.alt ~ dbh.vals, col = &quot;red&quot;) legend(&quot;topleft&quot;, col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, leg = c(&quot;original&quot;, &quot;alternate&quot;)) However, the two models imply very different relationships between the variance in cone production and tree size. Let’s look at the implied relationship between the standard deviation of cone production and tree size: mu.vals &lt;- seq(from = 0, to = max(fit.vals), length = 100) sd.vals.nb1 &lt;- sqrt(mu.vals + mu.vals ^ 2 / k.mle) sd.vals.nb2 &lt;- sqrt(mu.vals * (1 + 1 / k.mle.alt)) plot(mu.vals, sd.vals.nb1, xlab = &quot;mean&quot;, ylab = &quot;SD&quot;, type = &quot;l&quot;, col = &quot;blue&quot;) lines(mu.vals, sd.vals.nb2, col = &quot;red&quot;) legend(&quot;topleft&quot;, col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, leg = c(&quot;original&quot;, &quot;alternate&quot;)) We can calculate the AIC for this alternate parameterization as well: (aic.alt &lt;- 2 * fir.alt$value + 2 * 3) ## [1] 2262.805 Recall that the AIC value for the original fit was 2278.0. Thus the model with the alternative parameterization is considerably better by AIC. See $$6.4.1.1 to see how this follows from a result about the likelihood ratio.↩︎ "],["bayesian-computation.html", "Chapter 3 Bayesian computation 3.1 Computations with conjugate priors 3.2 JAGS in R 3.3 rstanarm", " Chapter 3 Bayesian computation This chapter of the computing companion will focus solely on the computing aspects of Bayesian computation in R. See the course notes or relevant sections of Bolker for the underlying theory. The landscape of computing tools available to fit Bayesian models is fluid. Here, we will look at three tools currently available: R2jags, which is based on the JAGS (Just Another Gibbs Sampler) platform, rstan, which is based on the computer program Stan (itself based on Hamiltonian Monte Carlo, or HMC), and the recent rstanarm, which seeks to put much of the computational details in the background. (The “arm” portion of the name rstanarm is an acronym for applied regression modeling.) Throughout, we will be working with two data sets: the horse kick data (again), and a data set that details how the rate at which a cricket chirps depends on the air temperature. The horse kick data are useful in this context because a Gamma distribution is a conjugate prior for Poisson data. Thus, if we use a Gamma prior, then we know the posterior exactly. Therefore, we can compare the approximations provided by stochastic sampling schemes to the known posterior. The cricket data set will be used as an example of a simple linear regression, even though the data hint that the actual relationship between temperature and the rate of chirping is nonlinear. 3.1 Computations with conjugate priors Suppose that we observe an iid random sample \\(X_1, \\ldots, X_n\\) from a Poisson distribution with unknown parameter \\(\\lambda\\). (This is the setting for the horse-kick data.) If we place a Gamma prior with shape parameter \\(a\\) and rate parameter \\(r\\) on \\(\\lambda\\), then the posterior distribution is also Gamma with shape parameter \\(a + \\sum_n X_n\\) and rate parameter \\(r + n\\). In other words, \\[\\begin{align*} \\lambda &amp; \\sim \\mbox{Gamma}(a, r) \\\\ X_1, \\ldots, X_n &amp; \\sim \\mbox{Pois}(\\lambda) \\\\ \\lambda | X_1, \\ldots, X_n &amp; \\sim \\mbox{Gamma}(a + \\sum_n X_n, r + n) \\\\ \\end{align*}\\] In the horse kick data, \\(\\sum_n x_n = 196\\) and \\(n = 280\\). Suppose we start with the vague Gamma prior \\(a=.01\\), \\(r = .01\\) on \\(\\lambda\\). This prior has mean \\(a/r = 1\\) and variance \\(a/r^2 = 100\\). The posterior distribution for \\(\\lambda\\) is then a Gamma with shape parameter \\(a = 196.01\\) and rate parameter \\(280.01\\). We can plot it: horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE, stringsAsFactors = TRUE) l.vals &lt;- seq(from = 0, to = 2, length = 200) plot(l.vals, dgamma(l.vals, shape = 196.01, rate = 280.01), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;&quot;) lines(l.vals, dgamma(l.vals, shape = .01, rate = .01), lty = &quot;dashed&quot;) abline(v = 0.7, col = &quot;red&quot;) legend(&quot;topleft&quot;, leg = c(&quot;prior&quot;, &quot;posterior&quot;), lty = c(&quot;dashed&quot;, &quot;solid&quot;)) The red line shows the MLE, which is displaced slightly from the posterior mode. As a point estimate, we might consider any of the following. The posterior mean can be found exactly as \\(a/r\\) = 0.70001. Alternatively, we might consider the posterior median qgamma(0.5, shape = 196.01, rate = 280.01) ## [1] 0.6988206 Finally, we might conisder the posterior mode: optimize(f = function(x) dgamma(x, shape = 196.01, rate = 280.01), interval = c(0.5, 1), maximum = TRUE) ## $maximum ## [1] 0.6964383 ## ## $objective ## [1] 7.995941 To find a 95% confidence interval, we might consider the central 95% interval: qgamma(c(0.025, 0.975), shape = 196.01, rate = 280.01) ## [1] 0.6054387 0.8013454 A 95% highest posterior density (HPD) interval takes a bit more work: diff.in.pdf &lt;- function(x){ upper &lt;- qgamma(p = x, shape = 196.01, rate = 280.01) lower &lt;- qgamma(p = x - .95, shape = 196.01, rate = 280.01) dgamma(upper, shape = 196.01, rate = 280.01) - dgamma(lower, shape = 196.01, rate = 280.01) } (upper.qtile &lt;- uniroot(diff.in.pdf, interval = c(0.95, 1))$root) ## [1] 0.9722176 (hpd.ci &lt;- qgamma(p = c(upper.qtile - .95, upper.qtile), shape = 196.01, rate = 280.01)) ## [1] 0.6031732 0.7988576 We might also ask questions like: What is the posterior probability that \\(\\lambda &gt; 2/3\\)? These caluclations are straightforward in a Bayesian context, and they make full sense. pgamma(2/3, shape = 196.01, rate = 280.01, lower.tail = FALSE) ## [1] 0.7434032 Thus we would say that there is a 0.743 posterior probability that \\(\\lambda &gt; 2/3\\). As an illustration, note that if we had begun with a more informative prior — say, a gamma distribution with shape parameter \\(a = 50\\) and rate parameter = \\(100\\) — then the posterior would have been more of a compromise between the prior and the information in the data: plot(l.vals, dgamma(l.vals, shape = 196 + 50, rate = 100 + 280), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;&quot;) lines(l.vals, dgamma(l.vals, shape = 50, rate = 100), lty = &quot;dashed&quot;) abline(v = 0.7, col = &quot;red&quot;) legend(&quot;topleft&quot;, leg = c(&quot;prior&quot;, &quot;posterior&quot;), lty = c(&quot;dashed&quot;, &quot;solid&quot;)) 3.2 JAGS in R All of the computational tools that we will examine in this section involve some form of stochastic sampling from the posterior. This computing companion will largely use the default settings, though in real practice the analyst will often have to do considerable work adjusting the settings to obtain a satisfactory approximation. We’ll use JAGS through R, using the library r2jags. Here is JAGS code to approximate the posterior to \\(\\lambda\\) for the horse kick data, using the vague prior. require(R2jags) ## Loading required package: R2jags ## Loading required package: rjags ## Loading required package: coda ## Linked to JAGS 4.3.1 ## Loaded modules: basemod,bugs ## ## Attaching package: &#39;R2jags&#39; ## The following object is masked from &#39;package:coda&#39;: ## ## traceplot horse.model &lt;- function() { for (j in 1:J) { # J = 280, number of data points y[j] ~ dpois (lambda) # data model: the likelihood } lambda ~ dgamma (0.01, 0.01) # prior # note that BUGS / JAGS parameterizes # gamma by shape, rate } jags.data &lt;- list(y = horse$deaths, J = length(horse$deaths)) jags.params &lt;- c(&quot;lambda&quot;) jags.inits &lt;- function(){ list(&quot;lambda&quot; = rgamma(0.01, 0.01)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = horse.model, n.chains = 3, n.iter = 5000) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 280 ## Unobserved stochastic nodes: 1 ## Total graph size: 283 ## ## Initializing model Let’s take a look at some summary statistics of the fit print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmp2nbvJv/model6f087f565b7.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## lambda 0.701 0.050 0.608 0.667 0.700 0.734 0.802 1.001 3800 ## deviance 629.299 1.363 628.310 628.409 628.766 629.640 633.001 1.002 1400 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 0.9 and DIC = 630.2 ## DIC is an estimate of expected predictive error (lower deviance is better). The Rhat values suggest that our chains have converged, as we might hope for such a simple model. We can generate a trace plot using traceplot to inspect convergence visually, but beware that visual assessment of convergence is prone to error. For an rjags object, the raw MCMC samples are stored in BUGSoutput$sims.list. Sometimes it is helpful to analyze these samples directly. For example, with these samples we can estimate other posterior quantities, such as the posterior median of \\(\\lambda\\), or generate a 95% central posterior confidence interval directly: mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) summary(mcmc.output) ## deviance lambda ## Min. :628.3 Min. :0.5462 ## 1st Qu.:628.4 1st Qu.:0.6667 ## Median :628.8 Median :0.7003 ## Mean :629.3 Mean :0.7013 ## 3rd Qu.:629.6 3rd Qu.:0.7341 ## Max. :639.7 Max. :0.8830 median(mcmc.output$lambda) ## [1] 0.7003436 quantile(mcmc.output$lambda, c(.025, .975)) ## 2.5% 97.5% ## 0.6075287 0.8019439 We can also use the lattice package to construct smoothed estimates of the posterior density: require(lattice) ## Loading required package: lattice jagsfit.mcmc &lt;- as.mcmc(jagsfit) densityplot(jagsfit.mcmc) For a more involved example, let’s take a look at the simple regression fit to the cricket data. First, we’ll make a plot of the data and fit a SLR model by least squares. cricket &lt;- read.table(&quot;data/cricket.txt&quot;, header = TRUE) cricket.slr &lt;- lm(chirps ~ temperature, data = cricket) summary(cricket.slr) ## ## Call: ## lm(formula = chirps ~ temperature, data = cricket) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.56009 -0.57930 0.03129 0.59020 1.53259 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.30914 3.10858 -0.099 0.922299 ## temperature 0.21193 0.03871 5.475 0.000107 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9715 on 13 degrees of freedom ## Multiple R-squared: 0.6975, Adjusted R-squared: 0.6742 ## F-statistic: 29.97 on 1 and 13 DF, p-value: 0.0001067 plot(chirps ~ temperature, data = cricket) abline(cricket.slr) Now we’ll fit the same model in JAGS, using vague priors for all model parameters cricket.model &lt;- function() { for (j in 1:J) { # J = number of data points y[j] ~ dnorm (mu[j], tau) # data model: the likelihood # note that BUGS / JAGS uses precision # instead of variance mu[j] &lt;- b0 + b1 * x[j] # compute the mean for each observation } b0 ~ dnorm (0.0, 1E-6) # prior for intercept b1 ~ dnorm (0.0, 1E-6) # prior for slope tau ~ dgamma (0.01, 0.01) # prior for tau # note that BUGS / JAGS parameterizes # gamma by shape, rate sigma &lt;- pow(tau, -1/2) # the SD of the residaul errors } jags.data &lt;- list(y = cricket$chirps, x = cricket$temperature, J = nrow(cricket)) jags.params &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;sigma&quot;) jags.inits &lt;- function(){ list(&quot;b0&quot; = rnorm(1), &quot;b1&quot; = rnorm(1), &quot;tau&quot; = runif(1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = cricket.model, n.chains = 3, n.iter = 5000) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 3 ## Total graph size: 70 ## ## Initializing model print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmp2nbvJv/model6f084b7a1e34.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## b0 -0.360 3.378 -7.178 -2.473 -0.304 1.819 6.318 1.001 2700 ## b1 0.212 0.042 0.129 0.185 0.212 0.239 0.297 1.001 2500 ## sigma 1.030 0.226 0.706 0.875 0.990 1.148 1.548 1.002 1100 ## tau 1.065 0.412 0.417 0.758 1.020 1.307 2.007 1.002 1100 ## deviance 42.915 2.776 39.792 40.912 42.241 44.204 49.821 1.001 2700 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.9 and DIC = 46.8 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) The output of the print function gives the quantiles that one would use to calculate a 95% central credible interval. To find a HPD credible interval, we can use the HPDinterval function in the coda library. The coda library contains a variety of routines for post-processing of MCMC ouput. If we simply pass the jagsfit object to the HPDinterval function, it will return an HPD interval for each of the three chains. This isn’t what we want, so we’ll extract the raw MCMC samples first, and then coerce them to a data frame. mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) summary(mcmc.output) ## b0 b1 deviance sigma ## Min. :-27.0445 Min. :0.07081 Min. :39.56 Min. :0.5712 ## 1st Qu.: -2.4726 1st Qu.:0.18548 1st Qu.:40.91 1st Qu.:0.8748 ## Median : -0.3042 Median :0.21227 Median :42.24 Median :0.9903 ## Mean : -0.3599 Mean :0.21250 Mean :42.91 Mean :1.0300 ## 3rd Qu.: 1.8193 3rd Qu.:0.23891 3rd Qu.:44.20 3rd Qu.:1.1485 ## Max. : 11.5566 Max. :0.51651 Max. :77.50 Max. :3.2191 ## tau ## Min. :0.0965 ## 1st Qu.:0.7581 ## Median :1.0198 ## Mean :1.0645 ## 3rd Qu.:1.3066 ## Max. :3.0653 Now we’ll coerce the data frame mcmc.output to an MCMC object, and pass it to HPDinterval: HPDinterval(as.mcmc(mcmc.output)) ## lower upper ## b0 -7.1782018 6.3029131 ## b1 0.1278054 0.2945144 ## deviance 39.5847864 48.1864169 ## sigma 0.6684013 1.4699785 ## tau 0.2984721 1.8414474 ## attr(,&quot;Probability&quot;) ## [1] 0.9498667 One of the merits of the Bayesian approach is that the posterior samples provide an immediate tool for propagating uncertainty to (possibly derived) quantities of interest. We can summarize the uncertainty in the regression fit graphically by randomly sampling a subset of these samples (say, 100 of them) and using them to plot a collection of regression lines: plot(chirps ~ temperature, data = cricket, type = &quot;n&quot;) # we&#39;ll add the points later so that they lie on top of the lines, # instead of the other way around subset.samples &lt;- sample(nrow(mcmc.output), size = 100) for(i in subset.samples) { with(mcmc.output, abline(a = b0[i], b = b1[i], col = &quot;deepskyblue&quot;, lwd = 0.25)) } with(cricket, points(chirps ~ temperature)) We can also propagate the uncertainty to estimate, say, the posterior distribution for the value of the regression line when the temperature is 85 F. This quantifies the uncertainty in the average number of chirps at this temperature. (We can think of it as a vertical slice through the above plot.) avg.chirps.85 &lt;- with(mcmc.output, b0 + b1 * 85) summary(avg.chirps.85) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.24 17.47 17.71 17.70 17.93 19.26 quantile(avg.chirps.85, probs = c(.025, 0.975)) ## 2.5% 97.5% ## 17.02601 18.40125 We could use the density function to get a quick idea of the shape of the distribution: plot(density(avg.chirps.85)) Thus, we might say that the posterior mean for the average number of chirps at 85 F is 17.7, and a central 95% credible interval is (17.03, 18.4). Finally, we can use the posterior samples to estimate the uncertainty in a future observation. When we use a posterior distribution to estimate the distribution of a future observation, we refer to it as a posterior predictive distribution. The posterior predictive distribution must also include the error around the regression line. We can estimate the posterior predictive distribution as follows. Suppose we denote sample \\(i\\) from the posterior as \\(\\beta_{0, i}\\), \\(\\beta_{1, i}\\), and \\(\\sigma_i\\). Then for each posterior sample we will generate a new hypothetical observation \\(y_i^\\star\\) by sampling from a Gaussian distribution with mean equal to ${0,i} + {1,i} x $ and standard deviation \\(\\sigma_i\\), where \\(x = 85\\). The distribution of the \\(y_i^*\\)’s then gives the posterior predictive distribution that we seek. n.sims &lt;- nrow(mcmc.output) new.errors &lt;- with(mcmc.output, rnorm(n.sims, mean = 0, sd = sigma)) new.chirps.85 &lt;- with(mcmc.output, b0 + b1 * 85) + new.errors plot(density(new.chirps.85)) summary(new.chirps.85) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 13.32 16.99 17.71 17.70 18.44 23.38 quantile(new.chirps.85, probs = c(.025, 0.975)) ## 2.5% 97.5% ## 15.51400 19.93126 Thus, the posterior predictive distribution has a central 95% credible interval of (15.51, 19.93). Although it hasn’t caused any difficulty here, the slope and intercept are strongly negatively correlated in the posterior. We can visualize this posterior correlation: library(hexbin) library(RColorBrewer) rf &lt;- colorRampPalette(rev(brewer.pal(11, &#39;Spectral&#39;))) with(jagsfit$BUGSoutput$sims.list, hexbinplot(b1 ~ b0, colramp = rf)) We can estimate the posterior correlation between the intercept and the slope by accessing the raw MCMC samples cor(mcmc.output[, -c(3:4)]) ## b0 b1 tau ## b0 1.000000000 -0.996542265 0.007030068 ## b1 -0.996542265 1.000000000 -0.005982299 ## tau 0.007030068 -0.005982299 1.000000000 Thus we estimate that the intercept and slope have a posterior correlation of -0.997. We could make life easier on ourselves by centering the predictor and trying again: cricket$temp.ctr &lt;- cricket$temperature - mean(cricket$temperature) jags.data &lt;- list(y = cricket$chirps, x = cricket$temp.ctr, J = nrow(cricket)) jags.params &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;sigma&quot;) jags.inits &lt;- function(){ list(&quot;b0&quot; = rnorm(1), &quot;b1&quot; = rnorm(1), &quot;tau&quot; = runif(1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = cricket.model, n.chains = 3, n.iter = 5000) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 3 ## Total graph size: 70 ## ## Initializing model print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmp2nbvJv/model6f083eeb60b9.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## b0 16.655 0.271 16.119 16.484 16.656 16.826 17.200 1.001 3700 ## b1 0.211 0.042 0.128 0.184 0.211 0.238 0.294 1.010 3000 ## sigma 1.031 0.222 0.706 0.876 0.993 1.143 1.552 1.002 1200 ## tau 1.060 0.411 0.415 0.766 1.014 1.304 2.009 1.002 1200 ## deviance 42.865 2.730 39.790 40.873 42.156 44.059 49.881 1.002 3500 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.7 and DIC = 46.6 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) The posteriors for the intercept and slope are now uncorrelated: library(hexbin) library(RColorBrewer) rf &lt;- colorRampPalette(rev(brewer.pal(11, &#39;Spectral&#39;))) with(jagsfit$BUGSoutput$sims.list, hexbinplot(b1 ~ b0, colramp = rf)) mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) cor(mcmc.output[, -c(3:4)]) ## b0 b1 tau ## b0 1.00000000 -0.01190526 -0.01067385 ## b1 -0.01190526 1.00000000 0.01648976 ## tau -0.01067385 0.01648976 1.00000000 3.3 rstanarm The rstanarm package is a recent set of routines that seeks to provide a user-friendly front end to Bayesian analysis with Stan. Specifically, rstanarm provides functions for fitting standard statistical models that are meant to mimic the analogous fitting functions in R. For example, the basic routine for fitting linear models in R is lm; rstanarm provides a function stan_lm that strives to have the same functionality and interface as lm, albeit using Stan “under the hood” to generate Bayesian inference. (That said, the main workhorse function in rstanarm for model fitting is stan_glm, which attempts to mimic the native R function glm for fitting generalized linear models. Separately, the developers of rstanarm have taken the not unreasonable stance that generalized linear models should supplant general linear models as the analyst’s default approach to model fitting.) To provide functionality that is similar to R’s native model-fitting routines, the functions in rstanarm make a number of operational decisions behind the scenes. Most notably, the model fitting routines in rstanarm will select default priors and default HMC parameters. While these defaults can always be modified by the analyst, the implementation of software that chooses priors by default is radical. First, the developers of rstanarm have their own particular view about what the role of the prior should be in data analysis. While their view is a considered one, by no means does it reflect a consensus that extends beyond the developers of the software. If you use rstanarm’s routines out of the box, you are accepting this view as your own if you do not specify the priors yourself. Second, as best I understand, the methods by which rstanarm chooses default priors still appear to be in some flux. That means that future versions of rstanarm may supply different default priors than those that are supplied today. As a result, the behavior of rstanarm today may differ from its behavior tomorrow, if you use the default priors. All that said, here is how you might use rstanarm to fit the simple regression to the cricket data: require(rstanarm) ## Loading required package: rstanarm ## Loading required package: Rcpp ## This is rstanarm version 2.21.4 ## - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors! ## - Default priors may change, so it&#39;s safest to specify priors, even if equivalent to the defaults. ## - For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()) stanarm.cricket.fit &lt;- stan_glm(chirps ~ temp.ctr, data = cricket, family = gaussian, seed = 1) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 6.3e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.027 seconds (Warm-up) ## Chain 1: 0.036 seconds (Sampling) ## Chain 1: 0.063 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 7e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.037 seconds (Warm-up) ## Chain 2: 0.027 seconds (Sampling) ## Chain 2: 0.064 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 7e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.026 seconds (Warm-up) ## Chain 3: 0.025 seconds (Sampling) ## Chain 3: 0.051 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 6e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.027 seconds (Warm-up) ## Chain 4: 0.024 seconds (Sampling) ## Chain 4: 0.051 seconds (Total) ## Chain 4: We can discover the priors that stan_glm has selected by using the function prior_summary. prior_summary(stanarm.cricket.fit) ## Priors for model &#39;stanarm.cricket.fit&#39; ## ------ ## Intercept (after predictors centered) ## Specified prior: ## ~ normal(location = 17, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 17, scale = 4.3) ## ## Coefficients ## Specified prior: ## ~ normal(location = 0, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 0, scale = 0.63) ## ## Auxiliary (sigma) ## Specified prior: ## ~ exponential(rate = 1) ## Adjusted prior: ## ~ exponential(rate = 0.59) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details The rstanarm package has made a variety of decisions about how many chains to run, how long to run them, etc. We can obtain a summary of the model fit by the print command: print(stanarm.cricket.fit, digits = 3) ## stan_glm ## family: gaussian [identity] ## formula: chirps ~ temp.ctr ## observations: 15 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) 16.648 0.257 ## temp.ctr 0.210 0.041 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 1.008 0.198 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg There are a few parts of this output that deserve comment. First, this summary reports the posterior median of the parameters instead of the posterior mean. Second, the authors of rstanarm have made the curious decision to replace the posterior standard deviation (itself the Bayesian counterpart to the frequentist’s standard error) with someting they call “MAD SD”. This takes a bit of explanation. The “MAD” part stands for median absolute deviation. It is the median of the absolute deviations of the posterior samples from the posterior median. In other words, if we have a generic parameter \\(\\theta\\) and label its posterior samples as \\(\\theta_1, \\theta_2, \\ldots, \\theta_n\\), then the MAD of \\(\\theta\\) is \\[ \\mathrm{median}_i(| \\theta_i - \\mathrm{median}_i(\\theta_i) |) \\] According to the authors of rstanarm, “Because we are so used to working with standard deviations, when we compute the median absolute deviation, we then rescale it by multiplying by 1.483, which reproduces the standard deviation in the special case of the normal distribution. We call this the mad sd.” In other words, the MAD SD is a measure of posterior uncertainty that is meant to be comparable to the posterior standard deviation. (The authors of rstanarm clearly must think this is a more desirable estimate of the posterior uncertainty than the posterior standard deviation; though their reasoning here is not immediately clear to me.) If we want to compute our own summary statistics, we can extract the MCMC samples from the stam_glm fit using the as.matrix command: mcmc.sims &lt;- as.matrix(stanarm.cricket.fit) summary(mcmc.sims) ## (Intercept) temp.ctr sigma ## Min. :15.65 Min. :0.02782 Min. :0.5582 ## 1st Qu.:16.48 1st Qu.:0.18191 1st Qu.:0.8869 ## Median :16.65 Median :0.21023 Median :1.0079 ## Mean :16.65 Mean :0.20955 Mean :1.0457 ## 3rd Qu.:16.83 3rd Qu.:0.23740 3rd Qu.:1.1627 ## Max. :17.68 Max. :0.36177 Max. :2.3400 We might, for example, then use this output to find the posterior standard deviation of each of the parameters, or to find central 95% credible intervals: apply(mcmc.sims, 2, sd) ## (Intercept) temp.ctr sigma ## 0.27003507 0.04262539 0.22262375 apply(mcmc.sims, 2, function(x) quantile(x, c(0.025, 0.975))) ## parameters ## (Intercept) temp.ctr sigma ## 2.5% 16.10111 0.1240071 0.7171401 ## 97.5% 17.17942 0.2906848 1.5816217 Compare these values to the posterior standard deviations and 95% central credible intervals reported in the JAGS fit. "],["smooth-regression.html", "Chapter 4 Smooth regression 4.1 Loess smoothers 4.2 Splines 4.3 Generalized additive models (GAMs)", " Chapter 4 Smooth regression 4.1 Loess smoothers We will illustrate LOESS smoothers with the bioluminescence data found in the ISIT data set. These data can be found by visiting the webpage for the book ``Mixed Effects Models and Extensions in Ecology with R’’ by Zuur et al. (2009). A link to this webpage appears on the course website. ## download the data from the book&#39;s website isit &lt;- read.table(&quot;data/ISIT.txt&quot;, head = T) ## extract the data from station 16 st16 &lt;- subset(isit, Station == 16) ## retain just the variables that we want, and rename st16 &lt;- st16[, c(&quot;SampleDepth&quot;, &quot;Sources&quot;)] names(st16) &lt;- c(&quot;depth&quot;, &quot;sources&quot;) with(st16, plot(sources ~ depth)) Fit a loess smoother using the factory settings: st16.lo &lt;- loess(sources ~ depth, data = st16) summary(st16.lo) ## Call: ## loess(formula = sources ~ depth, data = st16) ## ## Number of Observations: 51 ## Equivalent Number of Parameters: 4.33 ## Residual Standard Error: 4.18 ## Trace of smoother matrix: 4.73 (exact) ## ## Control settings: ## span : 0.75 ## degree : 2 ## family : gaussian ## surface : interpolate cell = 0.2 ## normalize: TRUE ## parametric: FALSE ## drop.square: FALSE Plot the fit, this takes a little work depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(object = st16.lo, newdata = depth.vals, se = TRUE) with(st16, plot(sources ~ depth)) lines(x = depth.vals, y = st16.fit$fit, col = &quot;blue&quot;) # add 95% error bars lines(x = depth.vals, y = st16.fit$fit + st16.fit$se.fit * qt(p = .975, df = st16.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - st16.fit$se.fit * qt(p = .975, df = st16.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Examine the residuals: ## see what the fit returns; maybe the residuals are already there names(st16.lo) # they are! ## [1] &quot;n&quot; &quot;fitted&quot; &quot;residuals&quot; &quot;enp&quot; &quot;s&quot; &quot;one.delta&quot; ## [7] &quot;two.delta&quot; &quot;trace.hat&quot; &quot;divisor&quot; &quot;robust&quot; &quot;pars&quot; &quot;kd&quot; ## [13] &quot;call&quot; &quot;terms&quot; &quot;xnames&quot; &quot;x&quot; &quot;y&quot; &quot;weights&quot; plot(st16.lo$residuals ~ st16$depth) abline(h = 0, lty = &quot;dotted&quot;) Let’s look at how changing the span changes the fit. We’ll write a custom function to fit a LOESS curve, and then call the function with various values for the span. PlotLoessFit &lt;- function(x, y, return.fit = FALSE, ...){ # Caluclates a loess fit with the &#39;loess&#39; function, and makes a plot # # Args: # x: predictor # y: response # return.fit: logical # ...: Optional arguments to loess # # Returns: # the loess fit my.lo &lt;- loess(y ~ x, ...) x.vals &lt;- seq(from = min(x), to = max(x), length = 100) my.fit &lt;- predict(object = my.lo, newdata = x.vals, se = TRUE) plot(x, y) lines(x = x.vals, y = my.fit$fit, col = &quot;blue&quot;) lines(x = x.vals, y = my.fit$fit + my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = x.vals, y = my.fit$fit - my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) if (return.fit) { return(my.lo) } } Now we’ll call the function several times, each time chanigng the value of the span argument to the loess function: PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.5) PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.25) PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.1) Let’s try a loess fit with a locally linear regression: PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.25, degree = 1) 4.2 Splines We’ll use the gam function in the mgcv package to fit splines and additive models. The name of the package is an acronym for “Mixed GAM Computation Vehicle”. GAM is an acronym for Generalized Additive Model. Warning. I do not understand much of the functionality of mgcv::gam. What follows is my best guess of how the procedure works. The code below fits a regression spline to the bioluminescence data. Actually, the code fits an additive model with the spline as the only predictor. We will say more about additive models later. For now, it is sufficient to think about an additive model as a type of regression in which the linear effect of the predictor has been replaced by a spline. In other words, in terms of a word equation, the model can be represented as \\[ \\mbox{response = intercept + spline + error} \\] The s() component of the model formula designates a spline, and specifies details about the particular type of spline to be fit. The fx = TRUE component of the formula indicates that the amount of smoothing is fixed. The default value for the fx argument is fx = FALSE, in which case the amount of smoothing is determined by (generalized) cross-validation. When fx = TRUE, the parameter k determines the dimensionality (degree of flexibility) of the spline. Larger values of k correspond to greater flexibility, and a less smooth fit. I think that the number of knots is \\(k-4\\), such that setting \\(k=4\\) fits a familiar cubic polynomial with no knots. Setting \\(k=5\\) then fits a regression spline with one knot, etc. I have not been able to figure out where the knots are placed. In any case, we’ll fit a regression spline with two knots: library(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-42. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. st16.rspline &lt;- mgcv::gam(sources ~ s(depth, k = 6, fx = TRUE), data = st16) plot(st16.rspline, se = TRUE) Note that the plot includes only the portion of the model attributable to the covariate effect. This is because we have actually fit an additive model (e.g., a GAM). The plot shows only the spline component, which thus does not include the intercept. To visualize the fit, we’ll need to do a bit more work. with(st16, plot(sources ~ depth)) st16.fit &lt;- predict(st16.rspline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st16.fit$fit) ## add +/- 2 SE following Zuur; this is only approximate. ## should probably use a critical value from a t-dist with n - edf df, that is, 51 - 5 = 46 df lines(x = depth.vals, y = st16.fit$fit + 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) We see that this particular fit is not flexible enough to capture the trend in luminescence at low depth. Let’s take a look at the information produced by a call to summary: summary(st16.rspline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sources ~ s(depth, k = 6, fx = TRUE) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4771 0.5858 21.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 5 5 122.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.924 Deviance explained = 93.2% ## GCV = 19.837 Scale est. = 17.503 n = 51 This summary requires a bit more explanation as well. In this GAM, the spline component of the model effectively creates a set of new predictor variables. A regression spline with \\(x\\) knots requires \\(x+3\\) new regression predictors to fit the spline. In this fit, there are two knots, so the spline requires 5 new predictor variables. Because the predictors are determined in advance with regression splines, we can use the usual theory of \\(F\\)-tests from regression to assess the statistical significance of the spline terms. In the section of the output labeled “Approximate significance of smooth terms”, we see that these 5 predictors together provide a significantly better fit than a model that does not include the spline. I believe this test is actually exact. I think that it is labeled “approximate” because the default behavior of mgcv::gam is to fit a smoothing spline, for which the test is indeed only approximate. We’ll discuss this more when we study a smoothing spline fit. Now we’ll fit and plot a smoothing spline. A smoothing spline differs from a regression spline by using generalized cross-validation to determine the appropriate smoothness. st16.spline &lt;- mgcv::gam(sources ~ s(depth), data = st16) plot(st16.spline, se = TRUE) # note that the plot does not include the intercept Again, we make a plot that includes both the points and the fit with(st16, plot(sources ~ depth)) st16.fit &lt;- predict(st16.spline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st16.fit$fit) ## add +/- 2 SE following Zuur; this is only approximate. ## should probably use a critical value from a t-dist with n - edf df, that is, 51 - 9.81 = 41.19 df lines(x = depth.vals, y = st16.fit$fit + 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) Let’s ask for a summary: summary(st16.spline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sources ~ s(depth) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4771 0.3921 31.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 8.813 8.99 158.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.966 Deviance explained = 97.2% ## GCV = 9.7081 Scale est. = 7.8402 n = 51 Note especially the edf component in the “Approximate significance of smooth terms” section. The label edf stands for effective degrees of freedom. We can think of the edf as the effective number of new predictors that have been added to the model to accommodate the spline. For a smoothing spline, the number and values of the newly created predictors are determined by fitting the model to the data. Because the predictors are calculated in this way, the usual theory of \\(F\\)-testing does not apply. This is why the \\(F\\)-test shown for the smoothing spline is labeled as “approximate”. Find the AIC for the smoothing spline fit: AIC(st16.spline) ## [1] 260.4811 Here’s a small detail. Notice that the syntax of the call to predict is slightly different when making a prediction for a loess object vs. making a prediction for a gam object (which the spline fit is). For a call to predict with a loess object, the new predictor values can be provided in the form of a vector. So, we were able to use depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(object = st16.lo, newdata = depth.vals, se = TRUE) However, for a call to predict with a gam object, the new predictor values must be provided in the form of a new data frame, with variable names that match the variables in the gam model. So, to get predicted values for the spline fit, we needed to use the more cumbersome depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(st16.spline, newdata = data.frame(depth = depth.vals), se = TRUE) 4.3 Generalized additive models (GAMs) Generalized additive models replace the usual linear terms that appear in multiple regression models with splines. That is, suppose we seek to model the relationship between a response \\(y\\) and two predictors, \\(x_1\\) and \\(x_2\\). A standard regression model without polynomial effects or interactions would be written as \\[ y = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + \\varepsilon \\] where \\(\\varepsilon\\) is assumed to be an iid Gaussian random variate with variance \\(\\sigma^2_\\varepsilon\\). This is an additive model, in the sense that the combined effects of the two predictors equal the sum of their individual effects. A generalized additive model (GAM) replaces the individual regression terms with splines. Continuing with the generic example, a GAM would instead model the effects of the two predictors as \\[ y = \\beta_0 + s(x_1) +s(x_2) + \\varepsilon \\] where \\(s(\\cdot)\\) represents a spline. We continue to assume that, conditional on the covariate effects, the responses are normally distributed with constant variance \\(\\sigma^2_\\varepsilon\\). We will illustrate additive modeling using the bird data found in Appendix A of Zuur et al. (2009). Zuur et al. report that these data originally appeared in Loyn (1987) and were featured in Quinn &amp; Keough (2002)’s text. Zuur et al. describe these data in the following way: “Forest bird densities were measured in 56 forest patches in south-eastern Victoria, Australia. The aim of the study was to relate bird densities to six habitat variables; size of the forest patch, distance to the nearest patch, distance to the nearest larger patch, mean altitude of the patch, year of isolation by clearing, and an index of stock grazing history (1 = light, 5 = intensive).” We first read the data and perform some light exploratory analysis and housekeeping. rm(list = ls()) require(mgcv) bird &lt;- read.table(&quot;data/Loyn.txt&quot;, head = T) summary(bird) ## Site ABUND AREA DIST ## Min. : 1.00 Min. : 1.50 Min. : 0.10 Min. : 26.0 ## 1st Qu.:14.75 1st Qu.:12.40 1st Qu.: 2.00 1st Qu.: 93.0 ## Median :28.50 Median :21.05 Median : 7.50 Median : 234.0 ## Mean :28.50 Mean :19.51 Mean : 69.27 Mean : 240.4 ## 3rd Qu.:42.25 3rd Qu.:28.30 3rd Qu.: 29.75 3rd Qu.: 333.2 ## Max. :56.00 Max. :39.60 Max. :1771.00 Max. :1427.0 ## LDIST YR.ISOL GRAZE ALT ## Min. : 26.0 Min. :1890 Min. :1.000 Min. : 60.0 ## 1st Qu.: 158.2 1st Qu.:1928 1st Qu.:2.000 1st Qu.:120.0 ## Median : 338.5 Median :1962 Median :3.000 Median :140.0 ## Mean : 733.3 Mean :1950 Mean :2.982 Mean :146.2 ## 3rd Qu.: 913.8 3rd Qu.:1966 3rd Qu.:4.000 3rd Qu.:182.5 ## Max. :4426.0 Max. :1976 Max. :5.000 Max. :260.0 # get rid of the &#39;Site&#39; variable; it is redundant with the row label bird &lt;- bird[, -1] # log-transform area, distance, ldistance, to remove right-skew bird$L.AREA &lt;- log(bird$AREA) bird$L.DIST &lt;- log(bird$DIST) bird$L.LDIST &lt;- log(bird$LDIST) # change YR.ISOL to years since isolation (study was published in 1987) bird$YR.ISOL &lt;- 1987 - bird$YR.ISOL # keep the only the variables we want bird &lt;- bird[, c(&quot;ABUND&quot;, &quot;L.AREA&quot;, &quot;L.DIST&quot;, &quot;L.LDIST&quot;, &quot;YR.ISOL&quot;, &quot;ALT&quot;, &quot;GRAZE&quot;)] summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 ## Median :24.50 Median :140.0 Median :3.000 ## Mean :37.25 Mean :146.2 Mean :2.982 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 ## Max. :97.00 Max. :260.0 Max. :5.000 Our first attempt at a GAM will entertain smoothing splines for all of the continuous predictors in the model. We will use a linear term for GRAZE because there are too few unique values to support a smooth term: bird.gam1 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + s(ALT), data = bird) summary(bird.gam1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + ## s(ALT) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.4443 2.7798 9.153 9.42e-12 *** ## GRAZE -1.9885 0.8968 -2.217 0.0318 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.446 3.089 12.635 3.98e-06 *** ## s(L.DIST) 3.693 4.559 0.855 0.461 ## s(L.LDIST) 1.000 1.000 0.386 0.538 ## s(YR.ISOL) 1.814 2.238 1.231 0.262 ## s(ALT) 1.000 1.000 0.629 0.432 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.72 Deviance explained = 77.6% ## GCV = 40.987 Scale est. = 32.238 n = 56 The output reports the partial regression coefficient for the lone quantitative predictor GRAZE, and approximate significance tests for the smooth terms for each of the other predictors. We can visualize these smooth terms with a call to plot: plot(bird.gam1) In the interest of time, we take a casual approach to variable selection here. We’ll drop smooth terms that are clearly not significant to obtain: bird.gam2 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + GRAZE, data = bird) summary(bird.gam2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + GRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.400 2.201 12.903 &lt; 2e-16 *** ## GRAZE -2.980 0.686 -4.344 6.56e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.284 2.903 13.18 3.4e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.68 Deviance explained = 69.9% ## GCV = 39.992 Scale est. = 36.932 n = 56 plot(bird.gam2) Note that the GRAZE variable is currently treated as a numerical predictor. We’ll try fitting a model with GRAZE as a factor. First we’ll create a new variable that treats GRAZE as a factor. We’ll use the summary command to confirm that the new variable fGRAZE is indeed a factor. bird$fGRAZE &lt;- as.factor(bird$GRAZE) summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE fGRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 1:13 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 2: 8 ## Median :24.50 Median :140.0 Median :3.000 3:15 ## Mean :37.25 Mean :146.2 Mean :2.982 4: 7 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 5:13 ## Max. :97.00 Max. :260.0 Max. :5.000 Now we’ll proceed to fit the model bird.gam3 &lt;- gam(ABUND ~ s(L.AREA) + fGRAZE, data = bird) plot(bird.gam3) summary(bird.gam3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + fGRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.727275 1.944080 11.691 1.11e-15 *** ## fGRAZE2 0.006623 2.845343 0.002 0.998152 ## fGRAZE3 -0.660124 2.585878 -0.255 0.799592 ## fGRAZE4 -2.170994 3.050736 -0.712 0.480122 ## fGRAZE5 -11.913966 2.872911 -4.147 0.000136 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.761 3.478 11.67 4.71e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.723 Deviance explained = 75.7% ## GCV = 37.013 Scale est. = 31.883 n = 56 To formally compare the models with GRAZE as a numerical vs. categorical predictor, we’ll have to use AIC. We can’t use an \\(F\\)-test here because we have used smoothing splines to capture the effect of L.AREA. Thus, the models are not nested. (If we had used regression splines for L.AREA, then the models would have been nested.) We can extract the AICs for these models by a simple call to the AIC function. AIC(bird.gam2) ## [1] 367.1413 AIC(bird.gam3) ## [1] 361.9655 We can see the contrasts used to incorporate the factor fGRAZE in the model by a call to contrasts: with(bird, contrasts(fGRAZE)) ## 2 3 4 5 ## 1 0 0 0 0 ## 2 1 0 0 0 ## 3 0 1 0 0 ## 4 0 0 1 0 ## 5 0 0 0 1 The output here is somewhat opaque because the levels of fGRAZE are 1, 2, \\(\\ldots\\), 5. The output of the call to contrasts shows each of the newly created indicator variables as a column. For example, the first column shows that the predictor named fGRAZE2 takes the value of 1 when the variable fGRAZE equals 2, and is 0 otherwise. Fit an additive model with only a smooth effect of L.AREA, in order to show residuals vs. GRAZE: bird.gam4 &lt;- gam(ABUND ~ s(L.AREA), data = bird) plot(x = bird$GRAZE, y = bird.gam4$residuals) abline(h = 0, lty = &quot;dashed&quot;) Both the plot and the model output suggest that the effect of grazing is primarily due to lower bird abundance in the most heavily grazed category. To conclude, we’ll conduct a formal test of whether the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE. In this case, we have to use regression splines for the smooth effect of L.AREA. We’ll use regression “splines” without any internal knots, (which are actually not splines at all, just a cubic trend) because the effect of log area seems to be reasonably well captured by a cubic trend anyway: bird.gam5 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE, data = bird) bird.gam6 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE, data = bird) anova(bird.gam5, bird.gam6, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE ## Model 2: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 51 1869.0 ## 2 48 1543.1 3 325.93 3.3796 0.02565 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both AIC and the \\(F\\)-test suggest that the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE (\\(F_{3,48} = 3.38, p = 0.026\\)). As a final note, Zuur et al. (p.550) observe that “the non-linear L.AREA effect is mainly due to two large patches. It would be useful to sample more of this type of patch in the future.” (Note the rug plots in any of the plots of the area effect above.) "],["generalized-least-squares.html", "Chapter 5 Generalized Least Squares 5.1 Heterogeneous variance 5.2 Temporal (serial) correlation 5.3 Spatial data", " Chapter 5 Generalized Least Squares 5.1 Heterogeneous variance We will illustrate generalized least squares (GLS) using a data set that gives the percentage of male births for four countries (Canada, Denmark, the Netherlands, and the US) for several decades in the late twentieth century. The data were originally reported in Davis et al., JAMA 279:1018–1023 (1998). The data set that we will work with was scraped from this publication by Ramsey and Schafer for their book “The Statistical Sleuth” (2e, 2002). The data can be found as the data set ‘ex0726’ in the r library ‘sleuth2’. We will begin by reading the data and performing some housekeeping. #-------------------- # Ex 07.26 from the Statistical Sleuth, 2e #-------------------- library(Sleuth2) str(ex0726) ## &#39;data.frame&#39;: 45 obs. of 5 variables: ## $ Year : num 1950 1951 1952 1953 1954 ... ## $ Denmark : num 0.512 0.517 0.515 0.517 0.515 ... ## $ Netherlands: num 0.516 0.516 0.516 0.516 0.516 ... ## $ Canada : num NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... ## $ Usa : num NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... births &lt;- ex0726 require(reshape2) ## Loading required package: reshape2 names(births) &lt;- c(&quot;year&quot;, &quot;DK&quot;, &quot;NL&quot;, &quot;CA&quot;, &quot;US&quot;) births.melt &lt;- melt(births, id.vars = c(&quot;year&quot;)) births &lt;- births.melt rm(births.melt) names(births) &lt;- c(&quot;year&quot;, &quot;country&quot;, &quot;pct.male&quot;) births$pct.male &lt;- 100 * births$pct.male We will focus only on the years 1970 – 1990, for which data are available for all countries: births &lt;- subset(births, year &gt;= 1970 &amp; year &lt;= 1990) summary(births) ## year country pct.male ## Min. :1970 DK:21 Min. :50.87 ## 1st Qu.:1975 NL:21 1st Qu.:51.22 ## Median :1980 CA:21 Median :51.28 ## Mean :1980 US:21 Mean :51.30 ## 3rd Qu.:1985 3rd Qu.:51.38 ## Max. :1990 Max. :51.73 head(births) ## year country pct.male ## 21 1970 DK 51.40 ## 22 1971 DK 51.70 ## 23 1972 DK 51.26 ## 24 1973 DK 51.33 ## 25 1974 DK 51.27 ## 26 1975 DK 51.08 Let’s have a look at the time trends in percentage male births in each of the four countries: par(mfrow = c(2, 2), las = 1) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Canada&quot;)) with(subset(births, country == &quot;CA&quot;), points(pct.male ~ year)) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;USA&quot;)) with(subset(births, country == &quot;US&quot;), points(pct.male ~ year)) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Denmark&quot;)) with(subset(births, country == &quot;DK&quot;), points(pct.male ~ year)) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Netherlands&quot;)) with(subset(births, country == &quot;NL&quot;), points(pct.male ~ year)) For these data, we might want to ask: Is there evidence that the percentage of male births is changing through time? If so, does the rate of change differ among countries? Among continents? These are the types of questions that we would usually address with a regression model. However, there’s a lot going on with these data that would cause us to question the appropriateness of the usual ordinary least squares (OLS) assumptions. The responses are proportions. We know that when the response is a proportion, the variance in the response depends on the mean, with the variance decreasing as the mean approaches 0% or 100%, and obtaining its maximal value when the mean response is at 50%. For these data, however, all of the responses are sufficiently close to 50% that we don’t need to worry about heterogeneous variances that arise from the proportional nature of the response. The variance of the response also depends inversely on the number of births. Evidently, this will be a major issue, because these countries differ substantially in the sizes of their populations. If we knew the number of births in each country in each year (in other words, if we knew the denominator of the each data point), then could account for these differences using grouped logistic regression. However, the data as we have them do not contain any information about the number of births that underlie each data point. So, we will need a different approach to deal with the heterogeneous variances among countries. The data are time series. We will devote our attention to time-series data more fully later in the course. For now, it suffices to realize that time series data are typically autocorrelated. In other words, the residual errors for consecutive data points are often correlated (and usually positively correlated), and this correlation typically decays as the time between data points increases. For these data, it is less clear why the errors might be autocorrelated, but we want to allow for the possibility all the same. We’ll regress the percentage of male births on year and country. Following Zuur et al.’s good advice, we’ll begin with a model with richly specified fixed effects. In this case, that means country specific intercepts and slopes. In an equation, this model is \\[\\begin{equation} y_{it} = a_i + b_i x_{it} + \\varepsilon_{it} \\end{equation}\\] where \\(i = 1, \\ldots, 4\\) is an index that distinguishes among the four countries, and \\(t = 1, \\ldots, 21\\) is an index that distinguishes among the 21 years. The response \\(y_{it}\\) is the percentage of male births in country \\(i\\) in year \\(t\\), \\(x_{it}\\) is the year to which measurement \\(y_{it}\\) corresponds, the \\(a_i\\) are the country-specific intercepts, the \\(b_i\\) are the country-specific slopes, and the \\(\\varepsilon_{it}\\)’s are the errors. To begin, we make the usual OLS assumption that the errors are iid, that is, \\(\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma^2)\\). fm1 &lt;- with(births, lm(pct.male ~ year * country)) summary(fm1) ## ## Call: ## lm(formula = pct.male ~ year * country) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.30707 -0.05931 0.00100 0.04787 0.35314 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.6995238 9.0048249 5.519 4.53e-07 *** ## year 0.0008442 0.0045479 0.186 0.8532 ## countryNL 14.4252381 12.7347455 1.133 0.2609 ## countryCA 23.6790476 12.7347455 1.859 0.0668 . ## countryUS 12.3090476 12.7347455 0.967 0.3368 ## year:countryNL -0.0073636 0.0064317 -1.145 0.2558 ## year:countryCA -0.0119610 0.0064317 -1.860 0.0668 . ## year:countryUS -0.0062727 0.0064317 -0.975 0.3325 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1262 on 76 degrees of freedom ## Multiple R-squared: 0.3052, Adjusted R-squared: 0.2412 ## F-statistic: 4.768 on 7 and 76 DF, p-value: 0.0001753 Inconveniently, the intercepts here refer to the percentage of male births extrapolated back to the year 1 BCE. That’s not very useful, so we’ll center the year predictor. Now, the predictor \\(x_{it}\\) will refer to the number of years before or after 1980, and the intercepts will give the fitted percentage of male births in the year 1980. births$yr.ctr &lt;- births$year - 1980 fm1 &lt;- with(births, lm(pct.male ~ yr.ctr * country)) summary(fm1) ## ## Call: ## lm(formula = pct.male ~ yr.ctr * country) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.30707 -0.05931 0.00100 0.04787 0.35314 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.3709524 0.0275387 1865.408 &lt; 2e-16 *** ## yr.ctr 0.0008442 0.0045479 0.186 0.85324 ## countryNL -0.1547619 0.0389456 -3.974 0.00016 *** ## countryCA -0.0038095 0.0389456 -0.098 0.92234 ## countryUS -0.1109524 0.0389456 -2.849 0.00564 ** ## yr.ctr:countryNL -0.0073636 0.0064317 -1.145 0.25584 ## yr.ctr:countryCA -0.0119610 0.0064317 -1.860 0.06680 . ## yr.ctr:countryUS -0.0062727 0.0064317 -0.975 0.33251 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1262 on 76 degrees of freedom ## Multiple R-squared: 0.3052, Adjusted R-squared: 0.2412 ## F-statistic: 4.768 on 7 and 76 DF, p-value: 0.0001753 Let’s plot the percentage of male births vs. year for each country, and overlay the fit of regression lines. To make it easy to extract the country-specific slopes and intercepts, we’ll first re-fit the model without the global intercept: fm1a &lt;- with(births, lm(pct.male ~ country + yr.ctr:country- 1)) summary(fm1a) ## ## Call: ## lm(formula = pct.male ~ country + yr.ctr:country - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.30707 -0.05931 0.00100 0.04787 0.35314 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## countryDK 51.3709524 0.0275387 1865.408 &lt;2e-16 *** ## countryNL 51.2161905 0.0275387 1859.788 &lt;2e-16 *** ## countryCA 51.3671429 0.0275387 1865.270 &lt;2e-16 *** ## countryUS 51.2600000 0.0275387 1861.379 &lt;2e-16 *** ## countryDK:yr.ctr 0.0008442 0.0045479 0.186 0.8532 ## countryNL:yr.ctr -0.0065195 0.0045479 -1.434 0.1558 ## countryCA:yr.ctr -0.0111169 0.0045479 -2.444 0.0168 * ## countryUS:yr.ctr -0.0054286 0.0045479 -1.194 0.2363 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1262 on 76 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 1.735e+06 on 8 and 76 DF, p-value: &lt; 2.2e-16 There’s probably a more elegant way to extract the slope and intercept, but we’ll use the crude approach for now. par(mfrow = c(2, 2), las = 1) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Canada&quot;)) with(subset(births, country == &quot;CA&quot;), points(pct.male ~ year)) abline(a = 51.3671 - (-0.01112 * 1980), b = -0.01112) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;USA&quot;)) with(subset(births, country == &quot;US&quot;), points(pct.male ~ year)) abline(a = 51.26 - (-0.0054286 * 1980), b = -0.0054286) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Denmark&quot;)) with(subset(births, country == &quot;DK&quot;), points(pct.male ~ year)) abline(a = 51.3709 - (0.0008442 * 1980), b = 0.0008442) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Netherlands&quot;)) with(subset(births, country == &quot;NL&quot;), points(pct.male ~ year)) abline(a = 51.2162 - (-0.00652 * 1980), b = -0.0065195) We would like to draw inferences about the time and country effects. However, the error variance clearly differs among the countries, because of the different sizes of the countries’ populations. Thus, we can’t trust the usual inference procedures that assume iid errors. We will cope by fitting a GLS model that allows the error variances to differ among the countries. The model equation is nearly the same as above: \\[\\begin{equation} y_{it} = a_i + b_i x_{it} + \\varepsilon_{it}. \\end{equation}\\] The only difference is that now we assume that the variance of the errors differs among the countries: \\(\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma^2_i)\\). This change looks trivial in the notation, but it’s an important change to the model! require(nlme) ## Loading required package: nlme gls1 &lt;- gls(pct.male ~ yr.ctr * country, data = births, weights = varIdent(form = ~ 1 | country)) summary(gls1) ## Generalized least squares fit by REML ## Model: pct.male ~ yr.ctr * country ## Data: births ## AIC BIC logLik ## -94.69995 -66.73115 59.34998 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7264997 0.3971728 0.1347962 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.04219635 1217.4264 0.0000 ## yr.ctr 0.00084 0.00696850 0.1211 0.9039 ## countryNL -0.15476 0.05215650 -2.9673 0.0040 ## countryCA -0.00381 0.04540269 -0.0839 0.9334 ## countryUS -0.11095 0.04257798 -2.6059 0.0110 ## yr.ctr:countryNL -0.00736 0.00861336 -0.8549 0.3953 ## yr.ctr:countryCA -0.01196 0.00749801 -1.5952 0.1148 ## yr.ctr:countryUS -0.00627 0.00703152 -0.8921 0.3752 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA cntrUS yr.:NL yr.:CA ## yr.ctr 0.000 ## countryNL -0.809 0.000 ## countryCA -0.929 0.000 0.752 ## countryUS -0.991 0.000 0.802 0.921 ## yr.ctr:countryNL 0.000 -0.809 0.000 0.000 0.000 ## yr.ctr:countryCA 0.000 -0.929 0.000 0.000 0.000 0.752 ## yr.ctr:countryUS 0.000 -0.991 0.000 0.000 0.000 0.802 0.921 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.18586069 -0.69331440 -0.01165266 0.66706364 1.82625135 ## ## Residual standard error: 0.193368 ## Degrees of freedom: 84 total; 76 residual Notice that the model estimates separate error standard deviations for each country. We would like to ask if model with country-specific variances provides a statistically significant improvement in fit relative to the model with homogeneous error variances. Here, it is crucial to remember that the default fitting scheme in nlme::gls is REML. However, because the models share the same fixed-effect structure, we can compare AIC values from the REML fits directly. Further, because the modes are nested, we can use the REML fits for a likelihood ratio test. The anova.gls command provides both. gls0 &lt;- gls(pct.male ~ yr.ctr * country, data = births) # OLS fit anova(gls0, gls1) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls0 1 9 -42.18264 -21.20604 30.09132 ## gls1 2 12 -94.69995 -66.73115 59.34998 1 vs 2 58.51731 &lt;.0001 Both the LRT and the AIC suggest that the GLS model with country-specific variances provides a statistically significant improvement over the OLS model with homogeneous error variances. If the fixed-effect structures had not been the same, it would not have been correct to compare the models using the REML fits. Instead, we would have to re-fit the models using ML. For the sake of illustration, let’s do this anyway, and see if and how the results differ. gls0ML &lt;- gls(pct.male ~ yr.ctr * country, data = births, method = &quot;ML&quot;) gls1ML &lt;- gls(pct.male ~ yr.ctr * country, data = births, weights = varIdent(form = ~ 1 | country), method = &quot;ML&quot;) anova(gls0ML, gls1ML) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls0ML 1 9 -99.76871 -77.89136 58.88435 ## gls1ML 2 12 -158.44573 -129.27593 91.22287 1 vs 2 64.67702 &lt;.0001 We obtain somewhat different numerical results based on the ML fit, even though the qualitative outcome of the test is unchanged (the model with country-specific variances is still strongly favored). In any event, we can also compare the variance estimates from the ML fit to those from the REML fit. summary(gls1ML) ## Generalized least squares fit by maximum likelihood ## Model: pct.male ~ yr.ctr * country ## Data: births ## AIC BIC logLik ## -158.4457 -129.2759 91.22287 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7264997 0.3971729 0.1347962 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.04219635 1217.4265 0.0000 ## yr.ctr 0.00084 0.00696850 0.1211 0.9039 ## countryNL -0.15476 0.05215650 -2.9673 0.0040 ## countryCA -0.00381 0.04540269 -0.0839 0.9334 ## countryUS -0.11095 0.04257798 -2.6059 0.0110 ## yr.ctr:countryNL -0.00736 0.00861336 -0.8549 0.3953 ## yr.ctr:countryCA -0.01196 0.00749801 -1.5952 0.1148 ## yr.ctr:countryUS -0.00627 0.00703152 -0.8921 0.3752 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA cntrUS yr.:NL yr.:CA ## yr.ctr 0.000 ## countryNL -0.809 0.000 ## countryCA -0.929 0.000 0.752 ## countryUS -0.991 0.000 0.802 0.921 ## yr.ctr:countryNL 0.000 -0.809 0.000 0.000 0.000 ## yr.ctr:countryCA 0.000 -0.929 0.000 0.000 0.000 0.752 ## yr.ctr:countryUS 0.000 -0.991 0.000 0.000 0.000 0.802 0.921 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.29802807 -0.72889177 -0.01225061 0.70129397 1.91996548 ## ## Residual standard error: 0.1839296 ## Degrees of freedom: 84 total; 76 residual Note that the estimate of the residual standard deviation (mislabeled as the “residual standard error” in the R output) is smaller for the ML fit than for the REML fit, as we expect. To continue, we can also fit a first-order autoregressive correlation structure to the residual errors within each country. Here, because the data are evenly spaced and are already sorted in the data set, it’s simple to add the within country autocorrelation. To write this model as an equation, the fixed-effect specification remains unchanged: \\[\\begin{equation} y_{it} = a_i + b_i x_{it} + \\varepsilon_{it}. \\end{equation}\\] The marginal distribution of the errors is also unchanged: \\(\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma^2_i)\\). However, the within-country errors are now correlated: \\[\\begin{equation} \\mathrm{Corr}(\\varepsilon_{it_1}, \\varepsilon_{jt_2}) = \\begin{cases} \\rho^{|t_1 - t_2|} &amp; i = j \\\\ 0 &amp; i \\neq j \\end{cases} \\end{equation}\\] gls2 &lt;- gls(pct.male ~ yr.ctr * country, data = births, weights = varIdent(form = ~ 1 | country), correlation = corAR1(form = ~ 1 | country)) summary(gls2) ## Generalized least squares fit by REML ## Model: pct.male ~ yr.ctr * country ## Data: births ## AIC BIC logLik ## -93.01222 -62.71269 59.50611 ## ## Correlation Structure: AR(1) ## Formula: ~1 | country ## Parameter estimate(s): ## Phi ## 0.07081995 ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7416576 0.4009707 0.1338036 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37134 0.04520738 1136.3486 0.0000 ## yr.ctr 0.00088 0.00741198 0.1187 0.9058 ## countryNL -0.15484 0.05628375 -2.7510 0.0074 ## countryCA -0.00385 0.04870615 -0.0791 0.9371 ## countryUS -0.11127 0.04561027 -2.4396 0.0170 ## yr.ctr:countryNL -0.00713 0.00922801 -0.7727 0.4421 ## yr.ctr:countryCA -0.01188 0.00798562 -1.4872 0.1411 ## yr.ctr:countryUS -0.00634 0.00747803 -0.8481 0.3991 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA cntrUS yr.:NL yr.:CA ## yr.ctr 0.000 ## countryNL -0.803 0.000 ## countryCA -0.928 0.000 0.746 ## countryUS -0.991 0.000 0.796 0.920 ## yr.ctr:countryNL 0.000 -0.803 0.000 0.000 0.000 ## yr.ctr:countryCA 0.000 -0.928 0.000 0.000 0.000 0.746 ## yr.ctr:countryUS 0.000 -0.991 0.000 0.000 0.000 0.796 0.920 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.15117280 -0.70400503 -0.02488281 0.66159776 1.82002897 ## ## Residual standard error: 0.1936784 ## Degrees of freedom: 84 total; 76 residual The estimate of the within-country correlation is small: only 0.071. The model with autocorrelated errors is nested within the model with heterogeneous variances, and both have the same fixed-effect structure, so we can compare the two REML fits directly: anova(gls0, gls1, gls2) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls0 1 9 -42.18264 -21.20604 30.09132 ## gls1 2 12 -94.69995 -66.73115 59.34998 1 vs 2 58.51731 &lt;.0001 ## gls2 3 13 -93.01222 -62.71269 59.50611 2 vs 3 0.31227 0.5763 By either AIC or the LRT, the model with the autocorrelated errors does not provide a statistically significant improvement in fit. We can now use the model with heterogeneous variances and independent errors to conduct the usual inferences on the fixed effects. Because we now compare models with different fixed-effect structures, we must work on the ML fits. Let’s start with a model that removes the interaction between time and country. The fixed-effects component of this model is: \\[\\begin{equation} y_{it} = a_i + b x_{it} + \\varepsilon_{it}. \\end{equation}\\] In other words, there is a common slope among the countries. gls3ML &lt;- gls(pct.male ~ yr.ctr + country, data = births, weights = varIdent(form = ~ 1 | country), method = &quot;ML&quot;) summary(gls3ML) ## Generalized least squares fit by maximum likelihood ## Model: pct.male ~ yr.ctr + country ## Data: births ## AIC BIC logLik ## -159.5456 -137.6682 88.7728 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7098035 0.4232237 0.1323319 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.04238044 1212.1383 0.0000 ## yr.ctr -0.00585 0.00086365 -6.7731 0.0000 ## countryNL -0.15476 0.05197129 -2.9778 0.0039 ## countryCA -0.00381 0.04601974 -0.0828 0.9342 ## countryUS -0.11095 0.04274991 -2.5954 0.0113 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA ## yr.ctr 0.000 ## countryNL -0.815 0.000 ## countryCA -0.921 0.000 0.751 ## countryUS -0.991 0.000 0.808 0.913 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.32703514 -0.72533260 0.03426803 0.82167128 2.12375963 ## ## Residual standard error: 0.1883428 ## Degrees of freedom: 84 total; 79 residual anova(gls3ML, gls1ML) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls3ML 1 9 -159.5456 -137.6683 88.77280 ## gls1ML 2 12 -158.4457 -129.2759 91.22287 1 vs 2 4.900132 0.1793 Both AIC and the LRT favor a model with a common slope. Let’s go further to see if the intercepts differ among the countries. In other words, we can entertain the model \\[\\begin{equation} y_{it} = a + b x_{it} + \\varepsilon_{it}. \\end{equation}\\] gls4ML &lt;- gls(pct.male ~ yr.ctr, data = births, weights = varIdent(form = ~ 1 | country), method = &quot;ML&quot;) summary(gls4ML) ## Generalized least squares fit by maximum likelihood ## Model: pct.male ~ yr.ctr ## Data: births ## AIC BIC logLik ## -136.0564 -121.4715 74.0282 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.6559362 0.6051856 0.1159405 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.26375 0.005328997 9619.775 0 ## yr.ctr -0.00558 0.000880055 -6.335 0 ## ## Correlation: ## (Intr) ## yr.ctr 0 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.5381852 -0.3580929 0.1883530 0.9250208 2.3348069 ## ## Residual standard error: 0.2164103 ## Degrees of freedom: 84 total; 82 residual anova(gls4ML, gls3ML, gls1ML) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls4ML 1 6 -136.0564 -121.4715 74.02820 ## gls3ML 2 9 -159.5456 -137.6683 88.77280 1 vs 2 29.489202 &lt;.0001 ## gls1ML 3 12 -158.4457 -129.2759 91.22287 2 vs 3 4.900132 0.1793 There is strong evidence that the percentage of male births differs among countries, after accounting for the effect of the temporal trend. We can visualize the model by making scatterplots and overlaying fitted regression lines. Having finished with model selection, we’ll revert to the REML fits for final parameter estimation. Again, we’ll use the trick of eliminating the global intercept to make it easier to find the country-specific intercepts. gls3a &lt;- gls(pct.male ~ yr.ctr + country - 1, data = births, weights = varIdent(form = ~ 1 | country)) summary(gls3a) ## Generalized least squares fit by REML ## Model: pct.male ~ yr.ctr + country - 1 ## Data: births ## AIC BIC logLik ## -122.7459 -101.4209 70.37297 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7099907 0.4237511 0.1352729 ## ## Coefficients: ## Value Std.Error t-value p-value ## yr.ctr -0.00586 0.00087529 -6.700 0 ## countryDK 51.37095 0.04213583 1219.175 0 ## countryNL 51.21619 0.02991605 1711.997 0 ## countryCA 51.36714 0.01785511 2876.888 0 ## countryUS 51.26000 0.00569984 8993.240 0 ## ## Correlation: ## yr.ctr cntrDK cntrNL cntrCA ## countryDK 0 ## countryNL 0 0 ## countryCA 0 0 0 ## countryUS 0 0 0 0 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.26855300 -0.70087717 0.03371643 0.79968076 2.07208985 ## ## Residual standard error: 0.1930906 ## Degrees of freedom: 84 total; 79 residual par(mfrow = c(2, 2), las = 1) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Canada&quot;)) with(subset(births, country == &quot;CA&quot;), points(pct.male ~ year)) abline(a = 51.3671 + 0.00586 * 1980, b = -0.00586) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;USA&quot;)) with(subset(births, country == &quot;US&quot;), points(pct.male ~ year)) abline(a = 51.26 + 0.00586 * 1980, b = -0.00586) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Denmark&quot;)) with(subset(births, country == &quot;DK&quot;), points(pct.male ~ year)) abline(a = 51.371 + 0.00586 * 1980, b = -0.00586) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Netherlands&quot;)) with(subset(births, country == &quot;NL&quot;), points(pct.male ~ year)) abline(a = 51.2162 + 0.00586 * 1980, b = -0.00586) It is interesting to compare the estimate of the slope between the GLS model and the naive OLS fit. In the GLS model, the slope is estimated to be \\(-0.00586\\%\\) per year, with a standard error of \\(8.8 \\times 10^{-4}\\). In the OLS fit, the estimate is \\(-0.00555\\%\\) per year, with a standard error of \\(2.8 \\times 10^{-3}\\). Thus the GLS fit has substantially improved the precision of the estimate of the temporal trend. summary(gls(pct.male ~ yr.ctr + country, data = births)) ## Generalized least squares fit by REML ## Model: pct.male ~ yr.ctr + country ## Data: births ## AIC BIC logLik ## -70.12178 -55.9051 41.06089 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.02762942 1859.2846 0.0000 ## yr.ctr -0.00556 0.00228142 -2.4350 0.0171 ## countryNL -0.15476 0.03907390 -3.9607 0.0002 ## countryCA -0.00381 0.03907390 -0.0975 0.9226 ## countryUS -0.11095 0.03907390 -2.8396 0.0057 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA ## yr.ctr 0.000 ## countryNL -0.707 0.000 ## countryCA -0.707 0.000 0.500 ## countryUS -0.707 0.000 0.500 0.500 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.517325067 -0.553487986 0.009393865 0.508672668 3.142893232 ## ## Residual standard error: 0.1266139 ## Degrees of freedom: 84 total; 79 residual 5.2 Temporal (serial) correlation Temporal structure often induces a (positive) correlation between data points that occur close together in time. These are the same types of correlations that we would expect to find for any data that occur as part of a series, or serial correlation. (Other data types may display serial correlations that are not driven by time, such as positions along a one-dimensional spatial transect.) We will illustrate how to handle temporal correlations using a time series of annual moorhen abundance on the island of Kauai. These data are analyzed in Ch. 6 of Zuur et al. (2009), and are originally from Reed et al. (2007). The data are available for download from the website associated with Zuur et al.’s text. More details about the models available to handle serial correlations in nlme::gls can be found in \\(\\S\\) 5.3.1 of Pinheiro &amp; Bates (2000). First we load the data and do some housekeeping. rm(list = ls()) require(nlme) birds &lt;- read.table(&quot;data/Hawaii.txt&quot;, head = T) ## extract moorhen data moorhen &lt;- birds[, c(&quot;Year&quot;, &quot;Rainfall&quot;, &quot;Moorhen.Kauai&quot;)] ## rename variables names(moorhen) &lt;- c(&quot;year&quot;, &quot;rainfall&quot;, &quot;abundance&quot;) ## remove NAs moorhen &lt;- na.omit(moorhen) with(moorhen, plot(abundance ~ year)) with(moorhen, plot(log(abundance) ~ year)) with(moorhen, plot(log(abundance) ~ rainfall)) Suppose we want to characterize any possible (linear) temporal trend in moorhen abundance, and/or any association between moorhen abundance and annual rainfall. We log transform the abundance data to convert any multiplicative time trends into linear trends. First we will fit an OLS model and use the function acf to plot the autocorrelation function (ACF) of the residuals. fm1 &lt;- nlme::gls(log(abundance) ~ rainfall + year, data = moorhen) plot(residuals(fm1) ~ moorhen$year) acf(residuals(fm1)) The significant first-order autocorrelation suggests a first-order autoregressive model might be appropriate for these errors. We will fit such a model using the corAR1 correlation structure. In doing so, we use the formula form = ~ year to indicate that the year variable in the data set provides the time index. This is a necessary step with these data because some years are missing. fm2 &lt;- nlme::gls(log(abundance) ~ rainfall + year, data = moorhen, correlation = corAR1(form = ~ year)) summary(fm2) ## Generalized least squares fit by REML ## Model: log(abundance) ~ rainfall + year ## Data: moorhen ## AIC BIC logLik ## 124.6062 133.2946 -57.3031 ## ## Correlation Structure: ARMA(1,0) ## Formula: ~year ## Parameter estimate(s): ## Phi1 ## 0.5599778 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) -161.17809 32.93180 -4.894299 0.0000 ## rainfall -0.00783 0.01433 -0.546369 0.5877 ## year 0.08326 0.01663 5.005461 0.0000 ## ## Correlation: ## (Intr) ranfll ## rainfall -0.006 ## year -1.000 -0.001 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -3.3338721 -0.5125953 0.2117251 0.6813604 1.5181543 ## ## Residual standard error: 0.9112434 ## Degrees of freedom: 45 total; 42 residual The fit suggests that the residuals from adjacent years have a reasonably strong positive correlation of \\(\\approx 0.56\\). To see if the AR1 model has successfully accounted for the correlation structure in the residuals, we will inspect the “normalized” residuals (see the R help for residuals.gls for details). If all the structure in the residuals has been successfully accounted for, then the normalized residuals should look like iid draws from a standard Gaussian distribution. acf(residuals(fm2, type = &quot;normalized&quot;)) None of the autocorrelations among the normalized residuals differ significantly from zero. Finally, because the AR1 model nests the OLS model, we can use a LRT to inspect whether the first-order autoregression provides a significant improvement in fit. anova(fm1, fm2) ## Model df AIC BIC logLik Test L.Ratio p-value ## fm1 1 4 134.5734 141.5240 -63.28668 ## fm2 2 5 124.6062 133.2946 -57.30310 1 vs 2 11.96716 5e-04 The LRT suggests that the model with a first-order autocorrelation signficantly improves on the OLS model. We would then proceed to use this model to characterize the temporal trend in moorhen abundance, and the (lack of) association between moorhen abundance and rainfall. 5.3 Spatial data Data that are organized in space are also often correlated, with data points that occur close together in space being strongly (positively) correlated with one another. To illustrate spatial correlations, we will use the Wheat2 data provided as part of the nlme package. Pinheiro &amp; Bates (2000, p. 260) introduce the data as follows: “Stroup and Baenziger (1994) describe an agronomic experiment to compare the yield of 56 different varieties of wheat planted in four blocks arranged according to a randomized complete complete block design. All 56 varieties of wheat were used in each block. The latitude and longitude of each experimental unit in the trial were also recorded.” rm(list = ls()) data(&quot;Wheat2&quot;) summary(Wheat2) ## Block variety yield latitude longitude ## 4:56 ARAPAHOE : 4 Min. : 1.05 Min. : 4.30 Min. : 1.20 ## 2:56 BRULE : 4 1st Qu.:23.52 1st Qu.:17.20 1st Qu.: 7.20 ## 3:56 BUCKSKIN : 4 Median :26.85 Median :25.80 Median :14.40 ## 1:56 CENTURA : 4 Mean :25.53 Mean :27.22 Mean :14.08 ## CENTURK78: 4 3rd Qu.:30.39 3rd Qu.:38.70 3rd Qu.:20.40 ## CHEYENNE : 4 Max. :42.00 Max. :47.30 Max. :26.40 ## (Other) :200 A plot of the spatial locations of these data shows that the blocks hide a lot of information about the actual spatial position of the individual plots. While a traditional RCBD analysis might account for some of the spatial variation, we could perhaps do better by ignoring the block designations and modeling spatial correlations based on the actual location of each plot. with(Wheat2, plot(x = longitude, y = latitude, pch = as.numeric(Block))) Our goal is simply to characterizes the differences in mean yield among the 56 varieties while accounting for possible spatial correlations. We begin by fitting a simple one-factor ANOVA model and inspecting the residuals. First, we will use the plot_ly function to generate a three-dimensional view of the residuals. This 3D plot can be rotated in R, although the rotation is not possible in this Rbook. fm1 &lt;- nlme::gls(yield ~ variety, data = Wheat2) require(plotly) ## Loading required package: plotly ## Loading required package: ggplot2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout plot_ly(x = Wheat2$latitude, y = Wheat2$longitude, z = resid(fm1), type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, color = resid(fm1)) The residuals suggest a clear spatial trend in fertility. Next, We plot the semivariogram using nlme::Variogram. This command will actually plot the semivariance normalized by the sill, such that the quantity plotted is 1 minus the correlation between two points. In the plot below, the smooth curve is a loess curve fit to the calculated points. plot(Variogram(fm1, form = ~ latitude + longitude)) The semivariogram suggests a non-zero nugget. Here, we will fit spherical, Gaussian, and linear correlation models based on the latitude and longitude coordinates of each data point. For each fit, we will then plot a semivariogram of the normalized residuals. Again, if the model has done a good job accounting for the correlation structure in the data, then the normalized residuals should be independent. See \\(\\S\\) 5.3.2 of Pinheiro &amp; Bates (2000) for more details about the different spatial correlation structures available in nlme::gls. In particular, see their Fig. 5.9 for a display of how different spatial correlation models compare. For each model, we must supply starting values for the range and nugget. Rough starting values based on the semivariogram of the raw residuals will suffice. Calls to Variogram will plot the calculated semivariances and overlay the fitted semivariogram. ## spherical covariance fm2 &lt;- nlme::gls(yield ~ variety, data = Wheat2, correlation = corSpher(c(28, 0.2), form = ~ latitude + longitude, nugget = TRUE)) # need to supply starting values plot(Variogram(fm2, form = ~ latitude + longitude)) ## Gaussian covariance fm3 &lt;- nlme::gls(yield ~ variety, data = Wheat2, correlation = corGaus(c(28, 0.2), form = ~ latitude + longitude, nugget = TRUE)) # need to supply starting values plot(Variogram(fm3, form = ~ latitude + longitude)) ## linear covariance fm4 &lt;- nlme::gls(yield ~ variety, data = Wheat2, correlation = corLin(c(28, 0.2), form = ~ latitude + longitude, nugget = TRUE)) # need to supply starting values plot(Variogram(fm4, form = ~ latitude + longitude)) If we wish, we can extract the estimated nugget and range from each model by calling print. print(fm4) ## Generalized least squares fit by REML ## Model: yield ~ variety ## Data: Wheat2 ## Log-restricted-likelihood: -533.2815 ## ## Coefficients: ## (Intercept) varietyBRULE varietyBUCKSKIN varietyCENTURA ## 28.41005933 -1.71161334 6.99635483 -2.58344151 ## varietyCENTURK78 varietyCHEYENNE varietyCODY varietyCOLT ## -2.46550574 -3.07214768 -4.49895886 -2.22833768 ## varietyGAGE varietyHOMESTEAD varietyKS831374 varietyLANCER ## -4.26332881 -6.20641501 -1.18492850 -4.53944671 ## varietyLANCOTA varietyNE83404 varietyNE83406 varietyNE83407 ## -6.32251412 -2.92856070 -1.17290874 -2.62264363 ## varietyNE83432 varietyNE83498 varietyNE83T12 varietyNE84557 ## -5.65619040 2.20715030 -5.32860054 -5.50430895 ## varietyNE85556 varietyNE85623 varietyNE86482 varietyNE86501 ## -0.19445894 -3.51776928 -2.99576076 -2.22781284 ## varietyNE86503 varietyNE86507 varietyNE86509 varietyNE86527 ## -0.13437309 -0.49972260 -5.64308018 -1.70998353 ## varietyNE86582 varietyNE86606 varietyNE86607 varietyNE86T666 ## -4.52784048 -0.04400593 -1.87905786 -11.34834217 ## varietyNE87403 varietyNE87408 varietyNE87409 varietyNE87446 ## -7.07640881 -3.87938223 -1.07868064 -5.46517624 ## varietyNE87451 varietyNE87457 varietyNE87463 varietyNE87499 ## -2.90699758 -3.59541052 -4.49563865 -5.67322707 ## varietyNE87512 varietyNE87513 varietyNE87522 varietyNE87612 ## -5.60494961 -4.84498289 -7.64855589 0.48268951 ## varietyNE87613 varietyNE87615 varietyNE87619 varietyNE87627 ## 1.37319577 -3.69500935 1.18693565 -10.07566895 ## varietyNORKAN varietyREDLAND varietyROUGHRIDER varietySCOUT66 ## -5.27977336 0.36480075 -1.53131835 0.30979481 ## varietySIOUXLAND varietyTAM107 varietyTAM200 varietyVONA ## -2.75246638 -5.45138542 -9.11014805 -3.25735184 ## ## Correlation Structure: Linear spatial correlation ## Formula: ~latitude + longitude ## Parameter estimate(s): ## range nugget ## 10.7962043 0.2050487 ## Degrees of freedom: 224 total; 168 residual ## Residual standard error: 6.960609 We can use AIC to compare the fits of the two different spatial correlation structures. anova(fm1, fm2, fm3, fm4) ## Model df AIC BIC logLik Test L.Ratio p-value ## fm1 1 57 1354.742 1532.808 -620.3709 ## fm2 2 59 1185.863 1370.177 -533.9315 1 vs 2 172.8787 &lt;.0001 ## fm3 3 59 1185.102 1369.416 -533.5509 ## fm4 4 59 1184.563 1368.877 -533.2815 The linear correlation structure is AIC best. At this point, if we were really interested in these data, we would proceed to analyze for significant differences among the 56 wheat varieties. For our present purposes, we will merely note that the usual \\(F\\)-test rejects the null hypothesis of equality of means when we account for the spatial correlation in the residuals, but does not do so when we assumed the residuals were independent. anova(fm1) ## Denom. DF: 168 ## numDF F-value p-value ## (Intercept) 1 2454.621 &lt;.0001 ## variety 55 0.730 0.9119 anova(fm4) ## Denom. DF: 168 ## numDF F-value p-value ## (Intercept) 1 233.98320 &lt;.0001 ## variety 55 2.65823 &lt;.0001 "],["hierarchical-mixed-models.html", "Chapter 6 Hierarchical (mixed) models 6.1 One-factor layout: Dyestuff data 6.2 Bayesian analysis 6.3 Negative within-group correlations 6.4 Random coefficient models: RIKZ data 6.5 Nested and crossed random effects", " Chapter 6 Hierarchical (mixed) models 6.1 One-factor layout: Dyestuff data We will illustrate the basic ideas of hierarchical models with the Dyestuff data contained in lme4. According to Bates (2012+), these data originally appeared in Davies (1947), and “are described in Davies and Goldsmith (1972, Table 6.3, p. 131) … as coming from ‘an investigation to find out how much the variation from batch to bach in the quality of an intermediate product contributes to the variation in the yield of the dyestuff made from it’”. The data consist of 6 batches, each of which gives rise to 5 observations. Preparatory work: require(lme4) ## Loading required package: lme4 ## Loading required package: Matrix data(Dyestuff) summary(Dyestuff) ## Batch Yield ## A:5 Min. :1440 ## B:5 1st Qu.:1469 ## C:5 Median :1530 ## D:5 Mean :1528 ## E:5 3rd Qu.:1575 ## F:5 Max. :1635 with(Dyestuff, stripchart(Yield ~ Batch, pch = 16)) To develop some notation, let \\(i = 1, \\ldots, 6\\) index the batches, let \\(j = 1, \\ldots, 5\\) index the observations within each batch, and let \\(y_{ij}\\) denote observation \\(j\\) from batch \\(i\\). As a starting point, we will fit the usual one-factor ANOVA model to these data. This model is \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(\\mu_i, \\sigma^2) \\end{align}\\] fm0 &lt;- lm(Yield ~ 1, data = Dyestuff) # model with common mean fm1 &lt;- lm(Yield ~ Batch - 1, data = Dyestuff) # mean varies by group anova(fm0, fm1) # usual F-test for differences among group means ## Analysis of Variance Table ## ## Model 1: Yield ~ 1 ## Model 2: Yield ~ Batch - 1 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 29 115187 ## 2 24 58830 5 56358 4.5983 0.004398 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(fm1) # eliminating the intercept gives sample means for each group ## ## Call: ## lm(formula = Yield ~ Batch - 1, data = Dyestuff) ## ## Residuals: ## Min 1Q Median 3Q Max ## -85.00 -33.00 3.00 31.75 97.00 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## BatchA 1505.00 22.14 67.97 &lt;2e-16 *** ## BatchB 1528.00 22.14 69.01 &lt;2e-16 *** ## BatchC 1564.00 22.14 70.64 &lt;2e-16 *** ## BatchD 1498.00 22.14 67.66 &lt;2e-16 *** ## BatchE 1600.00 22.14 72.26 &lt;2e-16 *** ## BatchF 1470.00 22.14 66.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 49.51 on 24 degrees of freedom ## Multiple R-squared: 0.9992, Adjusted R-squared: 0.999 ## F-statistic: 4763 on 6 and 24 DF, p-value: &lt; 2.2e-16 Now we will use nlme::gls to fit a model that assumes that the data within each batch are correlated. In other words, we fit the model \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(\\mu_i, \\sigma^2) \\\\ \\mathrm{Corr}(y_{ij}, y_{ik}) &amp; = \\rho \\end{align}\\] require(nlme) ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmList fm2 &lt;- gls(Yield ~ 1, data = Dyestuff, correlation = corCompSymm(form = ~ 1 | Batch)) summary(fm2) ## Generalized least squares fit by REML ## Model: Yield ~ 1 ## Data: Dyestuff ## AIC BIC logLik ## 325.6543 329.7562 -159.8271 ## ## Correlation Structure: Compound symmetry ## Formula: ~1 | Batch ## Parameter estimate(s): ## Rho ## 0.4184874 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 1527.5 19.38341 78.80449 0 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.34770180 -0.90488550 0.03850577 0.73160955 1.65574793 ## ## Residual standard error: 64.92534 ## Degrees of freedom: 30 total; 29 residual The most salient components of this output are the estimate of the overall mean, and the estimate of the within-batch correlation (\\(\\hat{\\rho} = 0.42\\)). Now we will fit a hierarchical model that includes a random effect for the batch. We can write the model as \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(B_i, \\sigma_\\varepsilon^2) \\\\ B_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma_B^2). \\end{align}\\] fm3 &lt;- lmer(Yield ~ 1 + (1 | Batch), data = Dyestuff) summary(fm3) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Yield ~ 1 + (1 | Batch) ## Data: Dyestuff ## ## REML criterion at convergence: 319.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4117 -0.7634 0.1418 0.7792 1.8296 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Batch (Intercept) 1764 42.00 ## Residual 2451 49.51 ## Number of obs: 30, groups: Batch, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1527.50 19.38 78.8 The estimate of the overall mean is the same as it is in the GLS fit. Note also that we can recover the estimate of the within-batch correlation from the estimates of the variances of the random effects: var.B &lt;- 1764 var.eps &lt;- 2451 var.B / (var.B + var.eps) ## [1] 0.4185053 To obtain the conditional modes (BLUPs) of the batch-level random effect, we can use the command ranef: ranef(fm3) ## $Batch ## (Intercept) ## A -17.6068514 ## B 0.3912634 ## C 28.5622256 ## D -23.0845385 ## E 56.7331877 ## F -44.9952868 ## ## with conditional variances for &quot;Batch&quot; The conditional modes given here correspond to the differences between the mean for each batch and the overall mean (\\(\\mu\\)). To convert these to best guesses for the mean of each batch, we have to the overall mean back. This can be done by using the command fixef to extract the lone fixed-effect estimate from the model: (batch.conditional.modes &lt;- (fixef(fm3) + ranef(fm3)$Batch$`(Intercept)`)) ## [1] 1509.893 1527.891 1556.062 1504.415 1584.233 1482.505 It is informative to compare the conditional models for each batch to the sample means. We can calculate the sample means with the tapply function (batch.means &lt;- with(Dyestuff, tapply(Yield, Batch, mean))) ## A B C D E F ## 1505 1528 1564 1498 1600 1470 These are the same as the LS estimates of the batch-specific means in the ANOVA model, fm1. Now plot the sample means against the conditional modes: cbind(batch.means, batch.conditional.modes) ## batch.means batch.conditional.modes ## A 1505 1509.893 ## B 1528 1527.891 ## C 1564 1556.062 ## D 1498 1504.415 ## E 1600 1584.233 ## F 1470 1482.505 plot(x = batch.means, y = batch.conditional.modes, xlim = range(batch.means), ylim = range(batch.means), xlab = &quot;sample means&quot;, ylab = &quot;conditional modes&quot;, pch = LETTERS[1:6]) abline(a = 0, b = 1) The conditional modes are “shrunken” towards the global mean relative to the sample means. Why is this so? To conduct inferences about the parameters in the hierarchical model, lme4::lmer offers likelihood profiling. This is the same idea that we encountered when we were using the likelihood to calculate profile-based confidence intervals earlier in the course. lme4::lmer does all its profiling on the ML fit, so we begin by refitting our hierarchical model using ML. To do so, set the optional argument REML to FALSE. fm3ML &lt;- lmer(Yield ~ 1 + (1 | Batch), data = Dyestuff, REML = FALSE) summary(fm3ML) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: Yield ~ 1 + (1 | Batch) ## Data: Dyestuff ## ## AIC BIC logLik deviance df.resid ## 333.3 337.5 -163.7 327.3 27 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4315 -0.7972 0.1480 0.7721 1.8037 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Batch (Intercept) 1388 37.26 ## Residual 2451 49.51 ## Number of obs: 30, groups: Batch, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1527.50 17.69 86.33 Switching to ML has decreased our estimate of the batch-level variance, and decreased it by quite a bit. To construct profile-based intervals, we use the lme4::profile function. We will use the xyplot function from the lattice package to display the profiles. pr &lt;- profile(fm3ML) lattice::xyplot(pr) There are three panels here, one for each of the parameters in the model. These parameters are labeled as \\(\\sigma_1\\) for the standard deviation of the block-level random effect (what we have written \\(\\sigma_B\\)), \\(\\sigma\\) for the standard deviation of the errors (what we have written as \\(\\sigma_\\varepsilon\\)), and “(Intercept)” for the global mean (what we have written as \\(\\mu\\)). Within each panel, the values plotted on the vertical axis are the signed square roots of the likelihood ratio test statistic. This value is denoted by the Greek letter \\(\\zeta\\) (“zeta”); hence, the plots we are viewing are called zeta-plots. I do not know how widely used zeta-plots are. They may be more common in other realms of science, although I have never encountered them outside lme4. Basically, they are a visualization of profile-likelihood based confidence intervals. The vertical lines in each panel give the limits of (working from the inside out) 50%, 80%, 90%, 95%, and 99% confidence intervals for each parameter. We can also extract these confidence limits using the confint function. Below, we show 95% confidence intervals; of course, one can change the confidence level as needed. confint(pr, level = .95) ## 2.5 % 97.5 % ## .sig01 12.19854 84.06305 ## .sigma 38.22998 67.65770 ## (Intercept) 1486.45150 1568.54849 So, a 95% confidence interval for \\(\\mu\\) ranges from 1486 to 1569. Here is a bit more about the logic behind zeta plots (feel free to skip this paragraph if you wish). Recall that negative log-likelihood profiles are convex and approximately quadratic. Plotting the LRT statistic instead of the likelihood itself has the effect of placing the nadir of these curves at 0, instead of at the value of the negative log-likelihood. Taking the square root of the LRT statistic converts a quadratic curve (a u-shape) into a linear one (a v-shape). We can see this v-shape by using the optional argument absVal = TRUE: lattice::xyplot(pr, absVal = TRUE) The purpose of using a signed square root is to turn these V-shaped plots into straight lines. Straight lines are useful in turn because if the profile log likelihood is actually quadratic, then the zeta plot will yield a perfectly straight line, and thus a local approximation to the confidence interval will be appropriate. If the zeta-plot is non-linear, then the confidence interval becomes more asymmetric, and a local approximation fares more poorly. (Start reading again if you skipped the above detail). There is an interesting detail to the zeta-plots. Consider the zeta-plot for \\(\\sigma_1\\) (the standard deviation of the batch-level random effects), and try to find the lower limit of a 99% confidence interval. You’ll notice that the zeta-plot hits 0 (the lowest possible value for a standard deviation) before the interval is completed. Thus, the lower limit of this interval is 0: confint(pr, level = 0.99) ## 0.5 % 99.5 % ## .sig01 0.00000 113.68769 ## .sigma 35.56317 75.66803 ## (Intercept) 1465.87401 1589.12602 Although we are not much in the habit of conducting hypothesis tests in this course, we would conclude that we would fail to reject the null hypothesis that the batch-level variance equals 0 with a 99% level test. We can also obtain bivariate confidence regions from the profile likelihood using the lattice::splom command. lattice::splom(pr) Panels above the diagonal show bivariate, profile-based confidence regions for each pair of parameters. Within each panel, we see 50%, 80%, 90%, 95%, and 99% confidence regions. (You should be able to figure out which is which.) The panels show, for example, that the estimate of the observation-level standard deviation is slightly negatively correlated with the estimate of the batch-level standard deviation. Panels below the diagonal show the same plots on the \\(\\zeta\\) scale. See Bates (2012+) 1.5.3 for a detailed description of how to interpret these plots. There is a second approach to calculating confidence intervals and/or conducting hypothesis tests for fixed-effect parameters. Famously, lme4::lmer does not provide degrees of freedom for the estimates of the fixed effects. The package lmerTest uses the Satterthwaite approximation to estimate these df. require(lmerTest) ## Loading required package: lmerTest ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step fm5 &lt;- lmerTest::lmer(Yield ~ 1 + (1 | Batch), data = Dyestuff) summary(fm5) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Yield ~ 1 + (1 | Batch) ## Data: Dyestuff ## ## REML criterion at convergence: 319.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4117 -0.7634 0.1418 0.7792 1.8296 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Batch (Intercept) 1764 42.00 ## Residual 2451 49.51 ## Number of obs: 30, groups: Batch, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1527.50 19.38 5.00 78.8 6.23e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lmerTest::lmer gives the associated df for the estimate of the intercept as 5. If this were a class in experimental design, we would regard the individual measurements from each batch as subsamples, in which case the correct df for inferences about the intercept should be \\(6 - 1 = 5\\). In this case, the calculation from lmerTest::lmer matches our intuition. Equipped with this information, we could construct a 95% confidence interval for the overall mean as 1527.5 + 19.38 * qt(c(0.025, 0.975), df = 5) ## [1] 1477.682 1577.318 Compare this interval with the profile interval generated by lme4::profile. 6.2 Bayesian analysis The mixed-model formulation is our first encounter with a hierarchical model. One often hears the phrase “Bayesian hierarchical model” in ecology. It is important to realize that not all Bayesian models are hierarchical, and not all hierarchical models are Bayesian. Indeed, we have seen examples of both: none of the Bayesian examples that we have seen in earlier chapters were hierarchical, and the mixed-model analysis of the Dyestuff data is a hierarchical model analyzed from a frequentist perspective. However, we can certainly analyze hierarchical models from a Bayesian perspective as well, leading to a Bayesian hierarchical model. The above paragraph begs the question: What defines a hierarchical model? One can find definitions in the literature, though it isn’t clear to me that any of these definitions are fully precise, although I may just be ignorant. As best I can tell, a hierarchical model is one that includes so-called “latent” variables. Latent variables are unobservable quantities on which the data depend, that in turn are related to model parameters via a statistical model. This “layered” construction of the model (observables depend on latent variables, and latent variables depend on model parameters) gives rise to the “hierarchy” that gives hierarchical models their name. This hierarchical perspective can be emphasized by how the model is written. For the dyestuff data, when we write \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(B_i, \\sigma^2_\\varepsilon) \\\\ B_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2_B) \\\\ \\end{align}\\] we emphasize that the distribution of the data depends in part on the latent variables \\(B_i\\), and the distribution of the latent variables depends in turn on some of the model parameters. To complete the Bayesian specification, we need to place priors on our three parameters: \\(\\mu\\), \\(\\sigma^2_B\\), and \\(\\sigma^2_\\varepsilon\\). In the absence of any information, we might choose a vague normal prior for \\(\\mu\\), and vague gamma priors for \\(1/\\sigma^2_B\\) and \\(1/\\sigma^2_\\varepsilon\\). The following R2Jags code implements such a model. require(R2jags) ## Loading required package: R2jags ## Loading required package: rjags ## Loading required package: coda ## Linked to JAGS 4.3.1 ## Loaded modules: basemod,bugs ## ## Attaching package: &#39;R2jags&#39; ## The following object is masked from &#39;package:coda&#39;: ## ## traceplot dyestuff.model &lt;- function() { ## likelihood for (j in 1:J) { y[j] ~ dnorm(B[batch[j]], tau_eps) # data distribution } ## latent variables for (b in 1:6){ B[b] ~ dnorm(mu, tauB) } mu ~ dnorm (0.0, 1E-6) # prior for the overall mean tau_eps ~ dgamma (0.01, 0.01) tauB ~ dgamma (0.01, 0.01) sd_eps &lt;- pow(tau_eps, -1/2) sdB &lt;- pow(tauB, -1/2) } jags.data &lt;- list(y = Dyestuff$Yield, batch = as.numeric(Dyestuff$Batch), J = nrow(Dyestuff)) jags.params &lt;- c(&quot;mu&quot;, &quot;sd_eps&quot;, &quot;sdB&quot;, &quot;B[1]&quot;, &quot;B[2]&quot;) jags.inits &lt;- function(){ list(&quot;mu&quot; = rnorm(1, .01), &quot;tauB&quot; = runif(1), &quot;tau_eps&quot; = runif(1)) } set.seed(1) jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = dyestuff.model, n.chains = 3, n.iter = 1e5) ## module glm loaded The code above requires a way to associate each observation with the batch from which the observation was drawn. We accomplish this by creating the variable batch that associates each observation with the numerical index of the batch. (That is, batch “A” is associated with the index 1, etc.) In the code, this happens with the line batch = as.numeric(Dyestuff$Batch) The as.numeric command returns numerical values for each level of the factor Batch. Note also that we have only asked for the posterior draws for the latent means of the first two batches, \\(B_1\\) and \\(B_2\\). This is merely to keep the output small for this example. We could of course ask for the means of the other batches as well. Let’s have a look at the output: print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpUXb3WQ/model385875c4561e.txt&quot;, fit using jags, ## 3 chains, each with 1e+05 iterations (first 50000 discarded), n.thin = 50 ## n.sims = 3000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat ## B[1] 1513.362 22.375 1471.329 1500.347 1514.592 1527.349 1551.127 1.008 ## B[2] 1527.140 21.282 1488.362 1515.118 1527.513 1539.242 1565.028 1.008 ## mu 1525.971 23.850 1481.395 1514.107 1526.465 1538.276 1569.052 1.006 ## sdB 38.744 26.358 0.269 23.345 36.521 51.151 101.022 1.002 ## sd_eps 54.105 12.349 39.388 47.081 52.538 59.337 75.857 1.001 ## deviance 323.591 7.099 314.873 318.505 321.822 327.461 337.074 1.001 ## n.eff ## B[1] 3000 ## B[2] 1500 ## mu 3000 ## sdB 3000 ## sd_eps 3000 ## deviance 3000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 25.2 and DIC = 348.8 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) require(lattice) ## Loading required package: lattice jagsfit.mcmc &lt;- as.mcmc(jagsfit) densityplot(jagsfit.mcmc) As of this writing, there are some aspects of the output that I don’t understand. The traceplots show that one chain starts far away from the region of high posterior density and then rapidly converges to it. This behavior seems to persist regardless of how long the burn-in period is, which doesn’t make sense to me. I also do not understand why densityplot produces a 6-by-1 stack of panels instead of a more useful layout. One especially appealing aspect of analyzing this model from a Bayesian perspective is that there is no awkwardness in analyzing the posterior distributions of the latent variables, namely, the batch-specific means. From the frequentist perspective, it is somewhat awkward (though not prohibitively so) to define the conditional modes (BLUPs). We also lack straightforward methods for quantifying the uncertainty in these conditional modes. From the Bayesian viewpoint, this awkwardness disappears, because the distinction between model parameters and latent variables vanishes. Both are simply unobserved quantities. Consequently, it is natural to summarize our posterior knowledge about the latent variables by their (marginal) posterior distributions, in the same way that we use marginal posteriors to summarize our inferences about the model parameters. 6.3 Negative within-group correlations The hierarchical model formulation, regardless of whether analyzed from a frequentist or Bayesian perspective, is just a model. All models are wrong, but some models are useful. One can encounter grouped data where the hierarchical formulation is not useful. To illustrate, we consider the Dyestuff2 data provided in lme4. These are synthetic (fake) data that have been created by Box &amp; Tiao (1973) for the sake of illustration. The Dyestuff2 data have the same structure as the original Dyestuff data, that is, 5 observations from each of 6 batches. data(Dyestuff2) summary(Dyestuff2) ## Batch Yield ## A:5 Min. :-0.892 ## B:5 1st Qu.: 2.765 ## C:5 Median : 5.365 ## D:5 Mean : 5.666 ## E:5 3rd Qu.: 8.151 ## F:5 Max. :13.434 with(Dyestuff2, stripchart(Yield ~ Batch, pch = 16)) We’ll begin by fitting a GLS model that includes a within-batch correlation: fm6 &lt;- gls(Yield ~ 1, data = Dyestuff2, correlation = corCompSymm(form = ~ 1 | Batch)) summary(fm6) ## Generalized least squares fit by REML ## Model: Yield ~ 1 ## Data: Dyestuff2 ## AIC BIC logLik ## 167.2092 171.3111 -80.60461 ## ## Correlation Structure: Compound symmetry ## Formula: ~1 | Batch ## Parameter estimate(s): ## Rho ## -0.0970284 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 5.6656 0.5271409 10.74779 0 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.77661357 -0.78584319 -0.08143986 0.67335540 2.10464878 ## ## Residual standard error: 3.691067 ## Degrees of freedom: 30 total; 29 residual Notice that the within-batch correlation is negative. What does this imply about the structure of the data? Let’s have a look at a hierarchical model, fit with lmer. fm6 &lt;- lme4::lmer(Yield ~ 1 + (1 | Batch), data = Dyestuff2) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(fm6) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Yield ~ 1 + (1 | Batch) ## Data: Dyestuff2 ## ## REML criterion at convergence: 161.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.7648 -0.7806 -0.0809 0.6689 2.0907 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Batch (Intercept) 0.00 0.000 ## Residual 13.81 3.716 ## Number of obs: 30, groups: Batch, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 5.6656 0.6784 8.352 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) The estimate of the among-batch variance, \\(\\sigma^2_B\\), is 0. The warning generated by R tells us that our fit is singular: one of the estimated parameters lies on the boundary of its possible values. To quote Bates (2012+, using our notation): “An estimate of 0 for \\(\\sigma_B\\) does not mean that there is no variation between the groups. Indeed, … there is some small amount of variability between the groups. The estimate, \\(\\hat{\\sigma}_B=0\\), simply indicates that the level of ‘between-group’ variability is not sufficient to warrant incorporating random effects in the model. “The important point … is that we must allow for the estimates of variance components to be zero. We describe such a model as being degenerate, in the sense that it corresponds to a linear model in which we have removed the randdom effects associated with Batch. Degenerate models can and do occur in practice. Eveen when the final fitted model is not degenerate, we must allow for such models when determining the parameter estimates through numerical optimization.” Can you think of any ecological mechanisms that might give rise to negative within-group correlations? 6.4 Random coefficient models: RIKZ data So-called random coefficient models are popular modeling structures in ecology. Random-coefficient models are useful when data are grouped, like the Dyestuff data. However, unlike the Dyestuff data, random-coefficient models refer to scenarios where the statistical model that we want to entertain within each group is more complex than just a simple random sample with a group-specific mean. To illustrate random-coefficient models, we will consider the RIKZ data from Zuur et al. (2009). These data were first analyzed in an earlier textbook (Zuur et al. 2007). Zuur et al. (2009, p. 101) describe the data as follows: “Zuur et al. (2007) used marine benthic data from nine inter-tidal areas along the Dutch coast. The data were collected by the Dutch institute RIKZ in the summer of 2002. In each inter-tidal area (denoted by ‘beach’), five samples were taken, and the macro-fauna and abiotic variables were measured. … The underlying question for these data is whether there is a relationship between species richness, exposure, and NAP (the height of a sampling station compared to mean tidal level). Exposure is an index composed of the following elements: wave action, length of the surf zone, slope, grain size, and the depth of the anaerobic layer.” In other words, there are 9 beaches, and 5 samples from each beach. The response, measured at each sample, is the macrofaunal species richness. There are two covariates: NAP, which is a sample-level covariate, and exposure, which is a beach-level covariate. Because species richness is a count variable and includes the occasional zero (and because we have not yet discussed hierarchical models for non-Gaussian responses) we will use the square-root of richness as a variance-stabilizing transformation. Using the square root of species richness as the response has the added benefit of making our analysis different from the analysis in Zuur et al. (2009). We are going to analyze these data exhaustively, considering various approaches for their analysis and comparing the pros and cons. In our first pass, we will ignore the exposure covariate, and seek only to model the relationship between species richness and NAP. Once that analysis is complete, we will circle back and consider how the analysis changes when we consider the beach-level covariate as well. Like all data from Zuur et al. (2009), the data are available for download from the book’s associated webpage. We will read in the data and do some housekeeping first. require(lme4) require(lmerTest) rikz &lt;- read.table(&quot;data/RIKZ.txt&quot;, head = T) with(rikz, plot(Richness ~ NAP, pch = Beach)) # raw response; note the non-constant variance with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach)) # transformation stabilizes the variance legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) # change the Beach variable to a factor # would have been better to code the beaches as b1, b2, ... rikz$fBeach &lt;- as.factor(rikz$Beach) 6.4.1 Analysis without beach-level covariate 6.4.1.1 Using fixed effects for among-beach differences To develop some notation for modeling, let \\(i=1, \\ldots, 9\\) index the different beaches, let \\(j = 1, \\ldots, 5\\) index the different samples at each beach, let \\(y_{ij}\\) be the square root of the species richness at sample \\(j\\) at beach \\(i\\), and let \\(x_{ij}\\) be the NAP covariate at sample \\(j\\) at beach \\(i\\). In a standard linear models course, we would identify this as a two-factor design with a categorical factor (the beaches) and a numerical factor (NAP). Suppose we wish to characterize the relationship between NAP and (the square root of) species richness, while controlling for differences among the beaches. To do so, we might entertain the additive model \\[\\begin{align*} y_{ij} &amp; = a_i + b x_{ij} + \\varepsilon_{ij} \\\\ \\varepsilon_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma_\\varepsilon^2). \\end{align*}\\] As usual, there are several different ways in which we could write the same model. We might instead write \\[\\begin{align*} y_{ij} &amp; = \\mu_{ij} + \\varepsilon_{ij} \\\\ \\mu_{ij} &amp; = a_i + b x_{ij} \\\\ \\varepsilon_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma_\\varepsilon^2) \\end{align*}\\] to emphasize that the mean of each observation (\\(\\mu_{ij}\\)) is equal to the sum of a beach-level intercept (\\(a_i\\)) and a common slope times the NAP value (\\(b x_{ij}\\)). Alternatively, we might write \\[\\begin{equation} y_{ij} \\sim \\mathcal{N}(a_i + b x_{ij}, \\sigma_\\varepsilon^2). \\end{equation}\\] In any case, the model states that each observation is drawn independently from a Gaussian distribution. The fitted value (or mean) for each datum is determined by a regression line with beach-specific intercepts and a common slope. (The additive model has the same structure has a parallel-lines ANCOVA.) The error variance is the same for all observations. Let’s fit the model and see what it yields. fm0 &lt;- lm(sqrt(Richness) ~ fBeach + NAP, data = rikz) summary(fm0) ## ## Call: ## lm(formula = sqrt(Richness) ~ fBeach + NAP, data = rikz) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.58544 -0.28653 -0.06544 0.23657 1.69043 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.99457 0.26711 11.211 3.92e-13 *** ## fBeach2 0.61544 0.37909 1.623 0.11346 ## fBeach3 -1.21158 0.37491 -3.232 0.00268 ** ## fBeach4 -1.13596 0.38510 -2.950 0.00564 ** ## fBeach5 -0.32863 0.38648 -0.850 0.40093 ## fBeach6 -0.90219 0.37835 -2.385 0.02265 * ## fBeach7 -0.98741 0.39419 -2.505 0.01705 * ## fBeach8 -0.89080 0.38392 -2.320 0.02628 * ## fBeach9 -0.79350 0.38561 -2.058 0.04712 * ## NAP -0.66410 0.09655 -6.878 5.49e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5882 on 35 degrees of freedom ## Multiple R-squared: 0.7596, Adjusted R-squared: 0.6978 ## F-statistic: 12.29 on 9 and 35 DF, p-value: 1.744e-08 This analysis gives us an estimate for the common slope (-0.664) and a basis to draw inferences about this slope. For example, we can get a confidence interval in the usual way: confint(fm0, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 2.4523092 3.53682298 ## fBeach2 -0.1541506 1.38502150 ## fBeach3 -1.9726894 -0.45047493 ## fBeach4 -1.9177509 -0.35416521 ## fBeach5 -1.1132200 0.45596483 ## fBeach6 -1.6702860 -0.13408870 ## fBeach7 -1.7876580 -0.18716421 ## fBeach8 -1.6702073 -0.11139365 ## fBeach9 -1.5763374 -0.01066776 ## NAP -0.8601180 -0.46808725 We can also use the anova command to test for whether the differences among the beaches are statistically significant, after accounting for the effect of NAP. Note that such a test makes sense in this case, because we have used fixed-effects to capture the differences among the beaches, and thus can carry out inference about these 9 beaches specifically. anova(fm0) ## Analysis of Variance Table ## ## Response: sqrt(Richness) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fBeach 8 21.900 2.7375 7.9109 5.119e-06 *** ## NAP 1 16.370 16.3701 47.3073 5.492e-08 *** ## Residuals 35 12.111 0.3460 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Finally, we can visualize the model by plotting each of the beach-specific fits. We’ll use the R trick of re-fitting the model without the intercept to obtain the beach-specific intercepts directly as model parameters, instead of having to back out those intercepts from the contrasts. fm0.temp &lt;- lm(sqrt(Richness) ~ fBeach + NAP - 1, data = rikz) coef(fm0.temp) ## fBeach1 fBeach2 fBeach3 fBeach4 fBeach5 fBeach6 fBeach7 ## 2.9945661 3.6100015 1.7829839 1.8586080 2.6659385 2.0923787 2.0071550 ## fBeach8 fBeach9 NAP ## 2.1037656 2.2010635 -0.6641026 For later comparison, we’ll make a note of the intercept for beach 1, which in this case is 2.995. Now we’ll proceed to make the plot. with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach, main = &quot;Fixed-effects fit, additive model&quot;)) legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) # add a line for each beach b &lt;- coef(fm0)[&quot;NAP&quot;] for(i in 1:9){ abline(a = coef(fm0.temp)[i], b = b, col = &quot;red&quot;, lty = &quot;dotted&quot;) } Continuing with the fixed-effects analysis, we might also consider a model in which the relationship between NAP and species richness varies among beaches. In other words, we might fit a model with a beach-by-NAP interaction. This model is \\[\\begin{align*} y_{ij} &amp; \\sim \\mathcal{N}(a_i + b_i x_{ij}, \\sigma_\\varepsilon^2). \\end{align*}\\] We fit this model in the usual way: fm0b &lt;- lm(sqrt(Richness) ~ fBeach * NAP, data = rikz) summary(fm0b) ## ## Call: ## lm(formula = sqrt(Richness) ~ fBeach * NAP, data = rikz) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.84831 -0.16080 -0.03091 0.14909 0.98737 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.28835 0.24259 13.555 1.45e-13 *** ## fBeach2 0.30239 0.32158 0.940 0.355394 ## fBeach3 -1.50542 0.31542 -4.773 5.61e-05 *** ## fBeach4 -1.56073 0.33715 -4.629 8.25e-05 *** ## fBeach5 0.11078 0.35432 0.313 0.756947 ## fBeach6 -1.25466 0.31812 -3.944 0.000513 *** ## fBeach7 -1.39537 0.41116 -3.394 0.002144 ** ## fBeach8 -1.17907 0.32697 -3.606 0.001242 ** ## fBeach9 -0.85912 0.33879 -2.536 0.017314 * ## NAP -0.05077 0.28172 -0.180 0.858319 ## fBeach2:NAP -0.54313 0.36261 -1.498 0.145780 ## fBeach3:NAP -0.47943 0.37104 -1.292 0.207267 ## fBeach4:NAP -0.37552 0.35511 -1.057 0.299666 ## fBeach5:NAP -1.82561 0.38805 -4.705 6.74e-05 *** ## fBeach6:NAP -0.36229 0.33258 -1.089 0.285636 ## fBeach7:NAP -0.48212 0.41379 -1.165 0.254155 ## fBeach8:NAP -0.62429 0.32975 -1.893 0.069089 . ## fBeach9:NAP -1.01278 0.35527 -2.851 0.008256 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4508 on 27 degrees of freedom ## Multiple R-squared: 0.8911, Adjusted R-squared: 0.8225 ## F-statistic: 13 on 17 and 27 DF, p-value: 7.079e-09 We can test for whether the differences among the slopes for the beaches are statistically significant with the usual \\(F\\)-test: anova(fm0, fm0b) ## Analysis of Variance Table ## ## Model 1: sqrt(Richness) ~ fBeach + NAP ## Model 2: sqrt(Richness) ~ fBeach * NAP ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 35 12.1113 ## 2 27 5.4865 8 6.6248 4.0753 0.002742 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We’ll save the model of the beach-specific intercepts and slopes, and use them to visualize the fit. We’ll do the usual trick of refitting the model without the intercept to make it easy to extract the beach-level intercepts and slopes fm0b.temp &lt;- lm(sqrt(Richness) ~ fBeach + fBeach:NAP - 1, data = rikz) (fixed.params &lt;- data.frame(beach = 1:9, intercept = coef(fm0b.temp)[1:9], slope = coef(fm0b.temp)[10:18])) ## beach intercept slope ## fBeach1 1 3.288351 -0.05077384 ## fBeach2 2 3.590739 -0.59390495 ## fBeach3 3 1.782930 -0.53019915 ## fBeach4 4 1.727622 -0.42629196 ## fBeach5 5 3.399128 -1.87638770 ## fBeach6 6 2.033687 -0.41306694 ## fBeach7 7 1.892979 -0.53289652 ## fBeach8 8 2.109276 -0.67506643 ## fBeach9 9 2.429231 -1.06355553 row.names(fixed.params) &lt;- NULL with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach, main = &quot;Fixed-effects fit, with beach-NAP interaction&quot;)) legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) for(i in 1:9){ abline(a = fixed.params$intercept[i], b = fixed.params$slope[i], col = &quot;red&quot;, lty = &quot;dotted&quot;) } 6.4.1.2 Using random-effects for among-beach differences Now let’s use random effects to capture the differences among beaches. A random effect is appropriate if we want to treat these beaches as a representative sample from a larger collection of beaches, and draw inferences about this larger collection. We’ll start with the additive model again, so that the random beach effect only affects the intercept. Using the notational style of mixed modeling, we could write our model as \\[\\begin{align*} y_{ij} &amp; \\sim \\mathcal{N}(A_i + b x_{ij}, \\sigma_\\varepsilon^2) \\\\ A_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(a, \\sigma_a^2). \\end{align*}\\] Here, the \\(A_i\\)’s are the random beach-level intercept. We’ll fit the model using lmerTest::lmer. fm1 &lt;- lmerTest::lmer(sqrt(Richness) ~ 1 + NAP + (1 | fBeach), data = rikz) summary(fm1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) ## Data: rikz ## ## REML criterion at convergence: 97.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5693 -0.4286 -0.1869 0.3230 2.9399 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.2957 0.5438 ## Residual 0.3460 0.5882 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.37424 0.20405 8.36034 11.635 1.88e-06 *** ## NAP -0.68063 0.09501 37.15971 -7.163 1.68e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## NAP -0.162 Note that the random-intercepts model includes the parameter \\(a\\), which is the average intercept across the population of beaches. This is the parameter listed as “(Intercept)” in the fixed-effects portion of the model output above. The model with fixed-effect for the beach-level intercepts has no such parameter. This makes sense, because that model did not envision a larger population of beaches. We can go further by finding the conditional modes of the intercepts for each beach. (beach.conditional.modes &lt;- (fixef(fm1)[&quot;(Intercept)&quot;] + ranef(fm1)$fBeach$`(Intercept)`)) ## [1] 2.870511 3.379324 1.895117 1.963771 2.618721 2.148964 2.088426 2.161791 ## [9] 2.241556 In particular, note the conditional mode for the intercept for beach 1, which is 2.871. This conditional mode is shrunken back towards the average intercept, compared to the intercept for this beach in the fixed-effects model. To visualize the model, we will again make a plot that shows the conditional modes of the fit for each beach. We can also add a line for the average relationship across the population of beaches. with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach, main = &quot;Random intercepts fit&quot;)) legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) a &lt;- coef(summary(fm1))[1, 1] b &lt;- coef(summary(fm1))[2, 1] abline(a = a, b = b, col = &quot;red&quot;, lwd = 2) # make a plot with a line for each beach for(i in 1:9){ abline(a = beach.conditional.modes[i], b = b, col = &quot;red&quot;, lty = &quot;dotted&quot;) } Though it’s subtle, notice again that the implied fits for each beach have been shrunken back to the overall mean. Now let’s consider a model that includes both separate intercepts and slopes for each beach, while continuing to model the among-beach differences in both with random effects. In other words, we’ll fit a “random coefficients” model with random intercepts and slopes for each beach. We have two options here. Either we can allow for the random intercept and slope for each beach to be a draw from a bivariate normal distribution with possible correlations, or we can insist that the random intercepts and slopes are independent. To write the first model in mixed-model notation, we might write \\[\\begin{align*} y_{ij} &amp; \\sim \\mathcal{N}(A_i + B_i x_{ij}, \\sigma_\\varepsilon^2) \\\\ \\left(\\begin{array}{c} A \\\\ B \\end{array} \\right)_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}_2 \\left(\\left(\\begin{array}{c} a \\\\ b \\end{array} \\right), \\left(\\begin{array}{cc} \\sigma_A^2 &amp; \\sigma_{AB} \\\\ \\sigma_{AB} &amp; \\sigma_B^2 \\end{array} \\right) \\right). \\end{align*}\\] To fit the model using lmerTest::lmer, we use fm2 &lt;- lmerTest::lmer(sqrt(Richness) ~ 1 + NAP + (1 + NAP | fBeach), data = rikz) summary(fm2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ 1 + NAP + (1 + NAP | fBeach) ## Data: rikz ## ## REML criterion at convergence: 92.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6245 -0.4430 -0.1095 0.3023 2.1610 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## fBeach (Intercept) 0.4389 0.6625 ## NAP 0.1582 0.3978 -0.41 ## Residual 0.2159 0.4647 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.4369 0.2348 7.9127 10.379 6.97e-06 *** ## NAP -0.7026 0.1543 6.7971 -4.552 0.00283 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## NAP -0.390 Because this model nests the random-intercept model, we can compare the two directly with a LRT: anova(fm1, fm2) ## refitting model(s) with ML (instead of REML) ## Data: rikz ## Models: ## fm1: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) ## fm2: sqrt(Richness) ~ 1 + NAP + (1 + NAP | fBeach) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## fm1 4 100.83 108.06 -46.416 92.833 ## fm2 6 101.26 112.10 -44.630 89.259 3.5736 2 0.1675 Interestingly, both the LRT and ANOVA suggest that the random-coefficients model does not provide a statistically significant improvement over the random-intercepts model. In other words, we cannot reject the null hypothesis that \\(\\sigma_B^2 = 0\\). Alternatively, we could try a model with independent random effects for intercepts and slopes. This model is \\[\\begin{align*} y_{ij} &amp; \\sim \\mathcal{N}(A_i + B_i x_{ij}, \\sigma_\\varepsilon^2) \\\\ A_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(a, \\sigma^2_A) \\\\ B_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(b, \\sigma^2_B). \\end{align*}\\] To fit it in R, we use fm3 &lt;- lmerTest::lmer(sqrt(Richness) ~ 1 + NAP + (1 | fBeach) + (0 + NAP | fBeach), data = rikz) summary(fm3) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) + (0 + NAP | fBeach) ## Data: rikz ## ## REML criterion at convergence: 93.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.5841 -0.4070 -0.1042 0.2804 2.2006 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.4247 0.6517 ## fBeach.1 NAP 0.1489 0.3858 ## Residual 0.2168 0.4657 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.4413 0.2314 7.8858 10.552 6.32e-06 *** ## NAP -0.6912 0.1510 6.6684 -4.576 0.00289 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## NAP -0.061 Because each of these random-effect models are nested within one another, we can compare them directly with LRTs: anova(fm1, fm3, fm2) ## refitting model(s) with ML (instead of REML) ## Data: rikz ## Models: ## fm1: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) ## fm3: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) + (0 + NAP | fBeach) ## fm2: sqrt(Richness) ~ 1 + NAP + (1 + NAP | fBeach) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## fm1 4 100.83 108.06 -46.416 92.833 ## fm3 5 100.16 109.19 -45.078 90.156 2.6767 1 0.1018 ## fm2 6 101.26 112.10 -44.630 89.259 0.8969 1 0.3436 Here we see a conflict between AIC and the LRT. AIC favors the model with random but independent intercepts and slopes, whereas the LRT continues to suggest that we cannot reject the null hypothesis that \\(\\sigma_B^2 = 0\\). Finally, let’s compare the conditional modes for the intercepts and slopes from model fm3 with the beach-specific intercept and slopes from the fixed-effects model. (conditional.modes &lt;- data.frame(beach = 1:9, intercept = fixef(fm3)[&quot;(Intercept)&quot;] + ranef(fm3)$fBeach$`(Intercept)`, slope = fixef(fm3)[&quot;NAP&quot;] + ranef(fm3)$fBeach$`NAP`)) ## beach intercept slope ## 1 1 3.091736 -0.3225870 ## 2 2 3.484038 -0.5930830 ## 3 3 1.843949 -0.5776709 ## 4 4 1.841492 -0.5218237 ## 5 5 3.066017 -1.4310799 ## 6 6 2.083390 -0.4693246 ## 7 7 2.032495 -0.6452486 ## 8 8 2.145453 -0.6869329 ## 9 9 2.383348 -0.9728640 Let’s make a scatterplot that compares the fixed-effect estimates to the conditional modes. par(mfrow = c(1, 2)) with(fixed.params, plot(slope ~ intercept, main = &quot;fixed-effects fit&quot;, pch = 1:9)) with(fixed.params, plot(slope ~ intercept, main = &quot;conditional modes&quot;, type = &quot;n&quot;)) with(conditional.modes, points(slope ~ intercept, pch = 1:9)) points(fixef(fm3)[1], fixef(fm3)[2], pch = 16, col = &quot;red&quot;, cex = 2) Note, again, that the conditional modes of the intercepts and slopes have shrunk (sometimes substantially) back towards the population means of each. The population means of the intercept and slope are shown by the red dot on the right-hand panel. Finally, we visualize the model by plotting beach-specific “fits”: par(mfrow = c(1, 1)) with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach, main = &quot;Random-coefficients fit&quot;)) legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) for(i in 1:9){ abline(a = conditional.modes$intercept[i], b = conditional.modes$slope[i], col = &quot;red&quot;, lty = &quot;dotted&quot;) } abline(a = fixef(fm3)[1], b = fixef(fm3)[2], col = &quot;red&quot;, lwd = 2) 6.4.2 Adding a beach-level covariate Finally, we consider the effect of exposure, a beach-level covariate. Exposure is coded in the data set as a numerical predictor. However, there are only three unique values: 8, 10, and 11 (and only one beach has exposure level 8). We will follow Zuur et al. (2009) in treating exposure as a binary predictor, grouping the beaches with exposure levels 8 and 10 together as low exposure beaches. # create an &#39;exposure&#39; factor, with two levels: low and high with(rikz, table(fBeach, Exposure)) ## Exposure ## fBeach 8 10 11 ## 1 0 5 0 ## 2 5 0 0 ## 3 0 0 5 ## 4 0 0 5 ## 5 0 5 0 ## 6 0 0 5 ## 7 0 0 5 ## 8 0 5 0 ## 9 0 5 0 rikz$fExp &lt;- rikz$Exposure # make a new variable so that we can leave the original alone rikz$fExp[rikz$Exposure == 8] &lt;- 10 # assign a value of 10 to the lone beach with exposure = 8 rikz$fExp &lt;- as.factor(rikz$fExp) # make the new variable into a factor summary(rikz) ## Sample Richness Exposure NAP Beach ## Min. : 1 Min. : 0.000 Min. : 8.00 Min. :-1.3360 Min. :1 ## 1st Qu.:12 1st Qu.: 3.000 1st Qu.:10.00 1st Qu.:-0.3750 1st Qu.:3 ## Median :23 Median : 4.000 Median :10.00 Median : 0.1670 Median :5 ## Mean :23 Mean : 5.689 Mean :10.22 Mean : 0.3477 Mean :5 ## 3rd Qu.:34 3rd Qu.: 8.000 3rd Qu.:11.00 3rd Qu.: 1.1170 3rd Qu.:7 ## Max. :45 Max. :22.000 Max. :11.00 Max. : 2.2550 Max. :9 ## ## fBeach fExp ## 1 : 5 10:25 ## 2 : 5 11:20 ## 3 : 5 ## 4 : 5 ## 5 : 5 ## 6 : 5 ## (Other):15 We will consider a model in which we use separate distributions of random intercepts for the the low- and high-exposure beaches. To fit this model, we will need to embellish our notation. Now, let \\(i=1, 2\\) index the exposure level of the beaches. Let \\(j=1,\\ldots, n_i\\) index the replicate beaches within each exposure level. Let \\(k=1, \\ldots, 5\\) index the samples at each beach. Here we see an interesting consequence of our decision to model the differences among the beaches with either a fixed or random effect. In order to draw inferences about the effect of low vs. high exposure, we must use a random effect for the beach-to-beach differences. This makes sense when we reflect upon it. In order to draw inferences about low vs. high exposure, we have to envision separate populations of low and high exposure beaches, and construe the beaches in this experiment as two separate random samples from those populations. If instead we treat the beach-to-beach differences with fixed effects, then we are effectively saying that these are the only low- and high-exposure beaches that we care about. Therefore, in the fixed-effects formulation, these nine beaches are our two populations, and there is no inference to be done. As above, we will consider a series of three models with various specifications of the random effect. We will consider: A model with random intercepts for the beaches but common slopes within each exposure group A model with independent random intercepts and slopes for beaches within each exposure group. A model with random intercepts and slopes for beaches within each exposure group, and potential correlations between them Initially, we fit a model with richly specified fixed effects. In this case, this will mean that the average slope and intercept will vary between the exposure groups. These models can be written and fit as follows. The random-intercepts model is \\[\\begin{align*} y_{ijk} &amp; \\sim \\mathcal{N}(A_{ij} + b_i x_{ijk}, \\sigma_\\varepsilon^2)\\\\ A_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(a_i, \\sigma^2_A). \\end{align*}\\] We fit this model in R with the code fm4 &lt;- lmerTest::lmer(sqrt(Richness) ~ fExp * NAP + (1 | fBeach), data = rikz) summary(fm4) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) ## Data: rikz ## ## REML criterion at convergence: 89.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4532 -0.5066 -0.0280 0.3478 2.6221 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.1395 0.3735 ## Residual 0.3182 0.5641 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.7722 0.2047 7.3111 13.544 1.92e-06 *** ## fExp11 -0.9213 0.3096 7.5513 -2.976 0.0189 * ## NAP -0.8580 0.1207 37.7420 -7.110 1.82e-08 *** ## fExp11:NAP 0.3979 0.1818 37.1348 2.189 0.0350 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fExp11 NAP ## fExp11 -0.661 ## NAP -0.174 0.115 ## fExp11:NAP 0.115 -0.212 -0.664 The model with independent intercepts and slopes can be written as \\[\\begin{align*} y_{ijk} &amp; \\sim \\mathcal{N}(A_{ij} + B_{ij} x_{ijk}, \\sigma_\\varepsilon^2)\\\\ A_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(a_i, \\sigma^2_A) \\\\ B_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(b_i, \\sigma^2_B) . \\end{align*}\\] We fit this model with the code fm5 &lt;- lmerTest::lmer(sqrt(Richness) ~ fExp * NAP + (1 | fBeach) + (0 + NAP | fBeach), data = rikz) summary(fm5) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) + (0 + NAP | fBeach) ## Data: rikz ## ## REML criterion at convergence: 85.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.70750 -0.40036 -0.01429 0.32923 2.02319 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.1783 0.4222 ## fBeach.1 NAP 0.1476 0.3842 ## Residual 0.2122 0.4606 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.8962 0.2153 7.1463 13.450 2.46e-06 *** ## fExp11 -1.0394 0.3248 7.2622 -3.200 0.01434 * ## NAP -0.8618 0.2007 5.7464 -4.295 0.00565 ** ## fExp11:NAP 0.3904 0.3012 5.7584 1.296 0.24444 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fExp11 NAP ## fExp11 -0.663 ## NAP -0.062 0.041 ## fExp11:NAP 0.041 -0.090 -0.666 The model with possibly correlated random intercepts and slopes can be written as \\[\\begin{align*} y_{ijk} &amp; \\sim \\mathcal{N}(A_{ij} + B_{ij} x_{ijk}, \\sigma_\\varepsilon^2)\\\\ \\left(\\begin{array}{c} A \\\\ B \\end{array} \\right)_i &amp; \\sim \\mathcal{N}_2 \\left(\\left(\\begin{array}{c} a_i \\\\ b_i \\end{array} \\right), \\left(\\begin{array}{cc} \\sigma_A^2 &amp; \\sigma_{AB} \\\\ \\sigma_{AB} &amp; \\sigma_B^2 \\end{array} \\right) \\right). \\end{align*}\\] We fit the model in R as follows fm6 &lt;- lmerTest::lmer(sqrt(Richness) ~ fExp * NAP + (1 + NAP| fBeach), data = rikz) summary(fm6) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ fExp * NAP + (1 + NAP | fBeach) ## Data: rikz ## ## REML criterion at convergence: 85.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.70561 -0.39766 -0.01305 0.32821 2.02192 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## fBeach (Intercept) 0.1767 0.4203 ## NAP 0.1464 0.3826 0.05 ## Residual 0.2125 0.4610 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.8956 0.2146 6.9871 13.494 2.93e-06 *** ## fExp11 -1.0389 0.3237 7.0837 -3.210 0.01462 * ## NAP -0.8588 0.2001 5.6602 -4.292 0.00587 ** ## fExp11:NAP 0.3874 0.3003 5.6586 1.290 0.24724 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fExp11 NAP ## fExp11 -0.663 ## NAP -0.026 0.017 ## fExp11:NAP 0.017 -0.054 -0.666 Because these models form a series of nested models, we can use LRTs or AIC to find the most parsimonious fit. anova(fm4, fm5, fm6) ## refitting model(s) with ML (instead of REML) ## Data: rikz ## Models: ## fm4: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) ## fm5: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) + (0 + NAP | fBeach) ## fm6: sqrt(Richness) ~ fExp * NAP + (1 + NAP | fBeach) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## fm4 6 94.275 105.11 -41.137 82.275 ## fm5 7 94.418 107.06 -40.209 80.418 1.8565 1 0.1730 ## fm6 8 96.373 110.83 -40.186 80.373 0.0454 1 0.8312 Both AIC and the LRT suggest that the model with only random intercepts provides the most parsimonious fit. Let’s take a closer look at that fit summary(fm4) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) ## Data: rikz ## ## REML criterion at convergence: 89.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4532 -0.5066 -0.0280 0.3478 2.6221 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.1395 0.3735 ## Residual 0.3182 0.5641 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.7722 0.2047 7.3111 13.544 1.92e-06 *** ## fExp11 -0.9213 0.3096 7.5513 -2.976 0.0189 * ## NAP -0.8580 0.1207 37.7420 -7.110 1.82e-08 *** ## fExp11:NAP 0.3979 0.1818 37.1348 2.189 0.0350 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fExp11 NAP ## fExp11 -0.661 ## NAP -0.174 0.115 ## fExp11:NAP 0.115 -0.212 -0.664 We can analyze the fixed-effects as follows. Based on the contrasts R has used, the low exposure beaches are the baseline; thus the values for the “Intercept” and “NAP” coefficients give the intercept and slope for low-exposure beaches. The values of the “fExp11” and “fExp11:NAP” coefficients (resp.) give the differences of the intercepts and slopes (resp.) between the high vs. low exposure beaches. Thus the negative value of the “NAP” coefficient suggests that species richness declines as NAP increases at low-exposure beaches. The positive coefficient for the “fExp11:NAP” interaction suggests that species richness declines more gradually with increasing NAP at high-exposure beaches as compared to low-exposure beaches. Because we have analyzed the square-root transform of the response, it is hard to assign any biological meaning to the magnitudes of the coefficients. This is one downside of using a transformation to stabilize the variance in the response. We’ll visualize the model with a plot. a0 &lt;- fixef(fm4)[1] # mean intercept for low-exposure beaches b0 &lt;- fixef(fm4)[2] # mean slope for low-exposure beaches a1 &lt;- fixef(fm4)[3] # difference in mean intercepts for high vs low b1 &lt;- fixef(fm4)[4] # difference in mean slopes for high vs low c.mode &lt;- ranef(fm4)$fBeach low.beaches &lt;- c(1, 2, 5, 8, 9) # indices of the low-exposure beaches high.beaches &lt;- c(3, 4, 6, 7) # indices of the high-exposure beacues par(mfrow = c(1, 2)) # split the plot region with(rikz, plot(sqrt(Richness) ~ NAP, type = &quot;n&quot;, main = &quot;Exposure = 8 or 10&quot;)) # set up the axes with(subset(rikz, fExp == &quot;10&quot;), points(sqrt(Richness) ~ NAP, pch = Beach)) # plot points for low exposure beaches abline(a = a0, b = b0, col = &quot;red&quot;, lwd = 2) # add the average line for low-exposure beaches for (i in 1:length(low.beaches)) { abline(a = a0 + c.mode[low.beaches[i], 1], b = b0, col = &quot;red&quot;, lty = &quot;dotted&quot;) } # Repeat for high exposure beaches with(rikz, plot(sqrt(Richness) ~ NAP, type = &quot;n&quot;, main = &quot;Exposure = 11&quot;)) # set up the axes with(subset(rikz, fExp == &quot;11&quot;), points(sqrt(Richness) ~ NAP, pch = Beach)) # plot points for low exposure beaches abline(a = a0 + a1, b = b0 + b1, col = &quot;blue&quot;, lwd = 2) # add the average line for low-exposure beaches for (i in 1:length(high.beaches)) { abline(a = a0 + a1 + c.mode[high.beaches[i], 1], b = b0 + b1, col = &quot;blue&quot;, lty = &quot;dotted&quot;) } 6.5 Nested and crossed random effects 6.5.1 Nested random effects A major advantage of the lme4::lmer software is that it allows nested and crossed random effects. We will look first at an example of nested random effects. These data come from a study by Yates (1935), as reported in Venables &amp; Ripley (Modern Applied Statistics with S, 4e, 2002). They are found in the MASS library as the oats data. The description in the help documentation states: The yield of oats from a split-plot field trial using three varieties and four levels of manurial treatment. The experiment was laid out in 6 blocks of 3 main plots, each split into 4 sub-plots. The varieties were applied to the main plots and the manurial treatments to the sub-plots. Let’s first take a look at the data. rm(list = ls()) library(MASS) require(lme4) require(lmerTest) data(oats) summary(oats) ## B V N Y ## I :12 Golden.rain:24 0.0cwt:18 Min. : 53.0 ## II :12 Marvellous :24 0.2cwt:18 1st Qu.: 86.0 ## III:12 Victory :24 0.4cwt:18 Median :102.5 ## IV :12 0.6cwt:18 Mean :104.0 ## V :12 3rd Qu.:121.2 ## VI :12 Max. :174.0 The variables are (respectively) [B]lock, [V]ariety, [N]itrogen (the manure), and the [Y]ield, in units of 1/4lbs, according to the help documentation. To develop notation, let \\(i = 1, \\ldots, 3\\) index the three varieties, let \\(j = 1, \\ldots, 4\\) index the four manure treatments, and let \\(k = 1, \\ldots, 6\\) index the blocks. We wish to entertain the usual model for a split-plot design with a blocking factor at the whole-plot level: \\[\\begin{align} y_{ijk} &amp; \\sim \\mathcal{N}(\\mu_{ij} + B_k + W_{ik}, \\sigma^2_\\varepsilon) \\\\ B_k &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2_B) \\\\ W_{ik} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2_W). \\end{align}\\] Here, \\(\\mu_{ij}\\) is the average response for the combination of variety \\(i\\) and manure treatment \\(j\\); the \\(B_k\\) are the random effects for the blocks, the \\(W_{ik}\\) are the whole-plot errors, and the \\(\\varepsilon_{ijk}\\) are the split-plot (observation-level) errors. We proceed to fit the model using lmerTest::lmer. A key to the coding here is to notice that each combination of block and variety uniquely specifies one of the 18 whole plots. (In other words, variety is not replicated within the blocks.) Therefore, we can code the whole-plot random effect as (1 | B : V), which creats a random effect for each unique combination of block and variety. The rest of the model coding is straightforward. fm1 &lt;- lmerTest::lmer(Y ~ V * N + (1 | B) + (1 | B : V), data = oats) # for nested random effects, lmer provides the coding shortcut fm1a &lt;- lmerTest::lmer(Y ~ V * N + (1 | B / V), data = oats) summary(fm1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Y ~ V * N + (1 | B) + (1 | B:V) ## Data: oats ## ## REML criterion at convergence: 529 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.81301 -0.56145 0.01758 0.63865 1.57034 ## ## Random effects: ## Groups Name Variance Std.Dev. ## B:V (Intercept) 106.1 10.30 ## B (Intercept) 214.5 14.65 ## Residual 177.1 13.31 ## Number of obs: 72, groups: B:V, 18; B, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 80.0000 9.1070 16.0816 8.784 1.55e-07 *** ## VMarvellous 6.6667 9.7150 30.2308 0.686 0.4978 ## VVictory -8.5000 9.7150 30.2308 -0.875 0.3885 ## N0.2cwt 18.5000 7.6829 45.0000 2.408 0.0202 * ## N0.4cwt 34.6667 7.6829 45.0000 4.512 4.58e-05 *** ## N0.6cwt 44.8333 7.6829 45.0000 5.835 5.48e-07 *** ## VMarvellous:N0.2cwt 3.3333 10.8653 45.0000 0.307 0.7604 ## VVictory:N0.2cwt -0.3333 10.8653 45.0000 -0.031 0.9757 ## VMarvellous:N0.4cwt -4.1667 10.8653 45.0000 -0.383 0.7032 ## VVictory:N0.4cwt 4.6667 10.8653 45.0000 0.430 0.6696 ## VMarvellous:N0.6cwt -4.6667 10.8653 45.0000 -0.430 0.6696 ## VVictory:N0.6cwt 2.1667 10.8653 45.0000 0.199 0.8428 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) VMrvll VVctry N0.2cw N0.4cw N0.6cw VM:N0.2 VV:N0.2 VM:N0.4 ## VMarvellous -0.533 ## VVictory -0.533 0.500 ## N0.2cwt -0.422 0.395 0.395 ## N0.4cwt -0.422 0.395 0.395 0.500 ## N0.6cwt -0.422 0.395 0.395 0.500 0.500 ## VMrvll:N0.2 0.298 -0.559 -0.280 -0.707 -0.354 -0.354 ## VVctry:N0.2 0.298 -0.280 -0.559 -0.707 -0.354 -0.354 0.500 ## VMrvll:N0.4 0.298 -0.559 -0.280 -0.354 -0.707 -0.354 0.500 0.250 ## VVctry:N0.4 0.298 -0.280 -0.559 -0.354 -0.707 -0.354 0.250 0.500 0.500 ## VMrvll:N0.6 0.298 -0.559 -0.280 -0.354 -0.354 -0.707 0.500 0.250 0.500 ## VVctry:N0.6 0.298 -0.280 -0.559 -0.354 -0.354 -0.707 0.250 0.500 0.250 ## VV:N0.4 VM:N0.6 ## VMarvellous ## VVictory ## N0.2cwt ## N0.4cwt ## N0.6cwt ## VMrvll:N0.2 ## VVctry:N0.2 ## VMrvll:N0.4 ## VVctry:N0.4 ## VMrvll:N0.6 0.250 ## VVctry:N0.6 0.500 0.500 anova(fm1) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## V 526.1 263.0 2 10 1.4853 0.2724 ## N 20020.5 6673.5 3 45 37.6857 2.458e-12 *** ## V:N 321.8 53.6 6 45 0.3028 0.9322 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We proceed to analyze the fixed effects in the usual fashion, noting first that the variety-by-nitrogen interaction is not significant. We then proceed to inspect the \\(F\\)-tests of the fixed effects, and see that the marginal means for the manure treatment are significantly different, but there are no significant differences among the marginal means for the three varieties. Note that it would be incorrect to have coded the model as fm1.wrong &lt;- lmerTest::lmer(Y ~ V * N + (1 | B) + (1 | V), data = oats) as this would have treated the random effects for block and variety as crossed, not nested. To complete the analysis, we should notice that the levels of the nitrogen treatment correspond to equally spaced values of a numerical covariate. We can thus extract the polynomial trends by assigning polynomial contrasts to the nitrogen treatment. contrasts(oats$N) &lt;- contr.poly(n = 4) fm1 &lt;- lmerTest::lmer(Y ~ V * N + (1 | B) + (1 | B : V), data = oats) summary(fm1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Y ~ V * N + (1 | B) + (1 | B:V) ## Data: oats ## ## REML criterion at convergence: 533.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.81301 -0.56145 0.01758 0.63865 1.57034 ## ## Random effects: ## Groups Name Variance Std.Dev. ## B:V (Intercept) 106.1 10.30 ## B (Intercept) 214.5 14.65 ## Residual 177.1 13.31 ## Number of obs: 72, groups: B:V, 18; B, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 104.5000 7.7976 8.8688 13.402 3.45e-07 *** ## VMarvellous 5.2917 7.0789 10.0000 0.748 0.472 ## VVictory -6.8750 7.0789 10.0000 -0.971 0.354 ## N.L 33.6901 5.4327 45.0000 6.201 1.57e-07 *** ## N.Q -4.1667 5.4327 45.0000 -0.767 0.447 ## N.C -0.8199 5.4327 45.0000 -0.151 0.881 ## VMarvellous:N.L -4.8075 7.6829 45.0000 -0.626 0.535 ## VVictory:N.L 2.5715 7.6829 45.0000 0.335 0.739 ## VMarvellous:N.Q -1.9167 7.6829 45.0000 -0.249 0.804 ## VVictory:N.Q -1.0833 7.6829 45.0000 -0.141 0.888 ## VMarvellous:N.C 3.9877 7.6829 45.0000 0.519 0.606 ## VVictory:N.C -2.8696 7.6829 45.0000 -0.374 0.711 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) VMrvll VVctry N.L N.Q N.C VM:N.L VV:N.L VM:N.Q ## VMarvellous -0.454 ## VVictory -0.454 0.500 ## N.L 0.000 0.000 0.000 ## N.Q 0.000 0.000 0.000 0.000 ## N.C 0.000 0.000 0.000 0.000 0.000 ## VMrvlls:N.L 0.000 0.000 0.000 -0.707 0.000 0.000 ## VVictry:N.L 0.000 0.000 0.000 -0.707 0.000 0.000 0.500 ## VMrvlls:N.Q 0.000 0.000 0.000 0.000 -0.707 0.000 0.000 0.000 ## VVictry:N.Q 0.000 0.000 0.000 0.000 -0.707 0.000 0.000 0.000 0.500 ## VMrvlls:N.C 0.000 0.000 0.000 0.000 0.000 -0.707 0.000 0.000 0.000 ## VVictry:N.C 0.000 0.000 0.000 0.000 0.000 -0.707 0.000 0.000 0.000 ## VV:N.Q VM:N.C ## VMarvellous ## VVictory ## N.L ## N.Q ## N.C ## VMrvlls:N.L ## VVictry:N.L ## VMrvlls:N.Q ## VVictry:N.Q ## VMrvlls:N.C 0.000 ## VVictry:N.C 0.000 0.500 We see that only the linear trend of the nitrogen treatment is significant. Let’s make a quick plot to visualize this effect. with(oats, plot(Y ~ N)) 6.5.2 Crossed random effects To illustrate crossed random effects, we will model players’ scores from the 2018 US Open golf tournament. This data set includes scores for all 136 players who competed in the tournament. All players participated in the first two days of the tournament. Players who had a sufficiently low (good) total score from those first two days qualified to compete in the next two days. Players who with a high (poor) total score from the first two days were disqualified or “cut” from the tournament. To develop notation, let \\(i=1, \\ldots, 136\\) index the players, and let \\(j = 1, \\ldots, 4\\) index the days. We seek to fit the model \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(\\mu + A_i + B_j, \\sigma^2_\\varepsilon) \\\\ A_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2_A) \\\\ B_j &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2_B). \\end{align}\\] In this model, \\(\\mu\\) is the average score, \\(A_i\\) are the player-level random effects, \\(B_j\\) are the day-level random effects, and \\(\\varepsilon_{ij}\\) are the observation-level errors. Note that because each player played at most once in each day, there is no possibility to separate a possible player-by-day interaction (also a random effect) from the observation-level error. If the players had played multiple rounds in a given day, we could have tried to separate the player-by-day random effect from the observation-level error. We fit the model in lmerTest::lmer. rm(list = ls()) golf &lt;- read.table(&quot;data/golf.txt&quot;, head = T) fm1 &lt;- lmerTest::lmer(score ~ 1 + (1 | player) + (1 | round), data = golf) summary(fm1) # comparison of the std devs of the random effects is interesting ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: score ~ 1 + (1 | player) + (1 | round) ## Data: golf ## ## REML criterion at convergence: 2130.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6856 -0.6669 -0.0515 0.5825 4.5577 ## ## Random effects: ## Groups Name Variance Std.Dev. ## player (Intercept) 0.5059 0.7113 ## round (Intercept) 3.1513 1.7752 ## Residual 11.2507 3.3542 ## Number of obs: 400, groups: player, 136; round, 4 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 74.2631 0.9081 3.0093 81.78 3.9e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It is interesting to compare the standard deviations of the random effects. It is also interesting to use the profile function to see the asymmetry in the confidence intervals for these standard deviations. pp.golf &lt;- profile(fm1) confint(pp.golf) ## 2.5 % 97.5 % ## .sig01 0.0000000 1.551146 ## .sig02 0.8129721 3.837297 ## .sigma 3.0513011 3.666234 ## (Intercept) 72.2434040 76.265786 lattice::xyplot(pp.golf, absVal = TRUE) We can also extract the conditional modes for the players and rounds. player.modes &lt;- ranef(fm1)$player head(player.modes) ## (Intercept) ## Akiyoshi 0.3983034 ## Aphibarnrat -0.3069054 ## Axley -0.0142805 ## Babcock 0.1920115 ## Baddeley -0.1925652 ## Berger -0.4212456 (round.modes &lt;- ranef(fm1)$round) ## (Intercept) ## rd1 1.7211234 ## rd2 -0.9012137 ## rd3 1.1661848 ## rd4 -1.9860945 According to Wikipedia, on day 1 “conditions were extremely difficult as gusty winds hung around all day with sunny skies, making the course firm and fast.” This corresponds with the large conditional mode for round 1. Finally, it is interesting to compare the conditional modes for the players who qualified to play in rounds 3 and 4, vs. the players who were “cut”. player.stats &lt;- data.frame(name = row.names(player.modes), mode = player.modes[, 1], rds = with(golf, as.numeric(table(player)))) with(player.stats, stripchart(mode ~ as.factor(rds), method = &quot;jitter&quot;, ylab = &quot;rounds played&quot;, xlab = &quot;conditional mode&quot;, pch = 1)) Interestingly, some players who qualified to play in rounds 3 and 4 ended up with higher (worse) conditional modes than some of the players who were “cut”. We might infer that these players played above their abilities on days 1 and 2. Thanks to xkcd for the perspective. "],["generalized-linear-models.html", "Chapter 7 Generalized linear models 7.1 Poisson regression 7.2 Bayesian interlude 7.3 Binary responses 7.4 Zero-adjusted models for count data 7.5 Generalized additive models (GAMs)", " Chapter 7 Generalized linear models Generalized linear modes allow us to extend the machinery of the “general linear model” (regression and ANOVA) to data sets in which the response variable may have a non-Gaussian distribution. Generalized linear models do not encompass all possible distributions for the response variable. Instead, the distribution of the response variable must belong to a group of distributions known as the “exponential family”. (Note that there is also such a thing as an exponential distribution. The exponential distribution is one of the distributions in the exponential family, but it is not the only one.) The exponential family of distributions includes many of the distributions that we encounter in practical data analysis, including Poisson, negative binomial, binomial, gamma, and beta distributions. The Gaussian distribution is included in the exponential family as well. One notable distribution that is not part of the exponential family is the \\(t\\)-distribution. Distributions in the exponential family all give rise to likelihoods that share the same general form, and thus can be handled with a unified fitting scheme. In practice, logistic regression (with binomial responses) and Poisson regression are far and away the two most common forms of generalized linear models that one encounters. 7.1 Poisson regression We will begin with an example of Poisson regression. These data are originally from Poole, Anim. Behav. 37 (1989):842-49, and were analyzed in the second edition of Ramsey &amp; Schafer’s Statistical Sleuth. They describe an observational study of 41 male elephants over 8 years at Amboseli National Park in Kenya. Each record in this data set gives the age of a male elephant at the beginning of a study and the number of successful matings for the elephant over the study’s duration. The number of matings is a count variable. Our goal is to characterize how the number of matings is related to the elephant’s age. We’ll start by fitting a model with the canonical log link. elephant &lt;- read.table(&quot;data/elephant.txt&quot;, head = T) head(elephant) ## age matings ## 1 27 0 ## 2 28 1 ## 3 28 1 ## 4 28 1 ## 5 28 3 ## 6 29 0 with(elephant, plot(matings ~ age)) fm1 &lt;- glm(matings ~ age, family = poisson(link = &quot;log&quot;), data = elephant) # log link is the default summary(fm1) ## ## Call: ## glm(formula = matings ~ age, family = poisson(link = &quot;log&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.58201 0.54462 -2.905 0.00368 ** ## age 0.06869 0.01375 4.997 5.81e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 51.012 on 39 degrees of freedom ## AIC: 156.46 ## ## Number of Fisher Scoring iterations: 5 Thus the so-called pseudo-\\(R^2\\) for the model with the log link is \\[ \\mathrm{pseudo}-R^2 = 1 - \\frac{51.012}{75.372} = 32.3\\% \\] We can visualize the fit by plotting a best-fitting line with a 95% confidence interval. Because the scale parameter is not estimated here, we will use a critical value from a standard normal distribution. Later, when we estimate the scale parameter based on data, we will use a critical value from a \\(t\\)-distribution instead. new.data &lt;- data.frame(age = seq(from = min(elephant$age), to = max(elephant$age), length = 100)) predict.fm1 &lt;- predict(fm1, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm1$fit, col = &quot;red&quot;) # add lines for standard errors lines(x = new.data$age, y = predict.fm1$fit - 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm1$fit + 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) While the canonical link is a natural starting point, we are free to try other link functions as well. Below, we try the identity link and plot the fit. fm2 &lt;- glm(matings ~ age, family = poisson(link = &quot;identity&quot;), data = elephant) summary(fm2) ## ## Call: ## glm(formula = matings ~ age, family = poisson(link = &quot;identity&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.55205 1.33916 -3.399 0.000676 *** ## age 0.20179 0.04023 5.016 5.29e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 50.058 on 39 degrees of freedom ## AIC: 155.5 ## ## Number of Fisher Scoring iterations: 5 predict.fm2 &lt;- predict(fm2, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm2$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm2$fit - 1.96 * predict.fm2$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm2$fit + 1.96 * predict.fm2$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) Note that the choice of the link function has a substantial impact on the shape of the fit. The canonical (log) link suggests that the average number of matings increases with age at an accelerating rate, while the identity link suggests that the average number of matings increases steadily with age. The AIC favors the identity link here. We can also have a look at the residuals to see if they suggest any model deficiencies. In general, we prefer the deviance residuals, so we will look at them. plot(x = elephant$age, y = residuals(fm2, type = &quot;deviance&quot;), xlab = &quot;age&quot;, ylab = &quot;Deviance residuals&quot;) abline(h = 0, lty = &quot;dashed&quot;) The residuals do not suggest any deficiency in the fit. For this fit, the residual deviance suggests a small amount of overdispersion. To be on the safe side, we can fit a quasi-Poisson model in which the scale (overdispersion) parameter is estimated from the data. Note that when we estimate the overdispersion parameter, the estimates of the model parameters do not change, but their standard errors increase. Consequently, the uncertainty in the fit increases as well. In this case, however, the increase is so slight that it is barely noticeable. fm3 &lt;- glm(matings ~ age, family = quasipoisson(link = &quot;identity&quot;), data = elephant) summary(fm3) ## ## Call: ## glm(formula = matings ~ age, family = quasipoisson(link = &quot;identity&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.55205 1.42164 -3.202 0.00272 ** ## age 0.20179 0.04271 4.725 2.97e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 1.126975) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 50.058 on 39 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 predict.fm3 &lt;- predict(fm3, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm3$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm3$fit + qt(0.025, df = 39) * predict.fm3$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm3$fit + qt(0.975, df = 39) * predict.fm3$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) As an alternative, we could fit a model that uses a negative binomial distribution for the response. Negative binomial distributions belong to the exponential family, so we can fit them using the GLM framework. However, the authors of glm did not include a negative binomial family in their initial code. Venables &amp; Ripley’s MASS package includes a program called glm.nb which is specifically designed for negative binomial responses. MASS::glm.nb uses the parameterization familiar to ecologists, although they use the parameter \\(\\theta\\) instead of \\(k\\). So, in their notation, if \\(y \\sim \\mathrm{NB}(\\mu, \\theta)\\), then \\(\\mathrm{Var}(y) = \\mu + \\mu^2/\\theta\\). require(MASS) ## Loading required package: MASS fm4 &lt;- glm.nb(matings ~ age, link = identity, data = elephant) summary(fm4) ## ## Call: ## glm.nb(formula = matings ~ age, data = elephant, link = identity, ## init.theta = 15.80269167) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.56939 1.45770 -3.135 0.00172 ** ## age 0.20232 0.04428 4.569 4.9e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(15.8027) family taken to be 1) ## ## Null deviance: 64.836 on 40 degrees of freedom ## Residual deviance: 43.214 on 39 degrees of freedom ## AIC: 156.87 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 15.8 ## Std. Err.: 23.0 ## ## 2 x log-likelihood: -150.872 predict.fm4 &lt;- predict(fm4, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm4$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm4$fit + 1.96 * predict.fm4$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm4$fit - 1.96 * predict.fm4$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) Notice that \\(\\hat{\\theta} = 15.8\\), again indicating that the extra-Poisson variation is mild. Notice also that the error bounds on the fitted curve are ever so slightly larger than the error bounds from the Poisson fit, and nearly identical to the error bounds from the quasi-Poisson fit. 7.2 Bayesian interlude 7.2.1 rstanarm We will take a look at how Poisson regression can be handled from a Bayesian perspective. This interlude will also allow us to introduce a few more Bayesian concepts. We’ll first fit the Poisson regression with identity link using rstanarm::stan_glm. require(rstanarm) ## Loading required package: rstanarm ## Loading required package: Rcpp ## This is rstanarm version 2.21.4 ## - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors! ## - Default priors may change, so it&#39;s safest to specify priors, even if equivalent to the defaults. ## - For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()) fm5 &lt;- stan_glm(matings ~ age, family = poisson(link = &quot;identity&quot;), data = elephant, seed = 1) print(fm5, digits = 3) ## stan_glm ## family: poisson [identity] ## formula: matings ~ age ## observations: 41 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) -4.420 1.358 ## age 0.198 0.041 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg We can visualize the marginal posterior of the slope using bayesplot::mcmc_areas: posterior.fm5 &lt;- as.matrix(fm5) bayesplot::mcmc_areas(posterior.fm5, pars = c(&quot;age&quot;), prob = 0.95) Notice that the slope of the fitted regression line is slightly attenuated relative to the ML fit from the frequentist analysis. The reason is that the default priors selected by rstanarm are actually reasonably strong: prior_summary(fm5) ## Priors for model &#39;fm5&#39; ## ------ ## Intercept (after predictors centered) ## ~ normal(location = 0, scale = 2.5) ## ## Coefficients ## Specified prior: ## ~ normal(location = 0, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 0, scale = 0.38) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details rstanarm parameterizes the normal distribution by its mean and standard deviation. Thus, the prior for the slope has a mean of 0 and standard deviation of 2.5. (I think: I don’t yet understand the difference between the ‘specified prior’ and the ‘adjusted prior’.) This relatively strong prior reflects the views of the developers of rstanarm, who think there is benefit to shrinking most estimated effects slightly towards 0. If we want to override this default and use weaker priors, we can do so as follows. fm5 &lt;- stan_glm(matings ~ age, family = poisson(link = &quot;identity&quot;), data = elephant, prior = normal(0, 100), prior_intercept = normal(0, 100), seed = 1) print(fm5, digits = 3) ## stan_glm ## family: poisson [identity] ## formula: matings ~ age ## observations: 41 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) -4.444 1.320 ## age 0.200 0.039 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg Weakening the prior results in a posterior mean for the slope that is closer to the MLE from the frequentist analysis. We can visualize the fit in the usual way, by plotting sample draws from the MCMC sampler. mcmc.sims &lt;- as.matrix(fm5) with(elephant, plot(matings ~ age, type = &quot;n&quot;)) sample.sims &lt;- sample(nrow(mcmc.sims), size = 100) for (i in sample.sims) { abline(a = mcmc.sims[i, 1], b = mcmc.sims[i, 2], col = &quot;skyblue&quot;) } abline(a = coef(fm5)[&#39;(Intercept)&#39;], b = coef(fm5)[&#39;age&#39;], col = &quot;blue&quot;, lwd = 2) with(elephant, points(matings ~ age, pch = 16)) 7.2.2 Posterior predictive checks How might we assess the quality of this fit? A Bayesian approach to assessing the quality of fit is to generate posterior predictive distributions. A “posterior predictive” data set is a new data set of hypothetical observations simulated using the parameter values from one of the samples of the MCMC sampler. By creating many such data sets, we can build up a distribution of data sets predicted by the posterior distribution of the model parameters. By comparing the actual observed data to this distribution, we can gain a sense of whether our data are consistent with the posterior fit. The authors of rstanarm advocate using posterior predictive checks extensively to assess model adequacy. For that reason, they have included a number of tools that make posterior predictive checking simple. For example, the command rstanarm::posterior_predict generates a collection of hypothetical data sets by randomly sampling from the posterior distribution of the parameters. The default behavior is to generate a collection of 1000 such data sets. post.predict &lt;- posterior_predict(fm5) For example, we can have a look at a few of these hypothetical data sets. par(mfrow = c(2, 2)) for(i in 1:4) { with(elephant, plot(matings ~ age, type = &quot;n&quot;)) points(elephant$age, post.predict[i, ], pch = 1) } Our task is to determine if the real data set that we actually observed is consistent with this collection of hypothetical data sets. To do so, we can calculate a summary statistic that captures an interesting feature of each data set, and compare the value of that summary statistic for the real data set to the distribution of values across the simulated data sets. What summary statistic would you use to compare the real elephant data set to these simulated ones? We’ll use the marginal standard deviation of the number of matings below, but there might be other or better examples that you could think of. Typically, one uses several posterior predictive statistics, not just one. First, we calculate the distribution of marginal standard deviations across the collection of simulated data sets. This R code illustrates the use of the handy R command apply, which applies a function to one of the margins (rows or columns) of an array. Here, because the individual data sets are stored as the rows of post.predict, we will calculate the standard deviation of the contents of each row. post.predict.sd &lt;- apply(post.predict, 1, sd) Now we plot a histogram of these standard deviations and compare the distribution to the marginal standard deviation of the real data. hist(post.predict.sd, breaks = 20, xlab = &quot;Marginal SD of response&quot;) abline(v = sd(elephant$matings), col = &quot;red&quot;, lwd = 2) The actual standard deviation, shown by the red line, is consistent with the standard deviations of the simulated data sets, indicating that the model provides a reasonable description of the data, at least with regard to the marginal SD of the response variable. This is consistent with our previous conclusions that the extra-Poisson variation in these data is quite mild. The authors of rstanarm have provided the function pp_check (for [p]osterior [p]redictive check) to do all of this in one fell swoop. pp_check(fm5, plotfun = &quot;stat&quot;, stat = &quot;sd&quot;, binwidth = .1) 7.2.3 JAGS We can also code the model up directly in JAGS. Notice that the code here is barely more complicated than OLS regression. The code below shows the JAGS implementation of the elephant GLM with a Poisson distribution for the response and a log link. require(R2jags) ## Loading required package: R2jags ## Loading required package: rjags ## Loading required package: coda ## Linked to JAGS 4.3.1 ## Loaded modules: basemod,bugs ## ## Attaching package: &#39;R2jags&#39; ## The following object is masked from &#39;package:coda&#39;: ## ## traceplot elephant.model &lt;- function() { for (j in 1:J) { # J = number of data points y[j] ~ dpois(mu[j]) # data model: the likelihood mu[j] &lt;- exp(eta[j]) # inverse link eta[j] &lt;- a + b * x[j] # linear predictor } a ~ dnorm (0.0, 1E-6) # prior for intercept b ~ dnorm (0.0, 1E-6) # prior for slope } jags.data &lt;- list(y = elephant$matings, x = elephant$age, J = nrow(elephant)) jags.params &lt;- c(&quot;a&quot;, &quot;b&quot;) jags.inits &lt;- function(){ list(&quot;a&quot; = rnorm(1), &quot;b&quot; = rnorm(1)) } set.seed(1) jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = elephant.model, n.chains = 3, n.iter = 5E4, n.thin = 5) ## module glm loaded Let’s have a look at the posterior summaries: mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) apply(mcmc.output, 2, mean) ## a b deviance ## -1.59257609 0.06871434 154.47531083 HPDinterval(as.mcmc(mcmc.output), prob = 0.95) ## lower upper ## a -2.64905619 -0.54223141 ## b 0.04194793 0.09511553 ## deviance 152.45783408 158.39245393 ## attr(,&quot;Probability&quot;) ## [1] 0.95 Compare these to the values generated by the glm fit with the log link. We can also visualize the uncertainty in the fit by plotting the fit implied by a random sample of parameters from the posterior: plot(matings ~ age, data = elephant, type = &quot;n&quot;) subset.samples &lt;- sample(nrow(mcmc.output), size = 100) for(i in subset.samples) { a &lt;- mcmc.output$a[i] b &lt;- mcmc.output$b[i] my.fit &lt;- function(x) exp(a + b * x) curve(my.fit, from = min(elephant$age), to = max(elephant$age), col = &quot;deepskyblue&quot;, lwd = 1, add = TRUE) } with(elephant, points(matings ~ age, pch = 16)) post.means &lt;- apply(mcmc.output, 2, mean) my.fit &lt;- function(x) exp(post.means[&#39;a&#39;] + post.means[&#39;b&#39;] * x) curve(my.fit, from = min(elephant$age), to = max(elephant$age), col = &quot;blue&quot;, lwd = 2, add = TRUE) Suppose we wanted to fit this model using JAGS with the identity link, so that the relationship between mean matings and age was linear. To do so, we would have to be careful, because we would have to make sure that the MCMC sampler did not stray into regions that generated negative values for the observation-level mean, \\(\\mu\\). Mathematically, we simply want to construct a likelihood that evaluates to 0 whenever \\(\\mu \\leq 0\\). However, as of this writing, I haven’t figured out what the best approach is for implementing such a model in JAGS. 7.2.4 Observation-level random effects We use this opportunity to introduce an alternative approach to accommodating overdispersion in models with non-Gaussian responses. This alternative approach entails adding a latent Gaussian-distributed random effect to the linear predictor. Because this random effect takes a unique value for each observation, it is known as a “observation-level random effect”. Observation-level random effects are motivated by the idea that overdispersion is caused by additional inputs or factors that vary among observations and re not included in the model. For example, knowing next to nothing about elephants, we might speculate that there are additional factors that influence a male elephant’s mating success above and beyond age, such as (say) nutrition. The observation-level random effects are meant to capture this additional variation. Importantly, if we use an observation-level random effect, then we have no control on the sign of the linear predictor \\(\\eta\\). Consequently, we must use a link function that maps any value of the linear predictor (\\(\\eta\\)) to a valid value of the observation-level mean (\\(\\mu\\)). (In other words, we can’t use the identity link for a Poisson response.) For the elephant data, adding an observation-level random effect changes the model to: \\[\\begin{align*} y_i &amp; \\sim \\mathrm{Pois}(\\mu_i)\\\\ \\mu_i &amp; = \\exp({\\eta_i}) \\\\ \\eta_i &amp; \\sim \\mathcal{N}(a + b x_i, \\sigma^2_\\eta). \\end{align*}\\] Priors on \\(a\\), \\(b\\), and \\(\\sigma^2_\\eta\\) would complete the Bayesian specification. Implemeting the model in JAGS is straightforward: elephant.model.2 &lt;- function() { for (j in 1:J) { # J = number of data points y[j] ~ dpois(mu[j]) # data model: the likelihood mu[j] &lt;- exp(eta[j]) # inverse link eta[j] ~ dnorm(a + b * x[j], tau_eps) # linear predictor, includes observation-level random effect } a ~ dnorm (0.0, 1E-6) # prior for intercept b ~ dnorm (0.0, 1E-6) # prior for slope tau_eps ~ dgamma (0.01, 0.01) sd_eps &lt;- pow(tau_eps, -1/2) } jags.data &lt;- list(y = elephant$matings, x = elephant$age, J = nrow(elephant)) jags.params &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;sd_eps&quot;) jags.inits &lt;- function(){ list(&quot;a&quot; = rnorm(1), &quot;b&quot; = rnorm(1), &quot;tau_eps&quot; = runif(1)) } set.seed(1) jagsfit2 &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = elephant.model.2, n.chains = 3, n.iter = 5E4, n.thin = 5) Again, we can have a look at the posterior summaries. mcmc.output &lt;- as.data.frame(jagsfit2$BUGSoutput$sims.list) apply(mcmc.output, 2, mean) ## a b deviance sd_eps ## -1.65360318 0.06934144 147.14813848 0.27075058 HPDinterval(as.mcmc(mcmc.output), prob = 0.95) ## lower upper ## a -2.85901410 -0.4407061 ## b 0.03818321 0.1005042 ## deviance 133.86486164 158.2878871 ## sd_eps 0.06376511 0.5261329 ## attr(,&quot;Probability&quot;) ## [1] 0.95 Plotting the posterior fit in this case is more subtle. With the observation-level random effect, we now have a model that includes a random effect behind a non-linear link function. In other words, we have a generalizd linear mixed model (GLMM). The combination of a random effect and a non-linear link function means that there are two different ways to calculate the fitted value — the so-called “marginal means” and the “conditional means”. We will defer a discussion of this distinction until we discuss GLMMs in greater detail. Accordingly, we defer the plot as well. Formally, we can assess the need for the observation-level random effect by comparing the two models with the deviance information criterion (DIC), an information criterion that can be used to compare Bayesian models in the same way that AIC can be used to compare frequentist fits. DIC is provided as a portion to the output from the print command applied to a JAGS model fit. print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpywblRj/model62a8671b38a3.txt&quot;, fit using jags, ## 3 chains, each with 50000 iterations (first 25000 discarded), n.thin = 5 ## n.sims = 15000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a -1.593 0.544 -2.656 -1.961 -1.582 -1.223 -0.546 1.001 12000 ## b 0.069 0.014 0.042 0.059 0.069 0.078 0.095 1.001 9200 ## deviance 154.475 2.737 152.508 153.036 153.848 155.210 159.742 1.001 15000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.7 and DIC = 158.2 ## DIC is an estimate of expected predictive error (lower deviance is better). print(jagsfit2) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpywblRj/model62a83a311810.txt&quot;, fit using jags, ## 3 chains, each with 50000 iterations (first 25000 discarded), n.thin = 5 ## n.sims = 15000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a -1.654 0.622 -2.913 -2.065 -1.640 -1.233 -0.482 1.004 680 ## b 0.069 0.016 0.039 0.059 0.069 0.080 0.101 1.003 850 ## sd_eps 0.271 0.132 0.081 0.167 0.252 0.352 0.574 1.001 15000 ## deviance 147.148 6.377 133.573 142.883 147.837 151.760 158.083 1.001 15000 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 20.3 and DIC = 167.5 ## DIC is an estimate of expected predictive error (lower deviance is better). In this case, the DIC favors the model without the observation-level random effect. Before moving on, it is worth noting that this Poisson regression with an observation-level random effect is the type of model that is much easier to fit from a Bayesian perspective. To approach this model from the frequentist perspective, we would have to write down, and maximize, the likelihood, which is a function of the model’s three parameters: \\(a\\), \\(b\\), and \\(\\sigma^2_\\eta\\). However, to write down the likelihood, we would have to integrate out the observation-level random effects. For the elephant data, this would correspond to an integral with 41 dimensions! While we could conceivably write this down, for all intents and purposes the likelihood would be impossible to compute, let alone to maximize. From a Bayesian perspective, though, adding latent random effects to a model with a non-normal response is nearly trivial. The above discussion begs the question of just how programs like lme4::lmer fit mixed-effect models in the first place. It turns out that, with a Gaussian response, the high-dimensional integral in the likelihood can be simplified without having to compute the entire integral numerically. This simplification flows from the fact that Gaussian random variates have a natural multivariate form. With a non-Gaussian response, things become substantially harder. When we study generalized linear mixed models, we will see some ideas for computing these likelihood functions, but all such approaches ultimately rely on computational approximations instead of evaluating the underlying integrals directly. 7.3 Binary responses We generally distinguish between two types of data with binary responses: Data in which each individual record is a separate a binary response, and data in which each record consists of a group of binary observations. The same methods can be used for either type of data. We will begin by studying a data set with individual binary responses, and then use the industrial melanism data to illustrate grouped binary responses. 7.3.1 Individual binary responses: TB in boar To illustrate individual binary data, we will use a data set analyzed by Zuur et al. (2009) in their Ch. 10. As explained by Zuur et al., these data describe the incidence of “tuberculosis-like lesions in wild boar Sus scrofa” in southern Spain, and were originally collected by Vicente et al. (2006). The potential explanatory variables in the data set include a measure of the animal’s size, it’s sex, and a grouping into one of four age classes. Preparatory work: boar &lt;- read.table(&quot;data/boar.txt&quot;, head = T) # remove incomplete records boar &lt;- na.omit(boar) # convert sex to a factor boar$SEX &lt;- as.factor(boar$SEX) names(boar) &lt;- c(&quot;tb&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;length&quot;) summary(boar) ## tb sex age length ## Min. :0.0000 1:206 Min. :1.000 Min. : 46.5 ## 1st Qu.:0.0000 2:288 1st Qu.:3.000 1st Qu.:107.0 ## Median :0.0000 Median :3.000 Median :122.0 ## Mean :0.4575 Mean :3.142 Mean :117.3 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:130.4 ## Max. :1.0000 Max. :4.000 Max. :165.0 We’ll fit the usual logistic regression model first, considering only the animal’s size as a predictor. Size in this case is a measure of the length of the animal, in cm. fm1 &lt;- glm(tb ~ length, family = binomial(link = &quot;logit&quot;), data = boar) summary(fm1) ## ## Call: ## glm(formula = tb ~ length, family = binomial(link = &quot;logit&quot;), ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.137107 0.695381 -5.949 2.69e-09 *** ## length 0.033531 0.005767 5.814 6.09e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 641.23 on 492 degrees of freedom ## AIC: 645.23 ## ## Number of Fisher Scoring iterations: 4 with(boar, plot(tb ~ length)) # add a line for the fitted probabilities of tb new.data &lt;- data.frame(length = seq(from = min(boar$length), to = max(boar$length), length = 100)) predict.fm1 &lt;- predict(fm1, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) lines(x = new.data$length, y = predict.fm1$fit, col = &quot;red&quot;) # add lines for standard errors # use critical value from z distribution here because # the scale parameter is not estimated lines(x = new.data$length, y = predict.fm1$fit - 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x = new.data$length, y = predict.fm1$fit + 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) Regression coefficients in logistic regression can be a bit hard to interpret. One interpretation flows from exponentiating the regression coefficient to obtain an odds ratio. For the boar data, the regression coefficient of 0.0335 corresponds to an odds ratio of \\(e^{0.0335}\\) = 1.034. This means that for two boars that differ by one cm in length, the larger boar’s odds of having a TB-like lesion will be 1.034 times the smaller boar’s odds of having such a lesion. Overdispersion is typically not an issue with individual binary response data. Nonetheless, the pseudo-\\(R^2\\) here is fairly low. We can try the probit and complementary log-log links to see if we obtain a better fit: # probit link fm1a &lt;- glm(tb ~ length, family = binomial(link = &quot;probit&quot;), data = boar) # complementary log-log link fm1b &lt;- glm(tb ~ length, family = binomial(link = &quot;cloglog&quot;), data = boar) AIC(fm1, fm1a, fm1b) ## df AIC ## fm1 2 645.2265 ## fm1a 2 645.2665 ## fm1b 2 645.6100 # make a plot to compare the fits with the different links predict.fm1a &lt;- predict(fm1a, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) predict.fm1b &lt;- predict(fm1b, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(boar, plot(tb ~ length)) lines(x = new.data$length, y = predict.fm1$fit, col = &quot;red&quot;, lwd = 2) lines(x = new.data$length, y = predict.fm1a$fit, col = &quot;blue&quot;, lwd = 2) lines(x = new.data$length, y = predict.fm1b$fit, col = &quot;forestgreen&quot;, lwd = 2) legend(&quot;left&quot;, leg = c(&quot;logit&quot;, &quot;probit&quot;, &quot;cloglog&quot;), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;forestgreen&quot;), pch = 16) The logit and probit links are nearly identical. The complementary log-log link differs slightly, but the logit link is AIC-best. Try adding sex and age class as predictors. Some of the MLEs cannot be found, because none of the individuals with and are infected, thus the MLE of the log odds of infection for this group (which happens to be the baseline) is \\(-\\infty\\). This phenomenon is known as “complete separation”. # fit a model with sex, age (as a categorical predictor) and their interaction fm2 &lt;- glm(tb ~ length + sex * as.factor(age), family = binomial, data = boar) summary(fm2) ## ## Call: ## glm(formula = tb ~ length + sex * as.factor(age), family = binomial, ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -16.55356 724.50177 -0.023 0.982 ## length 0.01840 0.01253 1.469 0.142 ## sex2 14.19739 724.50190 0.020 0.984 ## as.factor(age)2 13.83446 724.50169 0.019 0.985 ## as.factor(age)3 14.31136 724.50191 0.020 0.984 ## as.factor(age)4 14.68141 724.50219 0.020 0.984 ## sex2:as.factor(age)2 -14.53254 724.50204 -0.020 0.984 ## sex2:as.factor(age)3 -14.36861 724.50196 -0.020 0.984 ## sex2:as.factor(age)4 -14.53354 724.50196 -0.020 0.984 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 635.43 on 485 degrees of freedom ## AIC: 653.43 ## ## Number of Fisher Scoring iterations: 14 with(boar, table(tb, age, sex)) ## , , sex = 1 ## ## age ## tb 1 2 3 4 ## 0 4 37 37 28 ## 1 0 14 34 52 ## ## , , sex = 2 ## ## age ## tb 1 2 3 4 ## 0 7 40 62 53 ## 1 2 11 48 65 There are several possible remedies here. The first is to try to reduce the number of parameters in the model, perhaps by eliminating the interaction between sex and age class. # fit a model with sex, age (as a categorical predictor) and their interaction fm3 &lt;- glm(tb ~ length + sex + as.factor(age), family = binomial, data = boar) summary(fm3) ## ## Call: ## glm(formula = tb ~ length + sex + as.factor(age), family = binomial, ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.67730 1.07306 -2.495 0.0126 * ## length 0.01959 0.01237 1.584 0.1133 ## sex2 -0.24297 0.19354 -1.255 0.2093 ## as.factor(age)2 -0.19847 0.92641 -0.214 0.8304 ## as.factor(age)3 0.33908 1.06938 0.317 0.7512 ## as.factor(age)4 0.59041 1.20582 0.490 0.6244 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 637.41 on 488 degrees of freedom ## AIC: 649.41 ## ## Number of Fisher Scoring iterations: 4 A second option is to use so-called ``exact’’ methods for inference. There doesn’t appear to be a good package available for implementing these methods in R. Other software packages might be necessary. 7.3.2 Grouped binary data: Industrial melanism We’ll start with the standard logistic regression model for the industrial melanism data. We are primarily interested in determining if the effect of color morph on removal rate changes with distance from Liverpool. For grouped binary data, we need to specify both the number of “successes” and number of “failures” as the response variable in the model. Here, we use cbind to create a two-column matrix with the number of “successes” (moths removed) in the first column, and the number of “failures” (moths not removed) in the second column. See the help documentation for glm for more details. moth &lt;- read.table(&quot;data/moth.txt&quot;, head = TRUE, stringsAsFactors = TRUE) fm1 &lt;- glm(cbind(removed, placed - removed) ~ morph * distance, family = binomial(link = &quot;logit&quot;), data = moth) summary(fm1) ## ## Call: ## glm(formula = cbind(removed, placed - removed) ~ morph * distance, ## family = binomial(link = &quot;logit&quot;), data = moth) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.128987 0.197906 -5.705 1.17e-08 *** ## morphlight 0.411257 0.274490 1.498 0.134066 ## distance 0.018502 0.005645 3.277 0.001048 ** ## morphlight:distance -0.027789 0.008085 -3.437 0.000588 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 35.385 on 13 degrees of freedom ## Residual deviance: 13.230 on 10 degrees of freedom ## AIC: 83.904 ## ## Number of Fisher Scoring iterations: 4 Grouped binary data are often overdispersed relative to the variance implied by a binomial distribution. In this case, we would call the overdispersion “extra-binomial” variation. As with count data, we can deal with overdispersion through a quasi-likelihood approach: fm1q &lt;- glm(cbind(removed, placed - removed) ~ morph * distance, family = quasibinomial(link = &quot;logit&quot;), data = moth) summary(fm1q) ## ## Call: ## glm(formula = cbind(removed, placed - removed) ~ morph * distance, ## family = quasibinomial(link = &quot;logit&quot;), data = moth) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.128987 0.223104 -5.060 0.000492 *** ## morphlight 0.411257 0.309439 1.329 0.213360 ## distance 0.018502 0.006364 2.907 0.015637 * ## morphlight:distance -0.027789 0.009115 -3.049 0.012278 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasibinomial family taken to be 1.270859) ## ## Null deviance: 35.385 on 13 degrees of freedom ## Residual deviance: 13.230 on 10 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 As with count data, using quasi-likelihood to estimate the scale (or dispersion) parameter increases the estimates of the standard errors of the coefficients by an amount equal to the square root of the estimated scale parameter. The \\(t\\)-test of the interaction between color morph and distance indicates that there is a statistically significant difference in how the proportion of moth removes changes over the distance transect between the two color morphs. plot(x = moth$distance, y = residuals(fm1q, type = &quot;deviance&quot;), xlab = &quot;distance&quot;, ylab = &quot;Deviance residuals&quot;) abline(h = 0, lty = &quot;dashed&quot;) The plot of the residuals suggests that we should include a random effect for the sampling station. This makes complete sense. The data for the two color morphs at each station share whatever other characteristics make the station unique, and are thus correlated. To account for this correlation, we need to introduce a random effect for the station. This again gets us into the world of generalized linear mixed models. Before proceeding, we’ll write the model down. Let \\(i=1,2\\) index the two color morphs, and let \\(j = 1, \\ldots, 7\\) index the stations. Let \\(y_{ij}\\) be the number of moths removed, let \\(n_{ij}\\) be the number of moths placed, and let \\(x_j\\) be the distance of the station from Liverpool. We wish to fit the model \\[\\begin{align*} y_{ij} &amp; \\sim \\mathrm{Binom}(p_{ij}, n_{ij})\\\\ \\mathrm{logit}(p_{ij}) &amp; = \\eta_{ij} \\\\ \\eta_{ij} &amp; = a_i + b_i x_j + L_j \\\\ L_j &amp; \\sim \\mathcal{N}(0, \\sigma^2_L) \\end{align*}\\] The \\(L_j\\)’s are our [l]ocation-specific random effects that capture any other station-to-station differences above and beyond the station’s distance from Liverpool. (It turns out that an observation-level random effect does not improve the model, at least as indicated by DIC.) Because this model includes both a random effect for the station and a non-Gaussian response, it is a generalized linear mixed model (GLMM). We postpone our discussion accordingly. 7.4 Zero-adjusted models for count data In count data, the frequency of zeros often differs from what the standard Poisson or negative binomial distributions would predict. This occurs because the processes that lead to zero counts are often separate from the processes that govern the intensity of non-zero counts. For example, when counting parasite loads of animals, one process may determine if an individual animal is infected at all, and a second process may determine the intensity of the parasite load for those animals that are infected. This section discusses models for count data in which the proportion of zeros is modeled separately from the rest of the distribution. Any of these models may be paired with Poisson or negative binomial models for the non-zero counts. 7.4.1 Zero-truncated models Zero-truncated distributions are simply those that condition on the count variable being greater than 0. They are appropriate when the sampling procedure makes zeros unobservable. Zero-truncated Poisson and negative binomial distributions are available as something of a side benefit the R library VGAM (an acronym for vector generalized additive models). This package provides the function vglm (a version of glm that allows for vector-valued responses), which in turn accommodates the distribution families pospoisson and posnegbinomial. The “pos” portion of each refers to the fact that the counts must be strictly positive. We illustrate the use of a zero-truncated distribution with a species-abundance data set for butterflies provided in the VGAM package. The description of the data set in VGAM reads: “About 3300 individual butterflies were caught in Malaya by naturalist Corbet trapping butterflies. They were classified to about 500 species.” The data give the frequencies in a species abundance distribution. require(VGAM) data(corbet) head(corbet) ## ofreq species ## 1 1 118 ## 2 2 74 ## 3 3 44 ## 4 4 24 ## 5 5 29 ## 6 6 22 with(corbet, barplot(species, names = ofreq, xlab = &quot;no. of individuals&quot;, ylab = &quot;frequency&quot;)) We will fit a zero-truncated negative binomial distribution to these data, and compare the fitted zero-truncated distribution to the data. corbet.fit &lt;- vglm(ofreq ~ 1, family = posnegbinomial, weights = species, data = corbet) Coef(corbet.fit) ## munb size ## 4.4802271 0.4906395 mu.hat &lt;- Coef(corbet.fit)[&quot;munb&quot;] k.hat &lt;- Coef(corbet.fit)[&quot;size&quot;] # 2023-05-26: dgainnbinom seems to have been deprecated from VGAM # fitted.probs &lt;- dgaitnbinom(x = corbet$ofreq, k.hat, munb.p = fitted(corbet.fit), truncate = 0) # fitted.vals &lt;- sum(corbet$species) * fitted.probs # barplot(cbind(corbet$species, fitted.vals), beside = T, names = c(&quot;actual&quot;, &quot;fitted&quot;)) 7.4.2 Zero-inflated models As the name suggests, zero-inflated (henceforth ZI) models are appropriate when zeros are more frequent than the distribution of the non-zero counts would suggest. ZI models have two components. In the “zero-inflation” component, a logistic regression (or some other generalized linear model for a binary response) captures whether the response is a “false” zero or a realization from a proper count distribution. In the “count” component, a generalized linear model for count data (such as Poisson regression) is used to model the distribution of the non-false-zero counts. The ZI model accords with the thinking that the data arise from a combination of two separate mechanisms: a first mechanism that determines whether or not the count is a “false” zero, and a second that determines the count if it is not a false zero. In ecology, the zero-inflation component might correspond to (true) presence vs. absence, and the count component might correspond to the detected intensity conditional on presence. To illustrate ZI models, we will use the cod parasite data described in \\(\\S\\) 11.3.2 of Zuur et al. (2009). These data are counts of the number of trypanosome blood parasites in individual cod, and were initially reported in Hemmingsen et al. (2005). First we load the data and do some housekeeping. cod &lt;- read.table(&quot;data/ParasiteCod.txt&quot;, head = T) # remove observations with missing data cod &lt;- na.omit(cod) summary(cod) ## Sample Intensity Prevalence Year ## Min. : 1.0 Min. : 0.000 Min. :0.0000 Min. :1999 ## 1st Qu.: 299.5 1st Qu.: 0.000 1st Qu.:0.0000 1st Qu.:1999 ## Median : 598.0 Median : 0.000 Median :0.0000 Median :2000 ## Mean : 613.7 Mean : 6.209 Mean :0.4534 Mean :2000 ## 3rd Qu.: 955.5 3rd Qu.: 4.000 3rd Qu.:1.0000 3rd Qu.:2001 ## Max. :1254.0 Max. :257.000 Max. :1.0000 Max. :2001 ## Depth Weight Length Sex ## Min. : 50.0 Min. : 34 Min. : 17.00 Min. :0.000 ## 1st Qu.:110.0 1st Qu.: 769 1st Qu.: 44.00 1st Qu.:1.000 ## Median :180.0 Median :1446 Median : 54.00 Median :1.000 ## Mean :176.3 Mean :1718 Mean : 53.53 Mean :1.427 ## 3rd Qu.:236.0 3rd Qu.:2232 3rd Qu.: 62.00 3rd Qu.:2.000 ## Max. :293.0 Max. :9990 Max. :101.00 Max. :2.000 ## Stage Age Area ## Min. :0.000 Min. : 0.000 Min. :1.000 ## 1st Qu.:1.000 1st Qu.: 3.000 1st Qu.:2.000 ## Median :1.000 Median : 4.000 Median :3.000 ## Mean :1.426 Mean : 4.118 Mean :2.568 ## 3rd Qu.:2.000 3rd Qu.: 5.000 3rd Qu.:3.000 ## Max. :4.000 Max. :10.000 Max. :4.000 There are a large number of potential predictors. Following the analysis in Zuur et al., we will focus on the effects of length (a measure of the fish’s size), year (which we will treat as a categorical predictor for flexibility), and “area”, a categorical predictor for one of four areas in which the fish was sampled. Before beginning, we will plot the data. In the plot below, each row of the plot corresponds to a year, and each column (unlabeled) corresponds to one of the four areas, going from area 1 (the leftmost column) to area 4 (the rightmost column). Because most of the parasite loads are zero or close to zero, we plot parasite load on a log(y + 1) scale. par(mfrow = c(3, 4), mar = c(2, 2, 1, 1), oma = c(3, 7, 0, 0), las = 1) for (i in unique(cod$Year)) { for (j in sort(unique(cod$Area))) { with(subset(cod, Year == i &amp; Area == j), plot(log(Intensity + 1) ~ Length, xlim = range(cod$Length), ylim = log(range(cod$Intensity) + 1), xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;)) axis(2, at = log(c(0, 10, 100) + 1), lab = c(0, 10, 100)) if (j == 1) mtext(i, side = 2, line = 3) } } mtext(&quot;length&quot;, side = 1, outer = T, line = 2) mtext(&quot;parasite intensity&quot;, side = 2, outer = T, line = 5, las = 0) With ZI models, one can use separate model combinations of predictors for the two components. Indeed, if we think of the two components as capturing two different natural processes, then there is no reason that the predictors should have the same affect on both components. Following Zuur et al., however, we will start with a ZINB model that includes length, year, area, and an interaction between year and area for both components. Because both components are generalized linear models, both include a link function. Here, we will use the default logit link for the zero-inflation component and log link for the count component. To fit ZI models, we will use the pscl library from R. The name pscl is an acronym for political science computing laboratory. The pscl library includes functions for ZI poisson and negative binomial models (henceforth abbreviated ZIP and ZINB models). require(pscl) ## Loading required package: pscl ## Classes and Methods for R developed in the ## Political Science Computational Laboratory ## Department of Political Science ## Stanford University ## Simon Jackman ## hurdle and zeroinfl functions by Achim Zeileis formula.1 &lt;- formula(Intensity ~ Length + as.factor(Year) * as.factor(Area)) cod.nb.fm1 &lt;- zeroinfl(formula.1, data = cod, dist = &quot;negbin&quot;) summary(cod.nb.fm1) ## ## Call: ## zeroinfl(formula = formula.1, data = cod, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -0.6142 -0.4429 -0.3581 -0.1252 11.4529 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.733905 0.344398 10.842 &lt; 2e-16 *** ## Length -0.036410 0.005108 -7.128 1.02e-12 *** ## as.factor(Year)2000 0.063894 0.295639 0.216 0.82889 ## as.factor(Year)2001 -0.939183 0.606056 -1.550 0.12122 ## as.factor(Area)2 0.197761 0.329133 0.601 0.54794 ## as.factor(Area)3 -0.646736 0.277793 -2.328 0.01991 * ## as.factor(Area)4 0.707493 0.252266 2.805 0.00504 ** ## as.factor(Year)2000:as.factor(Area)2 -0.653983 0.535418 -1.221 0.22192 ## as.factor(Year)2001:as.factor(Area)2 0.967305 0.718165 1.347 0.17801 ## as.factor(Year)2000:as.factor(Area)3 1.024957 0.429599 2.386 0.01704 * ## as.factor(Year)2001:as.factor(Area)3 1.002732 0.677502 1.480 0.13886 ## as.factor(Year)2000:as.factor(Area)4 0.534524 0.414976 1.288 0.19772 ## as.factor(Year)2001:as.factor(Area)4 0.855195 0.654394 1.307 0.19126 ## Log(theta) -0.966631 0.096345 -10.033 &lt; 2e-16 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.19094 0.78133 0.244 0.806941 ## Length -0.03884 0.01203 -3.227 0.001249 ** ## as.factor(Year)2000 -1.07163 2.00007 -0.536 0.592100 ## as.factor(Year)2001 3.29316 0.71043 4.635 3.56e-06 *** ## as.factor(Area)2 2.01367 0.57289 3.515 0.000440 *** ## as.factor(Area)3 1.90528 0.54988 3.465 0.000530 *** ## as.factor(Area)4 -0.73626 0.86228 -0.854 0.393187 ## as.factor(Year)2000:as.factor(Area)2 0.46531 2.07876 0.224 0.822883 ## as.factor(Year)2001:as.factor(Area)2 -3.20740 0.83596 -3.837 0.000125 *** ## as.factor(Year)2000:as.factor(Area)3 -0.79465 2.15756 -0.368 0.712642 ## as.factor(Year)2001:as.factor(Area)3 -3.50409 0.83000 -4.222 2.42e-05 *** ## as.factor(Year)2000:as.factor(Area)4 -13.65190 1572.67468 -0.009 0.993074 ## as.factor(Year)2001:as.factor(Area)4 -2.91045 1.10462 -2.635 0.008419 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 0.3804 ## Number of iterations in BFGS optimization: 50 ## Log-likelihood: -2450 on 27 Df The neatly organized output gives coefficients for each component of the model, with the count component presented first and the zero-inflation component presented second. The output for the zero-inflation component shows a very large standard error associated with the coefficient for area 4 in the year 2000. Let’s take a look at the frequency of zeros, as broken down by year and area. with(cod, table(Year, Area, Intensity &gt; 0)) ## , , = FALSE ## ## Area ## Year 1 2 3 4 ## 1999 57 65 121 34 ## 2000 16 31 32 6 ## 2001 63 72 112 42 ## ## , , = TRUE ## ## Area ## Year 1 2 3 4 ## 1999 89 27 45 69 ## 2000 39 18 43 44 ## 2001 7 34 45 80 We see that there are only a few zeros in area 4 for 2000. Thus, the zero-inflation component is trying to fit a zero probability of zero inflation for area 4 in year 2000, leading to complete separation. We need to reduce the number of parameters somehow. Lacking any better ideas, we’ll follow Zuur et al. and remove the interaction between year and area for the zero-inflation portion of the model. To do so, we need a model formula that differs between the two model components. In pscl, we implement this model by providing a model formula where the differing predictor combinations for the two components are separated by a vertical bar. formula.2 &lt;- formula(Intensity ~ Length + as.factor(Year) * as.factor(Area) | Length + as.factor(Year) + as.factor(Area)) cod.nb.fm2 &lt;- zeroinfl(formula.2, data = cod, dist = &quot;negbin&quot;) summary(cod.nb.fm2) ## ## Call: ## zeroinfl(formula = formula.2, data = cod, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -0.5911 -0.4480 -0.3794 -0.1258 12.0795 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.78614 0.34093 11.105 &lt; 2e-16 *** ## Length -0.03669 0.00502 -7.309 2.70e-13 *** ## as.factor(Year)2000 0.03438 0.29188 0.118 0.90624 ## as.factor(Year)2001 -2.32204 0.38125 -6.091 1.12e-09 *** ## as.factor(Area)2 0.12404 0.32906 0.377 0.70621 ## as.factor(Area)3 -0.82582 0.27838 -2.966 0.00301 ** ## as.factor(Area)4 0.65502 0.25266 2.592 0.00953 ** ## as.factor(Year)2000:as.factor(Area)2 -0.75547 0.50817 -1.487 0.13711 ## as.factor(Year)2001:as.factor(Area)2 2.39047 0.52066 4.591 4.41e-06 *** ## as.factor(Year)2000:as.factor(Area)3 1.21457 0.42003 2.892 0.00383 ** ## as.factor(Year)2001:as.factor(Area)3 2.52720 0.45772 5.521 3.36e-08 *** ## as.factor(Year)2000:as.factor(Area)4 0.59338 0.41572 1.427 0.15347 ## as.factor(Year)2001:as.factor(Area)4 2.23077 0.44602 5.001 5.69e-07 *** ## Log(theta) -1.01444 0.09655 -10.507 &lt; 2e-16 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.92837 0.70657 1.314 0.188873 ## Length -0.04686 0.01252 -3.744 0.000181 *** ## as.factor(Year)2000 -1.25039 0.50281 -2.487 0.012889 * ## as.factor(Year)2001 0.25485 0.32785 0.777 0.436962 ## as.factor(Area)2 1.62679 0.48056 3.385 0.000711 *** ## as.factor(Area)3 1.19153 0.49143 2.425 0.015324 * ## as.factor(Area)4 -1.26323 0.64393 -1.962 0.049792 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 0.3626 ## Number of iterations in BFGS optimization: 29 ## Log-likelihood: -2460 on 21 Df There’s a lot of output here to process. One observation we might make is that there seems to be a strongly significant negative association between length and both the probability of a false zero in the first model component, and the parasite abundance in the second model component. To visualize the model, we might plot the fitted probability of a false zero vs. length for each combination of area and year, and then plot the fitted intensity (mean) of the count process vs. length, also for each combination of area and year. See Ch. 11 of Zuur et al. for such plots. Alternatively, we might merge the two model components to generate predicted values as a function of length for each year and area. The library pscl provides methods to extract predicted values directly; see the help documentation for pscl::predict.zeroinfl. We use that method here to generate predicted mean parasite counts (merging the zero-inflation and count components) for each combination of year and area. In the plot below, bear in mind that because the data are shown on a log scale, the fitted line will not necessarily pass through the center of the plotted data cloud. length.vals &lt;- seq(from = min(cod$Length), to = max(cod$Length), length = 100) par(mfrow = c(3, 4), mar = c(2, 2, 1, 1), oma = c(3, 7, 0, 0), las = 1) for (i in unique(cod$Year)) { for (j in sort(unique(cod$Area))) { new.data &lt;- data.frame(Length = length.vals, Year = i, Area = j) predicted.vals &lt;- predict(cod.nb.fm2, newdata = new.data, type = &quot;response&quot;) plot(x = range(cod$Length), y = log(range(cod$Intensity) + 1), xlab = &quot;&quot;, ylab = &quot;&quot;, type = &quot;n&quot;, yaxt = &quot;n&quot;) lines(log(predicted.vals + 1) ~ length.vals) with(subset(cod, Year == i &amp; Area == j), points(log(Intensity + 1) ~ Length)) axis(2, at = log(c(0, 10, 100) + 1), lab = c(0, 10, 100)) if (j == 1) mtext(i, side = 2, line = 3) } } mtext(&quot;length&quot;, side = 1, outer = T, line = 2) mtext(&quot;parasite intensity&quot;, side = 2, outer = T, line = 6, las = 0) 7.4.3 Zero-altered, or “hurdle”, models Zero-altered (ZA) models, also known as hurdle models, are similar to ZI models in the sense that there are two components. However, in ZA models, the binary component models whether the response is 0 or greater than 0. This binary component of the model is called the hurdle component. The count component is then modeled using a zero-truncated distribution. Consequently, in ZA models, the proportion of zeros may be either greater or less than the distribution of the non-zero counts suggests. The analysis path for ZA models is the same as it is for ZI models. The two components of the model can be contemplated independently of one another, and either a zero-truncated Poisson or negative binomial distribution can be used for the count component. The pscl library contains the hurdle function for fitting ZA modes. However, in ZA models, the two components are entirely separate from one another, so one doesn’t really need additional specialized software. ZA models could be fit just as readily by bolting together a logistic regression for the hurdle component with a model for zero-truncated responses in the count component. This is not true for ZI models, though. For ZI models, the estimated proportion of false zeros affects the estimate of the intensity for the non-false-zero counts, so the two components cannot be fit separately from one another. We will use the cod parasite data to illustrate hurdle models also. This is something of an artificial example, however, because the fact that some infected fish will probably yield samples with zero counts suggests that a ZI model has a more natural ecological interpretation. We will proceed directly to the final model chosen by Zuur et al., which includes additive effects of length, year, and area in the intensity component, and an interaction between area and year (but no effect of length) in the hurdle component. formula.3 &lt;- formula(Intensity ~ Length + as.factor(Year) + as.factor(Area) | as.factor(Year) * as.factor(Area)) cod.hurdle.fm3 &lt;- hurdle(formula.3, data = cod, dist = &quot;negbin&quot;) summary(cod.hurdle.fm3) ## ## Call: ## hurdle(formula = formula.3, data = cod, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -0.6387 -0.3825 -0.3312 -0.1209 11.9366 ## ## Count model coefficients (truncated negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.127498 0.387197 8.077 6.62e-16 *** ## Length -0.036524 0.005678 -6.432 1.26e-10 *** ## as.factor(Year)2000 0.384774 0.202231 1.903 0.0571 . ## as.factor(Year)2001 -0.153655 0.198692 -0.773 0.4393 ## as.factor(Area)2 0.404208 0.265277 1.524 0.1276 ## as.factor(Area)3 0.021424 0.231773 0.092 0.9264 ## as.factor(Area)4 1.060123 0.223756 4.738 2.16e-06 *** ## Log(theta) -1.591345 0.256348 -6.208 5.37e-10 *** ## Zero hurdle model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.4456 0.1696 2.627 0.00863 ** ## as.factor(Year)2000 0.4454 0.3419 1.303 0.19273 ## as.factor(Year)2001 -2.6428 0.4330 -6.103 1.04e-09 *** ## as.factor(Area)2 -1.3241 0.2850 -4.647 3.37e-06 *** ## as.factor(Area)3 -1.4347 0.2434 -5.893 3.78e-09 *** ## as.factor(Area)4 0.2622 0.2696 0.972 0.33085 ## as.factor(Year)2000:as.factor(Area)2 -0.1105 0.5071 -0.218 0.82758 ## as.factor(Year)2001:as.factor(Area)2 2.7711 0.5322 5.207 1.92e-07 *** ## as.factor(Year)2000:as.factor(Area)3 0.8392 0.4493 1.868 0.06182 . ## as.factor(Year)2001:as.factor(Area)3 2.7201 0.4991 5.450 5.05e-08 *** ## as.factor(Year)2000:as.factor(Area)4 0.8393 0.5918 1.418 0.15612 ## as.factor(Year)2001:as.factor(Area)4 2.5794 0.5174 4.985 6.19e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta: count = 0.2037 ## Number of iterations in BFGS optimization: 21 ## Log-likelihood: -2448 on 20 Df 7.5 Generalized additive models (GAMs) Recall that splines are essentially just another way of specifying the predictors in a regression model. Generalized additive models (GAMs) are to additive models what generalized linear models are to (general) linear models. In other words, GAMs use splines to create a linear predictor in a GLM that is a smooth function of a covariate. We illustrate GAMs by considering a data set that gives the size and annual survival of colonies of the stony coral Acropora millepora. (Recall that stony corals are a colonial organism, so here a coral “colony” means a colony of polyps. A colony is what most naive observers would recognize as an individual coral.) We wish to understand how the size of the colony is related to its survival. The size of the colony is measured as planar (top-down) area. The units of area are unclear; perhaps it is m\\(^2\\)? In the data file, the variable “mortality” is a binary response coded as a 0 if the colony survived, and as a 1 if the colony died. The size data have been log-transformed. These data are from Madin et al. (2014). In R, we fit the GAM with the function mgcv::gam. require(mgcv) ## Loading required package: mgcv ## Loading required package: nlme ## This is mgcv 1.8-42. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. ## ## Attaching package: &#39;mgcv&#39; ## The following object is masked from &#39;package:VGAM&#39;: ## ## s coral &lt;- read.csv(&quot;data/coral.csv&quot;, head = TRUE, stringsAsFactors = TRUE) head(coral) ## species growth_form ln_area mortality ## 1 Acropora millepora corymbose -3.427419 0 ## 2 Acropora millepora corymbose -3.964138 0 ## 3 Acropora millepora corymbose -3.682131 0 ## 4 Acropora millepora corymbose -5.144650 0 ## 5 Acropora millepora corymbose -3.814405 0 ## 6 Acropora millepora corymbose -3.661057 0 with(coral, plot(jitter(mortality, amount = 0.02) ~ ln_area, xlab = &quot;log area&quot;, ylab = &quot;mortality&quot;)) We fit a GAM with a binomial response, logit link, and use a smoothing spline to capture the relationship between log size and (the log odds of) mortality. Recall that a smoothing spline determines the degree of smoothness by generalized cross-validation. fm1 &lt;- gam(mortality ~ s(ln_area), family = binomial(link = &quot;logit&quot;), data = coral) summary(fm1) ## ## Family: binomial ## Link function: logit ## ## Formula: ## mortality ~ s(ln_area) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.6068 0.1667 -9.636 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(ln_area) 1.921 2.433 10.52 0.0089 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.0532 Deviance explained = 4.79% ## UBRE = -0.098118 Scale est. = 1 n = 270 # plot data with fit overlaid with(coral, plot(jitter(mortality, amount = 0.02) ~ ln_area, xlab = &quot;log area&quot;, ylab = &quot;mortality&quot;)) x.vals &lt;- with(coral, seq(from = min(ln_area), to = max(ln_area), length = 100)) fm1.fit &lt;- predict(fm1, newdata = data.frame(ln_area = x.vals), se = TRUE) inv.logit &lt;- function(x) exp(x) / (1 + exp(x)) lines(x.vals, inv.logit(fm1.fit$fit)) lines(x.vals, inv.logit(fm1.fit$fit + 1.96 * fm1.fit$se.fit), lty = &quot;dashed&quot;) lines(x.vals, inv.logit(fm1.fit$fit - 1.96 * fm1.fit$se.fit), lty = &quot;dashed&quot;) We might also try a complementary log-log link. fm2 &lt;- gam(mortality ~ s(ln_area), family = binomial(link = &quot;cloglog&quot;), data = coral) with(coral, plot(jitter(mortality, amount = 0.02) ~ ln_area, xlab = &quot;log area&quot;, ylab = &quot;mortality&quot;)) fm2.fit &lt;- predict(fm2, newdata = data.frame(ln_area = x.vals), se = TRUE) lines(x.vals, inv.logit(fm2.fit$fit), col = &quot;red&quot;) lines(x.vals, inv.logit(fm2.fit$fit + 1.96 * fm2.fit$se.fit), col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x.vals, inv.logit(fm2.fit$fit - 1.96 * fm2.fit$se.fit), col = &quot;red&quot;, lty = &quot;dashed&quot;) AIC(fm1, fm2) ## df AIC ## fm1 2.920822 243.5082 ## fm2 2.947335 243.0539 The AIC slightly favors the complementary log-log link. Bibliography "],["generalized-linear-mixed-models.html", "Chapter 8 Generalized linear mixed models 8.1 Example 1: Industrial melanism data 8.2 Example 2: Ticks on red grouse 8.3 GAMMs", " Chapter 8 Generalized linear mixed models 8.1 Example 1: Industrial melanism data We will examine several possible approaches to analyzing the industrial melanism data. The original source for these data is Bishop (1972); I obtained them from Ramsey and Schafer (2002). Recall that these data consist of paired binomial responses with two covariates: distance from Liverpool (a station-level covariate) and color morph (an observation-level covariate). In notation, the model that we seek to fit is \\[\\begin{align*} y_{ij} &amp; \\sim \\mathrm{Binom}(p_{ij}, n_{ij})\\\\ \\mathrm{logit}(p_{ij}) &amp; = \\eta_{ij} \\\\ \\eta_{ij} &amp; = a_i + b_i x_j + L_j \\\\ L_j &amp; \\sim \\mathcal{N}(0, \\sigma^2_L) \\end{align*}\\] where \\(i=1,2\\) indexes the two color morphs, \\(j = 1, \\ldots, 7\\) indexes the stations, \\(y_{ij}\\) is the number of moths removed, \\(n_{ij}\\) is the number of moths placed, and let \\(x_j\\) is the distance of the station from Liverpool. We are most interested in learning about the difference \\(b_1 - b_2\\), which quantifies how the relationship between log odds of removal and distance differs between the two color morphs, and determining whether there is evidence that this difference \\(\\neq 0\\). Alternatively, we might prefer to consider the quantity \\(e^{b_1 - b_2} = e^{b_1} / e^{b_2}\\), which tells us how the odds ratio for removal changes between the two morphs as distance increases. This odds ratio is a bit closer to something that we can mentall grasp. In terms of the odds ratio, we are interested in learning if the odds ratio \\(\\neq 1\\). Before proceeding, we note that one approach is simply to regress the difference of the empirical logits vs. distance. This reduces the problem to a simple regression. We try this approach first and use it as a benchmark. The data set used here is reformatted to include one record for each of the 7 stations. moth2 &lt;- read.table(&quot;data/moth2.txt&quot;, head = TRUE, stringsAsFactors = TRUE) head(moth2, n = 3) ## location distance morph l.placed l.removed d.placed d.removed ## 1 sp 0.0 light 56 17 56 14 ## 2 ef 7.2 light 80 28 80 20 ## 3 ha 24.1 light 52 18 52 22 elogit &lt;- function(x) log(x / (1 - x)) moth2$elogit.diff &lt;- with(moth2, elogit(d.removed / d.placed) - elogit(l.removed / l.placed)) fm1 &lt;- lm(elogit.diff ~ distance, data = moth2) with(moth2, plot(elogit.diff ~ distance, xlab = &quot;distance from city center (km)&quot;, ylab = &quot;difference in log odds of removal, dark - light&quot;)) abline(h = 0, lty = &quot;dashed&quot;) abline(fm1) summary(fm1) ## ## Call: ## lm(formula = elogit.diff ~ distance, data = moth2) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 0.10557 -0.30431 0.03501 0.26395 -0.09387 0.29714 -0.30349 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.373830 0.192503 -1.942 0.10980 ## distance 0.027579 0.005997 4.599 0.00585 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2698 on 5 degrees of freedom ## Multiple R-squared: 0.8088, Adjusted R-squared: 0.7706 ## F-statistic: 21.15 on 1 and 5 DF, p-value: 0.005846 confint(fm1) ## 2.5 % 97.5 % ## (Intercept) -0.86867531 0.12101544 ## distance 0.01216371 0.04299419 This approach tells us that the difference in log-odds slopes (defined as dark morph - light morph) is 0.0276, with a 95% confidence interval of (0.012, 0.043). This corresponds to an odds ratio of 1.028, with a 95% confidence interval of (1.012, 1.044). In other words, with every additional km from the city center, the odds ratio for a dark moth’s removal vs. a light moth’s removal increases by about 2.8%. The major disadvantage to the approach above is that it doesn’t account for the fact that differing numbers of moths were placed at the different stations. We could try to account for this with a weighted regression, but it’s not clear what the weights should be. We were also fortunate in the sense that there were no instances of either none or all of the moths being removed at a particular station, which would have led to an infinite empirical logit. 8.1.1 GEEs Next we try a GEE with a compound symmetry (“exchangable”) correlation structure imposed on the pair of measurements at each station. Because there are only two data records for each station, there is no loss of generality in assuming this correlation structure. We fit the model using geepack::geeglm. require(geepack) ## Loading required package: geepack moth &lt;- read.table(&quot;data/moth.txt&quot;, head = TRUE, stringsAsFactors = TRUE) contrasts(moth$morph) &lt;- contr.treatment(n = 2, base = 2) fm2 &lt;- geeglm(cbind(removed, placed - removed) ~ distance * morph, family = binomial(link = &quot;logit&quot;), data = moth, id = location, corstr = &quot;exchangeable&quot;) summary(fm2) ## ## Call: ## geeglm(formula = cbind(removed, placed - removed) ~ distance * ## morph, family = binomial(link = &quot;logit&quot;), data = moth, id = location, ## corstr = &quot;exchangeable&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) -0.714717 0.129102 30.648 3.09e-08 *** ## distance -0.009385 0.003221 8.489 0.00357 ** ## morph1 -0.410238 0.163953 6.261 0.01234 * ## distance:morph1 0.027762 0.005812 22.815 1.78e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = exchangeable ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 0.01313 0.006935 ## Link = identity ## ## Estimated Correlation Parameters: ## Estimate Std.err ## alpha 0.3911 0.298 ## Number of clusters: 7 Maximum cluster size: 2 The estimate of the difference between slopes on the log-odds scale is 0.0278, with an approximate 95% confidence interval of (0.0164, 0.0391). This corresponds to an odds ratio of 1.028, with an approximate 95% confidence interval of (1.017, 1.040). To visualize the model, we might plot the fitted proportion removed vs. distance for both color morphs. Bear in mind that fitted values here correspond to marginal mean removal rates. inv.logit &lt;- function(x) exp(x) / (1 + exp(x)) light.fit &lt;- function(d) inv.logit(-0.71472 - 0.00938 * d) dark.fit &lt;- function(d) inv.logit(-0.71472 - 0.41024 + (-0.00938 + 0.02776) * d) curve(dark.fit, from = min(moth$distance), to = max(moth$distance), xlab = &quot;distance from city center (km)&quot;, ylab = &quot;proportion removed&quot;, ylim = c(0.15, 0.5)) curve(light.fit, from = min(moth$distance), to = max(moth$distance), xlab = &quot;distance from city center (km)&quot;, ylab = &quot;proportion removed&quot;, add = TRUE, lty = &quot;dashed&quot;) with(subset(moth, morph == &quot;dark&quot;), points(removed / placed ~ distance, pch = 16)) with(subset(moth, morph == &quot;light&quot;), points(removed / placed ~ distance, pch = 1)) For the sake of comparing marginal means to conditional means, we will consider the predicted removal rate of dark morphs at a hypothetical location 20 km from the city center. This predicted removal rate is 0.319. dark.fit(20) ## [1] 0.3192197 8.1.2 GLMMs Next, we will fit the same model with lme4::glmer. require(lme4) ## Loading required package: lme4 ## Loading required package: Matrix fm3 &lt;- glmer(cbind(removed, placed - removed) ~ distance * morph + (1 | location), family = binomial(link = &quot;logit&quot;), data = moth) summary(fm3) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: cbind(removed, placed - removed) ~ distance * morph + (1 | location) ## Data: moth ## ## AIC BIC logLik deviance df.resid ## 85.7 88.9 -37.8 75.7 9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.73965 -0.41890 0.02967 0.66584 1.08052 ## ## Random effects: ## Groups Name Variance Std.Dev. ## location (Intercept) 0.01148 0.1072 ## Number of obs: 14, groups: location, 7 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.719786 0.205488 -3.503 0.000460 *** ## distance -0.009341 0.006270 -1.490 0.136243 ## morph1 -0.411128 0.274765 -1.496 0.134578 ## distance:morph1 0.027819 0.008094 3.437 0.000588 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) distnc morph1 ## distance -0.843 ## morph1 -0.641 0.538 ## dstnc:mrph1 0.558 -0.660 -0.859 confint(fm3, parm = c(&quot;distance:morph1&quot;)) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## distance:morph1 0.01201418 0.04377205 Nothing here is radically different. The parameter estimates are so similar to those from the GEE that a plot of the GEE and GLMM fits would be indistinguishable to the eye. Although summary.glmer doesn’t report the deviance, functions exist to obtain this information. We can use the deviance to assess overdispersion in the same way that we would in a GLM. deviance(fm3) ## [1] 9.250077 df.residual(fm3) ## [1] 9 The ratio of the deviance to the residual df is approximately 1, suggesting that the data are not overdispersed. Note that including the location random effect in the GLMM has eliminated the mild overdispersion that we detected in the GLM without the location random effect. To get a sense of how the conditional means compare to the marginal means, we will compute the conditional mean removal rate of dark morphs at a distance 20 km from the city center. dark.linpred.glmm &lt;- function(d) -0.71979 - 0.41113 + (-0.00934 + 0.02782) * d dark.fit.glmm &lt;- function(d) inv.logit(dark.linpred.glmm(d)) dark.fit.glmm(20) ## [1] 0.3183597 The conditional mean of the predicted removal rate is 0.318. Here, the difference between the marginal and conditional means is tiny. Nevertheless, we can gain a deeper understanding of the difference by taking a look at the fitted population of possible locations at 20 km distance on both the linear predictor scale and the data scale. linpred.sample &lt;- rnorm(1e6, mean = dark.linpred.glmm(20), sd = 0.1072) prob.sample &lt;- inv.logit(linpred.sample) (conditional.mean &lt;- inv.logit(dark.linpred.glmm(20))) ## [1] 0.3183597 (marginal.mean &lt;- mean(prob.sample)) ## [1] 0.3187751 par(mfrow = c(1, 2)) hist(linpred.sample, breaks = 50, xlab = &quot;linear predictor&quot;, main = &quot;&quot;) hist(prob.sample, breaks = 50, xlab = &quot;removal probability&quot;, main = &quot;&quot;) abline(v = conditional.mean, col = &quot;darkorange&quot;, lwd =2) abline(v = marginal.mean, col = &quot;blue&quot;, lwd = 2) We see that the variance of the location-level random effect is small enough that the inverse logit transformation is effectively linear. Thus, the distribution of removal probabilities across locations is nearly normal, and the conditional and marginal means nearly coincide. The estimate of the marginal mean that we have generated by simulation is not quite the same as the marginal mean generated by the GEE, which could either be due to the stochastic sampling that we have used above, and/or small numerical differences in the estimation. For the sake of illustration, we repeat these calculations by supposing that the location-to-location standard deviation was 10 times larger. linpred.sample &lt;- rnorm(1e6, mean = dark.linpred.glmm(20), sd = 10 * 0.1072) prob.sample &lt;- inv.logit(linpred.sample) (conditional.mean &lt;- inv.logit(dark.linpred.glmm(20))) ## [1] 0.3183597 (marginal.mean &lt;- mean(prob.sample)) ## [1] 0.3504195 par(mfrow = c(1, 2)) hist(linpred.sample, breaks = 50, xlab = &quot;linear predictor&quot;, main = &quot;&quot;) hist(prob.sample, breaks = 50, xlab = &quot;removal probability&quot;, main = &quot;&quot;) abline(v = conditional.mean, col = &quot;darkorange&quot;, lwd =2) abline(v = marginal.mean, col = &quot;blue&quot;, lwd = 2) 8.1.3 Bayesian fit We now fit the model using JAGS and vague priors. require(R2jags) moth.model &lt;- function() { for (j in 1:J) { # J = number of data points y[j] ~ dbin(p[j], n[j]) # data distribution p[j] &lt;- ilogit(eta[j]) # inverse link eta[j] &lt;- a[morph[j]] + b[morph[j]] * dist[j] + L[loc[j]] # linear predictor, } for (j in 1:7){ # random effects for location L[j] ~ dnorm(0, tau_L) } a[1] ~ dnorm (0.0, 1E-6) # priors for intercept a[2] ~ dnorm (0.0, 1E-6) # priors for intercept b[1] ~ dnorm (0.0, 1E-6) # prior for slope b[2] ~ dnorm (0.0, 1E-6) # prior for slope tau_L ~ dgamma (0.01, 0.01) # prior for location-level random effect sd_L &lt;- pow(tau_L, -1/2) b.diff &lt;- b[1] - b[2] } jags.data &lt;- list(y = moth$removed, n = moth$placed, dist = moth$distance, loc = as.numeric(moth$location), morph = as.numeric(moth$morph), J = nrow(moth)) jags.params &lt;- c(&quot;a[1]&quot;, &quot;a[2]&quot;, &quot;b[1]&quot;, &quot;b[2]&quot;, &quot;b.diff&quot;, &quot;sd_L&quot;) jags.inits &lt;- function(){ list(&quot;tau_L&quot; = runif(1)) } set.seed(1) jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = moth.model, n.chains = 3, n.iter = 5E4, n.thin = 5) ## module glm loaded For some reason this works without specifying initial values for \\(a\\) and \\(b\\) (now both vectors). Maybe the initial values are drawn from the prior? print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpMTJOiA/model2fd84f5416cb.txt&quot;, fit using jags, ## 3 chains, each with 50000 iterations (first 25000 discarded), n.thin = 5 ## n.sims = 15000 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## a[1] -1.141 0.290 -1.720 -1.320 -1.135 -0.963 -0.577 1.001 13000 ## a[2] -0.735 0.282 -1.295 -0.908 -0.733 -0.559 -0.185 1.001 15000 ## b.diff 0.028 0.008 0.012 0.022 0.028 0.033 0.044 1.001 15000 ## b[1] 0.019 0.009 0.002 0.013 0.019 0.024 0.036 1.001 6800 ## b[2] -0.009 0.009 -0.026 -0.015 -0.009 -0.004 0.008 1.001 9700 ## sd_L 0.249 0.147 0.080 0.151 0.215 0.306 0.620 1.002 2200 ## deviance 75.258 4.136 68.632 72.242 74.782 77.723 84.621 1.002 2700 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 8.5 and DIC = 83.8 ## DIC is an estimate of expected predictive error (lower deviance is better). mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) (post.mean &lt;- apply(mcmc.output, 2, mean)) ## a.1 a.2 b.diff b.1 b.2 deviance ## -1.140722211 -0.734757896 0.027796240 0.018614882 -0.009181358 75.257872581 ## sd_L ## 0.249026857 HPDinterval(as.mcmc(mcmc.output[&#39;b.diff&#39;])) ## lower upper ## b.diff 0.01168872 0.04370766 ## attr(,&quot;Probability&quot;) ## [1] 0.95 The posterior mean of the difference in the log-odds slopes — 0.0278 — is essentially the same value that we have seen in every analysis. We can have a look at the full posterior distribution for this difference, and calculate the posterior probability that the difference is \\(&gt;0\\). bayesplot::mcmc_areas(mcmc.output, pars = c(&quot;b.diff&quot;), prob = 0.95) table(mcmc.output$b.diff &gt; 0) ## ## FALSE TRUE ## 4 14996 Thus we would say that there is a 0.9997 posterior probability that the proportion of dark moths removed increases more rapidly with increasing distance from Liverpool than the proportion of light moths removed. We can plot the fit of the model using draws from the posterior distribution of the parameters. The heavy lines below show the fits using the posterior means of the parameters. Do these fits correspond to the marginal or conditional means? (There’s little difference here, but it’s a useful thought exercise.) subset.samples &lt;- sample(nrow(mcmc.output), size = 100) moth$prop.removed &lt;- with(moth, removed / placed) light &lt;- subset(moth, morph == &quot;light&quot;) dark &lt;- subset(moth, morph == &quot;dark&quot;) par(mfrow = c(1, 2)) #------ light morph plot(prop.removed ~ distance, data = moth, type = &quot;n&quot;, main = &quot;Light morph&quot;, ylab = &quot;proprotion removed&quot;) points(x = light$distance, y = light$prop.removed, pch = 16) for(i in subset.samples) { a &lt;- mcmc.output$a.2[i] b &lt;- mcmc.output$b.2[i] fitted.curve &lt;- function(x) inv.logit(a + b * x) curve(fitted.curve, from = min(moth$distance), to = max(moth$distance), add = TRUE, col = &quot;deepskyblue&quot;) } fitted.mean.curve &lt;- function(x) inv.logit(post.mean[&#39;a.2&#39;] + post.mean[&#39;b.2&#39;] * x) curve(fitted.mean.curve, from = min(moth$distance), to = max(moth$distance), add = TRUE, col = &quot;darkblue&quot;, lwd = 2) points(x = light$distance, y = light$prop.removed, pch = 16) #--------- dark morph plot(prop.removed ~ distance, data = moth, type = &quot;n&quot;, main = &quot;Dark morph&quot;, ylab = &quot;proprotion removed&quot;) for(i in subset.samples) { a &lt;- mcmc.output$a.1[i] b &lt;- mcmc.output$b.1[i] fitted.curve &lt;- function(x) inv.logit(a + b * x) curve(fitted.curve, from = min(moth$distance), to = max(moth$distance), add = TRUE, col = &quot;deepskyblue&quot;) } fitted.mean.curve &lt;- function(x) inv.logit(post.mean[&#39;a.1&#39;] + post.mean[&#39;b.1&#39;] * x) curve(fitted.mean.curve, from = min(moth$distance), to = max(moth$distance), add = TRUE, col = &quot;darkblue&quot;, lwd = 2) points(x = dark$distance, y = dark$prop.removed, pch = 16) 8.2 Example 2: Ticks on red grouse This example comes from Ben Bolker’s chapter in Fox, Negrete-Yankelevich, and Sosa (2015). Bolker describes the data as follows: “Elston et al. (2001) used data on numbers of ticks sampled from the heads of red grouse chicks in Scotland to explore patterns of aggregation. Ticks have potentially large fitness and demographic consequences on red grouse individuals and populations, but Elston et al.’s goal was just to decompose patterns of variation into different scales (within-brood, within-site, by altitude and year). The response is the tick count (TICKS, again Poisson or negative binomial); altitude (HEIGHT, treated as continuous) and year (YEAR, treated as categorical) are fixed predictor variables. Individual within brood (INDEX) and brood within location are nested random-effect grouping variables, with the baseline expected number of ticks (intercept) varying among groups.” An alternative analysis of these data can be found on Bolker’s Github page at https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html. The data include 3 years and 63 locations. Location and year are crossed (some locations are sampled in multiple years), but not every location is sampled every year. There are 118 broods, and each brood is nested within a year-location pair. Each observation corresponds to one of 403 individuals. Individuals are nested within broods. We also know each location’s elevation. To develop notation, let \\(i=1, \\ldots, 3\\) index the years, let \\(j = 1, \\ldots, 63\\) index the locations, let \\(k = 1, \\ldots, n_{ij}\\) index the broods sampled in each year-location pair, and let \\(l = 1, \\ldots, n_{ijk}\\) index the separate individuals in each brood. Note that almost all year-location pairs will be represented by at most 1 brood (\\(n_{ij}=0\\) or \\(=1\\) for almost all combination of years and locations) but there is at least one year-location pair with multiple broods (\\(n_{ij} &gt; 1\\)). Let \\(y_{ijkl}\\) (the response) be the number of ticks found on individual \\(l\\) of brood \\(k\\) at location \\(j\\) in year \\(i\\), and let \\(x_j\\) be the elevation of location \\(j\\). We are interested in characterizing how tick load varies among locations, among broods within locations, and among individuals within broods. We are also interested in characterizing the relationship between elevation and tick load. It seems unlikely that 3 years are enough to estimate year-to-year variation, so we will use fixed-effect parameters to capture the differences among years. We will begin by assuming that the tick load takes a Poisson distribution, and use the canonical log link. We include an observation-level random effect so that we have a variance parameter that captures individual-to-individual variation among individuals in the same brood. In notation, our GLMM is \\[\\begin{align*} y_{ijkl} &amp; \\sim \\mathrm{Pois}(\\mu_{ijkl})\\\\ \\log(\\mu_{ijkl}) &amp; = \\eta_{ijkl} \\\\ \\eta_{ijkl} &amp; = a_i + b x_j + L_j + B_{ijk} + \\varepsilon_{ijkl} \\\\ L_j &amp; \\sim \\mathcal{N}(0, \\sigma^2_L) \\\\ B_{ijk} &amp; \\sim \\mathcal{N}(0, \\sigma^2_B) \\\\ \\varepsilon_{ijkl} &amp; \\sim \\mathcal{N}(0, \\sigma^2_\\varepsilon) \\\\ \\end{align*}\\] require(lme4) require(lattice) ## Loading required package: lattice tick &lt;- read.table(&quot;data/tick.txt&quot;, head = T) names(tick) &lt;- c(&quot;index&quot;, &quot;ticks&quot;, &quot;brood&quot;, &quot;elevation&quot;, &quot;yr&quot;, &quot;loc&quot;) tick$index &lt;- as.factor(tick$index) tick$brood &lt;- as.factor(tick$brood) tick$yr &lt;- as.factor(tick$yr) tick$loc &lt;- as.factor(tick$loc) # center and scale elevation tick$elev.z &lt;- with(tick, (elevation - mean(elevation)) / sd(elevation)) Model fitting: fm1 &lt;- glmer(ticks ~ yr + elev.z + (1 | loc) + (1 | brood) + (1 | index), family = &quot;poisson&quot;, data = tick) summary(fm1) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: ticks ~ yr + elev.z + (1 | loc) + (1 | brood) + (1 | index) ## Data: tick ## ## AIC BIC logLik deviance df.resid ## 1794.5 1822.5 -890.3 1780.5 396 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6123 -0.5536 -0.1486 0.2850 2.4430 ## ## Random effects: ## Groups Name Variance Std.Dev. ## index (Intercept) 0.2932 0.5415 ## brood (Intercept) 0.5625 0.7500 ## loc (Intercept) 0.2796 0.5287 ## Number of obs: 403, groups: index, 403; brood, 118; loc, 63 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.3728 0.1964 1.898 0.057639 . ## yr96 1.1804 0.2381 4.957 7.15e-07 *** ## yr97 -0.9787 0.2628 -3.724 0.000196 *** ## elev.z -0.8543 0.1236 -6.910 4.83e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) yr96 yr97 ## yr96 -0.728 ## yr97 -0.610 0.514 ## elev.z 0.011 0.048 0.047 We can have a look at the profile confidence intervals and profile confidence regions for each of the model parameters. pp &lt;- profile(fm1) confint(pp) ## 2.5 % 97.5 % ## .sig01 0.45148400 0.6451853 ## .sig02 0.52127907 1.0569688 ## .sig03 0.00000000 0.8928761 ## (Intercept) -0.02822382 0.7485777 ## yr96 0.71308911 1.6583691 ## yr97 -1.50239867 -0.4606278 ## elev.z -1.10589101 -0.6090505 xyplot(pp, absVal = TRUE) splom(pp) We see that the standard deviation of the location-level random effect (“.sig03”) has a 95% confidence interval that includes 0. We might conclude that the location-level random effect is unnecessary. In other words, there is no evidence of additional variation among the locations beyond the variation attributable to differences in elevation. The bivariate confidence regions show us that the estimated SD of the location-level random effect is negatively correlated with the estimated SD of the brood-level random effect (“.sig02”). Thus, the location-to-location variability (above and beyond the elevation effect) from brood-to-brood variability are confounded, which makes sense, given that most locations are represented by a small number of broods. It is something of a judgment call as to whether it would make sense at this point to drop the location-level random effect. We could retain it on the grounds that one expects some location-to-location variation beyond the effect of elevation, even if that variation is small. Further, some (such as Bolker) would argue that model selection on the random effects is risky. In his GLMM FAQ page, Bolker writes: Consider not testing the significance of random effects. If the random effect is part of the experimental design, this procedure may be considered ‘sacrificial pseudoreplication’ (Hurlbert (1984)). Using stepwise approaches to eliminate non-significant terms in order to squeeze more significance out of the remaining terms is dangerous in any case. However, these red grouse data do not come from a designed experiment. If one is interested in testing for the significance of a random effect, the usual approach is to conduct a likelihood-ratio test to compare models with and without the random effect. In this case, the null hypothesis is that the variance of the tested random effect is 0, which is on the boundary of the allowable values for a variance. Thus, the \\(p\\)-value from a LRT is conservative (too big). In simple models, the \\(p\\)-value for the LRT is twice as big as it should be, suggesting that the appropriate correction is to divide the \\(p\\)-value by 2 (Pinheiro and Bates (2000)). For more complex models, however, the divide-by-2 rule is only a rough rule of thumb. We illustrate by comparing a model with the location-level random effect to one without it. (In notation, we are testing \\(\\sigma^2_L = 0\\).) fm2 &lt;- glmer(ticks ~ yr + elev.z + (1 | brood) + (1 | index), family = &quot;poisson&quot;, data = tick) anova(fm2, fm1) ## Data: tick ## Models: ## fm2: ticks ~ yr + elev.z + (1 | brood) + (1 | index) ## fm1: ticks ~ yr + elev.z + (1 | loc) + (1 | brood) + (1 | index) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## fm2 6 1794.0 1818.0 -891.02 1782.0 ## fm1 7 1794.5 1822.5 -890.27 1780.5 1.4973 1 0.2211 If we use the rough divide-by-2 rule, the approximate \\(p\\)-value is \\(p \\approx 0.11\\). Thus, the model with the location-level random effect does not improve significantly on the model without the location-level random effect. We might conclude that the location-level random effect is unnecessary. Although this analysis focused on how different random effects contributed to the overall variation in tick load, it is also helpful to visualize the fit of the model. The plot below shows the tick load for each individual (plotted on a log scale) vs. the centered and scaled elevation variable, along with the fitted mean line for each year. par(mfrow = c(1, 3)) plot.subset &lt;- function(year, a, b) { with(tick, plot(log(ticks + 1) ~ elev.z, type = &quot;n&quot;, main = year)) with(subset(tick, yr == year), points(jitter(log(ticks + 1)) ~ elev.z)) fit &lt;- function(x) log(1 + exp(a + b * x)) curve(fit, from = min(tick$elev.z), to = max(tick$elev.z), add = TRUE, col = &quot;red&quot;) } plot.subset(&quot;95&quot;, a = 0.3728, b = -0.8543) plot.subset(&quot;96&quot;, a = 0.3728 + 1.1804, b = -0.8543) plot.subset(&quot;97&quot;, a = 0.3728 - 0.9787, b = -0.8543) 8.3 GAMMs Generalized additive mixed models (GAMMs) include just about every model feature we’ve discussed: splines to capture smooth effects of predictors, non-Gaussian responses, and correlations. There are two software routines available for fitting GAMMs in R: mgcv::gamm and gamm4::gamm4. The routine mgcv::gamm is based on lme, and thus provides access to the non-constant variance and correlation structures that we saw when discussing generalized least squares. The routine gamm4::gamm4 is based on lme4, and thus provides access to the same fitting syntax as lmer and glmer. We will illustrate each in turn. As we have seen, serial data usually have a serial dependence structure. They are also data for which one might want to use splines to capture the underlying trend. Time series provide a prime example. Below, we will analyze daily average temperature data from RDU from Jan 1 1995 to May 13 2020. These data can be found at http://academic.udayton.edu/kissock/http/weather/. First, some housekeeping and exploratory analysis. rdu &lt;- read.table(&quot;data/rdu-temperature.txt&quot;, head = T) # remove NA&#39;s, coded as -99 with(rdu, table(temp == -99)) ## ## FALSE TRUE ## 9250 15 rdu &lt;- subset(rdu, temp &gt; -99) with(rdu, plot(temp ~ time, type = &quot;l&quot;, xlab = &quot;day&quot;)) We will fit a model that is the sum of two splines: a cyclic spine to capture the within-year trend in temperature, and a smoothing spline to capture the among-year trend in temperature. We also include an AR(1) structure on the errors. The AR(1) structure really should pertain to the entire time series, but the fitting takes too long if we do so. Instead, we just fit the AR(1) structure to the errors within each year, which is only a minimal modification of the model (the only consequence is that we have assumed the errors on Dec 31 and the following Jan 1 are independent), but it allows the model to be fit more quickly. require(mgcv) ## Loading required package: mgcv ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmList ## This is mgcv 1.8-42. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. fm1 &lt;- gamm(temp ~ s(doy, bs = &quot;cc&quot;, k = 20) + s(time), data = rdu, correlation = corAR1(form = ~ 1 | yr)) The output of mgcv::gamm is a list of two parts. The first part, named lme, includes the output of the model that includes most of the model fit except the smooth terms. The second part, named gam, includes any smoothing splines. For the model above, most of the interesting elements are in the gam portion. We’ll look at the lme portion, too, as this contains the estimate of the correlation parameter between consecutive days. summary(fm1$lme) ## Linear mixed-effects model fit by maximum likelihood ## Data: strip.offset(mf) ## AIC BIC logLik ## 58047.81 58090.6 -29017.9 ## ## Random effects: ## Formula: ~Xr - 1 | g ## Structure: pdIdnot ## Xr1 Xr2 Xr3 Xr4 Xr5 Xr6 Xr7 ## StdDev: 0.5311296 0.5311296 0.5311296 0.5311296 0.5311296 0.5311296 0.5311296 ## Xr8 Xr9 Xr10 Xr11 Xr12 Xr13 Xr14 ## StdDev: 0.5311296 0.5311296 0.5311296 0.5311296 0.5311296 0.5311296 0.5311296 ## Xr15 Xr16 Xr17 Xr18 ## StdDev: 0.5311296 0.5311296 0.5311296 0.5311296 ## ## Formula: ~Xr.0 - 1 | g.0 %in% g ## Structure: pdIdnot ## Xr.01 Xr.02 Xr.03 Xr.04 Xr.05 Xr.06 ## StdDev: 0.001644904 0.001644904 0.001644904 0.001644904 0.001644904 0.001644904 ## Xr.07 Xr.08 Residual ## StdDev: 0.001644904 0.001644904 7.596932 ## ## Correlation Structure: AR(1) ## Formula: ~1 | g/g.0/yr ## Parameter estimate(s): ## Phi ## 0.6815075 ## Fixed effects: y ~ X - 1 ## Value Std.Error DF t-value p-value ## X(Intercept) 60.55577 0.1805467 9248 335.4022 0e+00 ## Xs(time)Fx1 0.70615 0.1804134 9248 3.9140 1e-04 ## Correlation: ## X(Int) ## Xs(time)Fx1 0 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.071281352 -0.648513253 -0.002229428 0.590788103 3.645626473 ## ## Number of Observations: 9250 ## Number of Groups: ## g g.0 %in% g ## 1 1 summary(fm1$gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## temp ~ s(doy, bs = &quot;cc&quot;, k = 20) + s(time) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 60.5558 0.1805 335.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(doy) 10.13 18 310.51 &lt; 2e-16 *** ## s(time) 1.00 1 15.32 9.15e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.76 ## Scale est. = 57.713 n = 9250 plot(fm1$gam) Intriguingly, but not surprisingly, the fit to the within-year trend clearly shows that the spring warm-up in Raleigh is decidedly more gradual than the fall cool-down. Fall in the Piedmont is ever fleeting. Less substantially, but still interestingly, the estimate of the correlation between temperature anomalies on consecutive days is \\(\\approx\\) 0.68, which matches lived experience. The best-fitting smoothing spline for the among-year trend is linear. Let’s replace the smoothing spline by a linear term so that it is easier to extract the slope, which will now be contained in the lme portion. fm2 &lt;- gamm(temp ~ s(doy, bs = &quot;cc&quot;, k = 20) + time, data = rdu, correlation = corAR1(form = ~ 1 | yr)) summary(fm2$lme) ## Linear mixed-effects model fit by maximum likelihood ## Data: strip.offset(mf) ## AIC BIC logLik ## 58045.81 58081.47 -29017.9 ## ## Random effects: ## Formula: ~Xr - 1 | g ## Structure: pdIdnot ## Xr1 Xr2 Xr3 Xr4 Xr5 Xr6 Xr7 ## StdDev: 0.5311421 0.5311421 0.5311421 0.5311421 0.5311421 0.5311421 0.5311421 ## Xr8 Xr9 Xr10 Xr11 Xr12 Xr13 Xr14 ## StdDev: 0.5311421 0.5311421 0.5311421 0.5311421 0.5311421 0.5311421 0.5311421 ## Xr15 Xr16 Xr17 Xr18 Residual ## StdDev: 0.5311421 0.5311421 0.5311421 0.5311421 7.596946 ## ## Correlation Structure: AR(1) ## Formula: ~1 | g/yr ## Parameter estimate(s): ## Phi ## 0.6815089 ## Fixed effects: y ~ X - 1 ## Value Std.Error DF t-value p-value ## X(Intercept) 59.33167 0.3611717 9248 164.27550 0e+00 ## Xtime 0.00026 0.0000675 9248 3.91405 1e-04 ## Correlation: ## X(Int) ## Xtime -0.866 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.071273712 -0.648512192 -0.002229388 0.590786427 3.645621019 ## ## Number of Observations: 9250 ## Number of Groups: 1 The temperature trend is estimated as an increase of 2.64^{-4} \\(^\\circ\\)F per day. That equates to a trend of 0.0964 \\(^\\circ\\)F per year, or 0.964 per decade. Yikes! To see the effect of the AR(1) correlation structure, let’s compare our model fit to one that doesn’t account for autocorrelated errors. fm1a &lt;- gam(temp ~ s(doy, bs = &quot;cc&quot;, k = 20) + s(time), data = rdu) plot(fm1a) abline(h = 0, col = &quot;red&quot;) Without the autocorrelated errors, both smoothing splines are quite a bit wigglier. The confidence intervals around the fit are also too small. Both indicate overfitting. Accounting for the serial correlations in the errors has provided a substantially improved description of the trends in the data. Finally, we will use gamm4::gamm4 to fit a new model to the tick data from Elston et al. (2001), this time using a smoothing spline to estimate the effect of elevation on tick abundance. Like mgcv::gamm, gamm4::gamm4 returns models with two compoments: one called mer that contains output from the portion of the model that invokes lme4::(g)lmer, and one called gam that contains the smoothing terms. require(gamm4) fm4 &lt;- gamm4(ticks ~ yr + s(elev.z), random = ~ (1 | loc) + (1 | brood) + (1 | index), family = &quot;poisson&quot;, data = tick) summary(fm4$mer) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## ## AIC BIC logLik deviance df.resid ## 1796.5 1828.5 -890.3 1780.5 395 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6123 -0.5536 -0.1486 0.2849 2.4430 ## ## Random effects: ## Groups Name Variance Std.Dev. ## index (Intercept) 2.932e-01 0.5415175 ## brood (Intercept) 5.625e-01 0.7499952 ## loc (Intercept) 2.796e-01 0.5287786 ## Xr s(elev.z) 1.359e-08 0.0001166 ## Number of obs: 403, groups: index, 403; brood, 118; loc, 63; Xr, 8 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## X(Intercept) 0.3727 0.1964 1.898 0.057694 . ## Xyr96 1.1805 0.2381 4.958 7.14e-07 *** ## Xyr97 -0.9787 0.2628 -3.724 0.000196 *** ## Xs(elev.z)Fx1 -0.8533 0.1235 -6.910 4.84e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## X(Int) Xyr96 Xyr97 ## Xyr96 -0.728 ## Xyr97 -0.610 0.514 ## Xs(lv.z)Fx1 0.011 0.048 0.047 summary(fm4$gam) ## ## Family: poisson ## Link function: log ## ## Formula: ## ticks ~ yr + s(elev.z) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.3727 0.1904 1.958 0.050278 . ## yr96 1.1805 0.2356 5.010 5.44e-07 *** ## yr97 -0.9787 0.2630 -3.722 0.000198 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(elev.z) 1 1 48.03 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.156 ## glmer.ML = 220.92 Scale est. = 1 n = 403 plot(fm4$gam) Our best fitting model continues to contain a linear association between elevation and tick abundance. Again, it is interesting to compare this fit to one without the random effects for brood or location, and to see how the absence of these random effects produces a substantially different (and presumably much over-fit) relationship between elevation and tick abundance. fm5 &lt;- gam(ticks ~ yr + s(elev.z), family = &quot;poisson&quot;, data = tick) plot(fm5) Bibliography "],["bibliography.html", "Bibliography", " Bibliography "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
