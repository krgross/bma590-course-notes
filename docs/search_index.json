[["maximum-likelihood-estimation.html", "Computing companion for BMA / ST 590, Fall 2021 Chapter 1 Maximum likelihood estimation 1.1 A very simple example with a single observation 1.2 Horse-kick data 1.3 Myxomatosis data 1.4 Tadpole data", " Computing companion for BMA / ST 590, Fall 2021 Kevin Gross 2021-09-22 Chapter 1 Maximum likelihood estimation 1.1 A very simple example with a single observation Suppose we observe a single observation from a Poisson distribution. Suppose that observation is \\(X=2\\). We can use the dpois function to evaluate the likelihood for this single observation. For example, we can evaluate the likelihood at \\(\\lambda = 1.5\\): dpois(x = 2, lambda = 1.5) ## [1] 0.2510214 Or we could evaluate the likelihood at \\(\\lambda = 2\\) or \\(\\lambda = 2.5\\): dpois(x = 2, lambda = c(2, 2.5)) ## [1] 0.2706706 0.2565156 Now lets evaluate the likelihood at a sequence of \\(\\lambda\\) values: lambda.vals &lt;- seq(from = 0, to = 5, by = 0.01) my.lhood &lt;- dpois(x = 2, lambda = lambda.vals) plot(lambda.vals, my.lhood, xlab = expression(lambda), ylab = &quot;Likelihood&quot;, type = &quot;l&quot;) We might guess that the likelihood is maximized at \\(\\lambda = 2\\). Wed be right. plot(lambda.vals, my.lhood, xlab = expression(lambda), ylab = &quot;Likelihood&quot;, type = &quot;l&quot;) abline(v = 2, col = &quot;red&quot;) 1.2 Horse-kick data Most real data sets contain more than a single observation. Here is a data set that we can use to illustrate maximum likelihood estimation with a single parameter. Famously, Ladislaus van Bortkewitsch (1868 - 1931) published how many members of the Prussiam army were killed by horse kicks in each of 20 years, for each of 14 army corps. As a caveat, these data are often used to illustrate the Poisson distribution, as we will use them. They match the Poisson distribution more neatly than we might expect for most data sets. First import the data. Note that the path name used here is specific to the file directory that was used to create this file. The path name that you use will likely differ. horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) Ask for a summary of the data to make sure the data have been imported correctly. summary(horse) ## year corps deaths ## Min. :1875 Length:280 Min. :0.0 ## 1st Qu.:1880 Class :character 1st Qu.:0.0 ## Median :1884 Mode :character Median :0.0 ## Mean :1884 Mean :0.7 ## 3rd Qu.:1889 3rd Qu.:1.0 ## Max. :1894 Max. :4.0 We can also learn about the data by asking to see the first few records using the head command head(horse) ## year corps deaths ## 1 1875 GC 0 ## 2 1876 GC 2 ## 3 1877 GC 2 ## 4 1878 GC 1 ## 5 1879 GC 0 ## 6 1880 GC 0 or we can see the last few records using the tail command: tail(horse) ## year corps deaths ## 275 1889 C15 2 ## 276 1890 C15 2 ## 277 1891 C15 0 ## 278 1892 C15 0 ## 279 1893 C15 0 ## 280 1894 C15 0 Another useful function to keep in mind is the `str function which tells you about the [str]ucture of an R object: str(horse) ## &#39;data.frame&#39;: 280 obs. of 3 variables: ## $ year : int 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 ... ## $ corps : chr &quot;GC&quot; &quot;GC&quot; &quot;GC&quot; &quot;GC&quot; ... ## $ deaths: int 0 2 2 1 0 0 1 1 0 3 ... Lets plot a histogram of the values: hist(horse$deaths, breaks = seq(from = min(horse$deaths) - 0.5, to = max(horse$deaths) + 0.5, by = 1)) 1.2.1 Calculate and plot the log-likelihood function Create a function that calculates the log-likelihood for a value of \\(\\lambda\\): horse.ll &lt;- function(my.lambda){ ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) sum(ll.vals) } We can use this function to calculate the log-likelihood for any value of \\(\\lambda\\), such as \\(\\lambda = 1\\): horse.ll(1) ## [1] -328.2462 Lets calculate the log-likelihood for many values of \\(\\lambda\\), in preparation for making a plot. Well use a loop here, and not worry about vectorization. # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.01, to = 2.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.ll(lambda.vals[i.lambda]) } Now plot the log-likelihood values vs. the values of \\(\\lambda\\): plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;log likelihood&quot;, type = &quot;l&quot;) abline(v = 0.7, col = &quot;red&quot;) 1.2.2 Find the MLE numerically using optimize Bolkers book illustrates numerical optimization using the optim function. The R documentation recommends using optimize for one-dimensional optimization, and optim for optimizing a function in several dimensions. So, we will use optimize here. We will enclose the entire call to optimize in parentheses so that the output is dumped to the command line in addition to being stored as horse.mle. (horse.mle &lt;- optimize(f = horse.ll, interval = c(0.1, 2), maximum = TRUE)) ## $maximum ## [1] 0.7000088 ## ## $objective ## [1] -314.1545 The optimize function returns a list. A list is an R object that contains components of different types. The numerically calculated MLE is \\(\\hat{\\lambda} \\approx 0.7\\). The objective component of gives the value of the log-likelihood at that point. 1.3 Myxomatosis data The myxomatosis data are in Bolkers library emdbook. First load the library. If the library is not found, you will first have to download and install the library on your computer, using the Packages tab in RStudio. The call to data loads the particular myxomatosis data set that we want into memory. library(emdbook) data(MyxoTiter_sum) Inspect the data to make sure they have been imported correctly. summary(MyxoTiter_sum) ## grade day titer ## Min. :1.000 Min. : 2.000 Min. :1.958 ## 1st Qu.:3.000 1st Qu.: 4.000 1st Qu.:5.400 ## Median :4.000 Median : 8.000 Median :6.612 ## Mean :3.604 Mean : 9.564 Mean :6.331 ## 3rd Qu.:5.000 3rd Qu.:13.000 3rd Qu.:7.489 ## Max. :5.000 Max. :28.000 Max. :9.021 head(MyxoTiter_sum) ## grade day titer ## 1 1 2 5.207 ## 2 1 2 5.734 ## 3 1 2 6.613 ## 4 1 3 5.997 ## 5 1 3 6.612 ## 6 1 3 6.810 Extract the subset of the data that corresponds to the grade 1 viral strain. myxo &lt;- subset(MyxoTiter_sum, grade == 1) summary(myxo) ## grade day titer ## Min. :1 Min. :2.000 Min. :4.196 ## 1st Qu.:1 1st Qu.:3.500 1st Qu.:6.556 ## Median :1 Median :5.000 Median :7.112 ## Mean :1 Mean :5.037 Mean :6.924 ## 3rd Qu.:1 3rd Qu.:6.000 3rd Qu.:7.543 ## Max. :1 Max. :9.000 Max. :8.499 Out of curiosity, lets make a scatterplot of the titer vs. the day with(myxo, plot(titer ~ day)) For the sake of this example, we will ignore the apparent (and unsurprising) relationship between titer and day, and instead will consider only the titer data. We will regard these data as a random sample from a normal distribution. For the sake of illustration, we will estimate the mean and variance of the normal distribution using the optim function in R. First, we write a function to calculate the log likelihood. myxo.ll &lt;- function(m, v){ ll.vals &lt;- dnorm(myxo$titer, mean = m, sd = sqrt(v), log = TRUE) sum(ll.vals) } Note that Rs function for the pdf of a normal distribution  dnorm  is parameterized by the mean and standard deviation (SD) of the normal distribution. Although it would be just as easy to find the MLE of the standard deviation \\(\\sigma\\), for the sake of illustration, we will seek the MLE of the variance, \\(\\sigma^2\\). (It turns out that, if we write the MLE of the standard deviation as \\(\\hat{\\sigma}\\) and the MLE of the variance as \\(\\hat{\\sigma}^2\\), then \\(\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}\\). This is an example of the {} of MLEs.) We can use our function to calculate the likelihood for any choice of mean and variance. For example, lets try \\(\\mu = 6\\) and \\(\\sigma^2 = 1\\). myxo.ll(m = 6, v = 1) ## [1] -47.91229 We want to maximize the likelihood using optim. Unfortuantely, optim is a little finicky. To use optim, we have to re-write our function myxo.ll so that the parameters to be estimated are passed to the function as a single vector. Also, by default, optim performs minimization instead of maximization. We can change this behavior when we call optim. Alternatively, we can just re-define the function to return the negative log likelihood. myxo.neg.ll &lt;- function(pars){ m &lt;- pars[1] v &lt;- pars[2] ll.vals &lt;- dnorm(myxo$titer, mean = m, sd = sqrt(v), log = TRUE) -sum(ll.vals) } Now we can use optim: (myxo.mle &lt;- optim(par = c(7, 1), # starting values, just a ballpark guess fn = myxo.neg.ll)) ## $par ## [1] 6.9241029 0.8571471 ## ## $value ## [1] 36.23228 ## ## $counts ## function gradient ## 55 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Note that the MLE of the variance is \\[ \\hat{\\sigma}^2 = \\frac{\\sum_i (x_i - \\bar{x})}{n}. \\] Lets verify this by calculating the same quantity at the command line: residuals &lt;- with(myxo, titer - mean(titer)) ss &lt;- sum(residuals^2) n &lt;- length(myxo$titer) ss / n ## [1] 0.8572684 Compare this to the answer given by var, and to the more usual calculation of the variance as \\[ s^2 = \\frac{\\sum_i (x_i - \\bar{x})}{n-1}. \\] (var.usual &lt;- ss / (n - 1)) ## [1] 0.8902403 var(myxo$titer) ## [1] 0.8902403 One main take-home of this example is that when we use maximum likelihood to estimate variances for normally distributed data, the MLE is biased low. In other words, it underestimates the true variance. When we study hierarchical models later in the semester, we will regularly find ourselves estimating variances for normally distributed effects, and will have to deal with the consequences of the fact that the MLEs of these variances are biased low. For models with 2 parameters, we can visualize the likelihood surface with a contour plot. To do so, the first step is to define a lattice of values at which we want to calculate the log-likelihood. Well do so by defining vectors for \\(\\mu\\) and \\(\\sigma^2\\): m.vals &lt;- seq(from = 6, to = 8, by = 0.05) v.vals &lt;- seq(from = 0.3, to = 2.5, by = 0.05) Here is some fancy R code that shows this lattice. Dont worry about how this plot is created, as it isnt critical for what follows. plot(rep(m.vals, length(v.vals)), rep(v.vals, rep(length(m.vals), length(v.vals))), xlab = expression(mu), ylab = expression(sigma^2)) Now we will define the matrix that will store the values of the log-likelihood for each combination of \\(\\mu\\) and \\(\\sigma^2\\) in the lattice shown above. ll.vals &lt;- matrix(nrow = length(m.vals), ncol = length(v.vals)) Next, we will write a nested loop that cycles through the lattice points, calculates the log-likelihood for each, and stores the value of the log likelihood in the matrix ll.vals that we just created. for (i.m in 1:length(m.vals)) { for(i.v in 1:length(v.vals)) { ll.vals[i.m, i.v] &lt;- myxo.ll(m = m.vals[i.m], v = v.vals[i.v]) } } Now we will use the contour function to build the contour plot, and then add a red dot for the MLE. contour(x = m.vals, y = v.vals, z = ll.vals, nlevels = 100, xlab = expression(mu), ylab = expression(sigma^2)) # show the MLE points(x = myxo.mle$par[1], y = myxo.mle$par[2], col = &quot;red&quot;) 1.4 Tadpole data Finally, well take a look at the data from the functional response experiment of Vonesh &amp; Bolker (2005), described in section 6.3.1.1 of Bolkers book. This is another example of using likelihood to estimate parameters in a two-parameter model. This example differs from the previous two examples because we wont assume that the data constitute a simple random sample from some known distribution like the Gaussian or Poisson distribution. Instead, well build a somewhat more customized model for these data that incorporates some ecological ideas. This process of building a customized model is more typical of how one would analyze a real data set. Well start by using the rm command to clean up the workspace. rm(list = ls()) First, well read in the data and explore them in various ways. library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) summary(frog) ## Initial Killed ## Min. : 5.00 Min. : 1.00 ## 1st Qu.: 13.75 1st Qu.: 5.75 ## Median : 25.00 Median :10.00 ## Mean : 38.12 Mean :13.25 ## 3rd Qu.: 56.25 3rd Qu.:18.75 ## Max. :100.00 Max. :35.00 head(frog) ## Initial Killed ## 1 5 1 ## 2 5 2 ## 3 10 5 ## 4 10 6 ## 5 15 10 ## 6 15 9 plot(Killed ~ Initial, data = frog) Following Bolker, well assume that the number of individuals killed takes a binomial distribution, where the number of trials equals the initial tadpole density, and the probability that a tadpole is killed is given by the expression \\[ p_i = \\dfrac{a}{1 + a h N_i} \\] The two parameters to estimate are \\(a\\), which we interpret as the attack rate when the prey density is low, and \\(h\\), which is the handling time. Well first construct the negative log-likelihood function. # negative log-likelihood, for use with optim frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } Now well find the MLE using optim (frog.mle &lt;- optim(par = c(0.5, 1/40), fn = frog.neg.ll)) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced ## $par ## [1] 0.52592567 0.01660454 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 59 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL # note the warnings a.mle &lt;- frog.mle$par[1] h.mle &lt;- frog.mle$par[2] Well plot the data and overlay a fitted line. # add a line to our plot to show the fitted curve plot(Killed ~ Initial, data = frog) init.values &lt;- with(frog, seq(from = min(Initial), to = max(Initial), length = 100)) pred.values &lt;- a.mle * init.values / (1 + a.mle * h.mle * init.values) lines(x = init.values, y = pred.values, col = &quot;red&quot;) Finally, well plot the likelihood contours. # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) Note that, in contrast to the Myxomatosis data, here the likelihood contours form regions whose major axes are not parallel to the parameter axes. Well reflect on the implications of this shape in the next section. "],["beyond-the-mle-confidence-regions-and-hypothesis-tests-using-the-likelihood-function.html", "Chapter 2 Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function 2.1 Confidence intervals for single parameters 2.2 Confidence regions, profile likelihoods, and associated univariate intervals 2.3 Locally quadratic approximations to confidence intervals and regions 2.4 Comparing models: Likelihood ratio test and AIC 2.5 Transformable constraints 2.6 The negative binomial distriution, revisited", " Chapter 2 Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function Likelihood can be used for more than simply isolating the MLE. The likelihood can also be used to generate confidence intervals for single parameters, or confidence regions for several parameters. Well start by using the horse-kick data to see how to generate a confidence interval for a single parameter, and then move on to considering models with more than one parameter. 2.1 Confidence intervals for single parameters First well read in the data and recreate the negative log likelihood function. ################# ## Preparation ################ # read in the data horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) horse.neg.ll &lt;- function(my.lambda) { ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) -1 * sum(ll.vals) } # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.5, to = 1.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.neg.ll(lambda.vals[i.lambda]) } plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) Now well find the a (asymptotic) 95% confidence interval for \\(\\lambda\\) directly. cutoff.ll &lt;- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 # recreate the plot and add a line plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) # use uniroot to find the confidence bounds precisely my.function &lt;- function(my.lambda){ horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda) } (lower &lt;- uniroot(f = my.function, interval = c(0.6, 0.7))) ## $root ## [1] 0.6065198 ## ## $f.root ## [1] -3.556854e-05 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 (upper &lt;- uniroot(f = my.function, interval = c(0.7, 0.9))) ## $root ## [1] 0.8026265 ## ## $f.root ## [1] -0.0001007316 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 As an alternative programming style, we could have defined the objective function on the fly, and not bothered to create my.function. (lower &lt;- uniroot(f = function(x) horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(x) , interval = c(0.6, 0.7))) ## $root ## [1] 0.6065198 ## ## $f.root ## [1] -3.556854e-05 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 Lets recreate the plot and add vertical lines to indicate the confidence interval. plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = c(lower$root, upper$root), col = &quot;red&quot;) # clean up the workspace rm(list = ls()) Thus, the 95% CI for \\(\\lambda\\) is \\((0.607, 0.803)\\). 2.2 Confidence regions, profile likelihoods, and associated univariate intervals With a 2-parameter model, we can plot a confidence region directly. First some housekeeping to get started: library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } (frog.mle &lt;- optim(par = c(0.5, 1/60), fn = frog.neg.ll)) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced ## $par ## [1] 0.52585566 0.01660104 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 61 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle &lt;- frog.mle$par[1] h.mle &lt;- frog.mle$par[2] # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) Equipped with the contour plot, graphing the appropriate confidence region is straightforward. cut.off &lt;- frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2) # recreate the plot and add a line for the 95% confidence region contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) contour(x = a.vals, y = h.vals, z = ll.vals, levels = cut.off, add = TRUE, col = &quot;red&quot;, lwd = 2) However, there are several drawbacks to confidence regions. First, while a two-dimensional confidence region can be readily visualized, it is hard to summarize or describe. Second, and more importantly, most models have more than two parameters. In these models, a confidence region would have more than 2 dimensions, and thus would be impractical to visualize. Thus it is helpful, or even essential, to be able to generate univariate confidence intervals for single parameters from high-dimensional likelihoods. One approach to doing so is to calculate the so-called profile likelihood for a given parameter, and then to derive the univariate interval from this profile likelihood. We will illustrate this approach by computing a profile-based confidence interval for the attack rate \\(a\\) in the tadpole data. # profile log-likelihood function for the attack rate a profile.ll &lt;- function(my.a) { # Calculate the minimum log likelihood value for a given value of a, the attack rate my.ll &lt;- function(h) frog.neg.ll(c(my.a, h)) my.profile &lt;- optimize(f = my.ll, interval = c(0, 0.03), maximum = FALSE) my.profile$objective } # plot the profile likelihood vs. a # not necessary for finding the CI, but useful for understanding a.values &lt;- seq(from = 0.3, to = 0.8, by = 0.01) a.profile &lt;- double(length = length(a.values)) for (i in 1:length(a.values)) { a.profile[i] &lt;- profile.ll(a.values[i]) } plot(x = a.values, y = a.profile, xlab = &quot;a&quot;, ylab = &quot;negative log-likelihood&quot;, type = &quot;l&quot;) Now well follow the same steps as before to compute the profile-based 95% CI. # Now follow the same steps as before to find the profile 95% CI cut.off &lt;- profile.ll(a.mle) + qchisq(0.95, df = 1) / 2 (lower &lt;- uniroot(f = function(x) cut.off - profile.ll(x) , interval = c(0.3, a.mle))) ## $root ## [1] 0.4024268 ## ## $f.root ## [1] -0.0001303772 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 (upper &lt;- uniroot(f = function(x) cut.off - profile.ll(x) , interval = c(a.mle, 0.8))) ## $root ## [1] 0.6824678 ## ## $f.root ## [1] -9.763258e-06 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 plot(x = a.values, y = a.profile, xlab = &quot;a&quot;, ylab = &quot;negative log-likelihood&quot;, type = &quot;l&quot;) abline(v = c(lower$root, upper$root), col = &quot;blue&quot;) abline(h = cut.off, col = &quot;blue&quot;, lty = &quot;dashed&quot;) So, the 95% profile CI for \\(a\\) is (0.402, 0.682). 2.3 Locally quadratic approximations to confidence intervals and regions Likelihood profiling provides a straightforward way to summarize a high-dimensional confidence region by univariate confidence intervals. However, these profile intervals can still involve quite a bit of computation. Further, they are not able to capture possible correlations among parameter estimates, which are revealed in (two-dimensional) confidence regions. (Recall the shape of the joint confidence region for the parameters \\(a\\) and \\(h\\) in the tadpole data.) Locally quadratic approximations provide a way to approximate the (already approximate) univariate confidence intervals and bivariate confidence regions using only information about the curvature of the likelihood surface at the MLE. Well first start by revisiting the horse-kick data again. Of course, with the more precise \\(\\chi^2\\) based confidence interval in hand, there is no reason to seek an approximation. But doing so allows us to illustrate the calculations involved, and to see how well the approximation fares in this case. First some housekeepign to read the data into memory, etc. # clean up rm(list = ls()) # read in the data horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) horse.neg.ll &lt;- function(my.lambda) { ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) -1 * sum(ll.vals) } # use uniroot to find the confidence bounds precisely my.function &lt;- function(my.lambda){ horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda) } lower &lt;- uniroot(f = my.function, interval = c(0.6, 0.7)) upper &lt;- uniroot(f = my.function, interval = c(0.7, 0.9)) Now we will proceed to use a locally quadratic approximation to the negative log likelihood. ## this function finds the second derivative at the MLE by finite differences second.deriv &lt;- function(delta.l) { (horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2 } (horse.D2 &lt;- second.deriv(1e-04)) ## [1] 400 # see how the answer changes if we change delta second.deriv(1e-05) ## [1] 400.0003 Lets compare this answer to the answer obtained by numDeriv::hessian. numDeriv::hessian(func = horse.neg.ll, x = 0.7) ## [,1] ## [1,] 400 The approximate standard error of \\(\\hat{\\lambda}\\) is the square root of the inverse of the second derivative of the likelihood function. (lambda.se &lt;- sqrt(1 / horse.D2)) ## [1] 0.05 Now we can approximate the 95% confidence interval by using critical values from a standard normal distribution. (lower.approx &lt;- 0.7 - qnorm(.975) * lambda.se) ## [1] 0.6020018 (upper.approx &lt;- 0.7 + qnorm(.975) * lambda.se) ## [1] 0.7979982 Compare the approximation to the exact values lower$root ## [1] 0.6065198 upper$root ## [1] 0.8026265 Make a plot # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.5, to = 1.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.neg.ll(lambda.vals[i.lambda]) } plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) ################################### ## Now find the confidence interval, and plot it #################################### cutoff.ll &lt;- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 # add a line to the plot abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = c(lower$root, upper$root), col = &quot;red&quot;) abline(v = c(lower.approx, upper.approx), col = &quot;blue&quot;) legend(x = 0.65, y = 326, leg = c(&quot;exact&quot;, &quot;approximate&quot;), pch = 16, col = c(&quot;red&quot;, &quot;blue&quot;), bty = &quot;n&quot;) # clean up rm(list = ls()) Notice that the full \\(\\chi^2\\)-based confidence intervals capture the asymmetry in the information about \\(\\lambda\\). The intervals based on the quadratic approximation are symmetric. Now, use the quadratic approximation to find standard errors for \\(\\hat{a}\\) and \\(\\hat{h}\\) in the tadpole predation data. The first part is preparatory work from old classes. library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } frog.mle &lt;- optim(par = c(0.5, 1/60), fn = frog.neg.ll) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced (a.mle &lt;- frog.mle$par[1]) ## [1] 0.5258557 (h.mle &lt;- frog.mle$par[2]) ## [1] 0.01660104 Now find the hessian: (D2 &lt;- numDeriv::hessian(func = frog.neg.ll, x = c(a.mle, h.mle))) ## [,1] [,2] ## [1,] 616.5606 -7394.263 ## [2,] -7394.2628 130640.685 The matrix inverse of the hessian is the variance-covariance matrix of the parameters. Note that R uses the function solve to find the inverse of a matrix. # invert to get var-cov matrix (var.matrix &lt;- solve(D2)) ## [,1] [,2] ## [1,] 0.0050493492 2.857932e-04 ## [2,] 0.0002857932 2.383048e-05 We can use the handy cov2cor function to convert the variance matrix into a correlation matrix: cov2cor(var.matrix) ## [,1] [,2] ## [1,] 1.0000000 0.8238872 ## [2,] 0.8238872 1.0000000 Note the large correlation between \\(\\hat{a}\\) and \\(\\hat{h}\\). Compare with Figure 6.13 of Bolker. The standard errors of \\(\\hat{a}\\) and \\(\\hat{h}\\) are the square roots of the diagaonal elements of the variance-covariance matrix. (a.se &lt;- sqrt(var.matrix[1, 1])) ## [1] 0.07105877 (h.se &lt;- sqrt(var.matrix[2, 2])) ## [1] 0.004881647 Note the large correlation between \\(\\hat{a}\\) and \\(\\hat{h}\\). Lets use the (approximate) standard error of \\(\\hat{a}\\) to calculate an (approximate) 95% confidence interval: (ci.approx &lt;- a.mle + qnorm(c(0.025, .975)) * a.se) ## [1] 0.3865830 0.6651283 Recall that the 95% confidence interval we calculated by the profile likelihood was \\((0.402, 0.682)\\). So the quadratic approximation has gotten the width of the interval more or less correct, but it has fared less at capturing the asymmetry of the interval. 2.4 Comparing models: Likelihood ratio test and AIC Obtaining a parsimonious statistical description of data often requires arbitrating between competing model fits. Likelihood provides two tools for comparing models: likelihood ratio tests (LRTs) and information criteria. Of the latter, the best known information criterion is due to Akaike, and takes the name AIC. (Akaike didnt name AIC after himself; he used AIC to refer to An information criterion. In his honor, the acronym is now largely taken to stand for Akaikes information criterion.) LRTs and information criteria have complementary strengths and weaknesses. LRTs are direct, head-to-head comparisons of nested models. By nested, we mean that one model can be obtained as a special case of the other. The reduced, or less flexible (and thus more parsimonious) model plays the role of the null hypothesis, and the full, or more flexible (and thus less parsimonious) model plays the role of the alternative hypothesis. The LRT then formally evaluates whether the improvement in fit offered by the full model is statistically significant, that is, greater than what we would expect merely by chance. On the other hand, information criteria provide a penalized goodness-of-fit measure that can be used to compare many models at once. Information criteria produce a ranking of model fits, and thus a best-fitting model. The downside to information criteria is that there are no hard and fast guidelines to determine when one model provides a significantly better fit than another. The properties of information criteria are also less well understood than the properties of LRTs. To illustrate both, we will use the study of cone production by fir trees studied in \\(\\S\\) 6.6 of Bolker. These data are originally from work by Dodd and Silvertown. The data are much richer than we will examine here. Like Bolker, we will focus on whether the relationship between tree size (as measured by diameter at breast height, or dbh) and the number of cones produces differs between populations that have experienced wave-like die-offs and those that have not. First some preparatory work to import and assemble the data: require(emdbook) data(&quot;FirDBHFec&quot;) # give the data a simpler name fir &lt;- FirDBHFec rm(FirDBHFec) fir &lt;- fir[, c(&quot;WAVE_NON&quot;, &quot;DBH&quot;, &quot;TOTCONES&quot;)] # select just the variables we want summary(fir) ## WAVE_NON DBH TOTCONES ## n:166 Min. : 3.200 Min. : 0.0 ## w:205 1st Qu.: 6.400 1st Qu.: 14.0 ## Median : 7.600 Median : 36.0 ## Mean : 8.169 Mean : 49.9 ## 3rd Qu.: 9.700 3rd Qu.: 66.0 ## Max. :17.400 Max. :297.0 ## NA&#39;s :26 NA&#39;s :114 names(fir) &lt;- c(&quot;wave&quot;, &quot;dbh&quot;, &quot;cones&quot;) # rename the variables # get rid of the incomplete records fir &lt;- na.omit(fir) par(mfrow = c(1, 2)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;w&quot;)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;non-wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;n&quot;)) # any non-integral responses? with(fir, table(cones == round(cones))) # illustrate the use of &#39;with&#39; ## ## FALSE TRUE ## 6 236 # round the non-integral values fir$cones &lt;- round(fir$cones) # check with(fir, table(cones == round(cones))) ## ## TRUE ## 242 Like Bolker, we will assume that the average number of cones produced (\\(\\mu\\)) has a power-law relationship with tree dbh (\\(x\\)). We will also assume that the actual number of cones produced (\\(Y\\)) takes a negative binomial distribution with size-dependent mean and overdispersion parameter \\(k\\). That is, our model is \\[\\begin{align*} \\mu(x) &amp; = a x ^ b \\\\ Y &amp; \\sim \\mbox{NB}(\\mu(x), k) \\end{align*}\\] To head in a slightly different direction from Bolker, we will compare two models. In the first, or reduced, model the same parameters will prevail for both wave and non-wave populations. Thus this model has three parameters: \\(a\\), \\(b\\), and \\(k\\). In the second, or full, model, we will allow the \\(a\\) and \\(b\\) parameters to differ between the wave and non-wave populations. (We will continue to assume a common \\(k\\) for both population types.) Using subscripts on \\(a\\) and \\(b\\) to distinguish population types, the full model then has 5 parameters: \\(a_w\\), \\(a_n\\), \\(b_w\\), \\(b_n\\), and \\(k\\). Well fit the reduced model first. To do so, well use the dnbinom function in R, in which the \\(k\\) parameter is located in the formal argument size. fir.neg.ll &lt;- function(parms, x, y){ a &lt;- parms[1] b &lt;- parms[2] k &lt;- parms[3] my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k, mu = my.mu, log = TRUE) neg.ll &lt;- -1 * sum(ll.values) return(neg.ll) } Note a subtle difference here. In preparation for fitting this same model to different subsets of the data, the function fir.neg.ll has formal arguments that receive the values of the \\(x\\) and \\(y\\) variables. In the call to optim, we can supply those additional values as subsequent arguments in the optim function, as illustrated below. # fit reduced model (fir.reduced &lt;- optim(f = fir.neg.ll, par = c(a = 1, b = 1, k = 1), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## 0.3041425 2.3190142 1.5033525 ## ## $value ## [1] 1136.015 ## ## $counts ## function gradient ## 134 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle &lt;- fir.reduced$par[1] b.mle &lt;- fir.reduced$par[2] k.mle &lt;- fir.reduced$par[3] Make a plot of the reduced model fit, with both populations pooled together: dbh.vals &lt;- seq(from = min(fir$dbh), to = max(fir$dbh), length = 100) fit.vals &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals[i] &lt;- a.mle * dbh.vals[i] ^ b.mle } par(mfrow = c(1, 1)) # don&#39;t break the next figure into two panels with(fir, plot(cones ~ dbh)) # plot the data points lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) Now fit the full model with separate values of \\(a\\) and \\(b\\) for each population: fir.neg.ll.full &lt;- function(parms) { a.w &lt;- parms[1] b.w &lt;- parms[2] a.n &lt;- parms[3] b.n &lt;- parms[4] k &lt;- parms[5] wave &lt;- subset(fir, wave == &quot;w&quot;) nonwave &lt;- subset(fir, wave == &quot;n&quot;) # note how we call fir.neg.ll here, but each time only # passing a subset of the data neg.ll.wave &lt;- fir.neg.ll(parms = c(a = a.w, b = b.w, k = k), x = wave$dbh, y = wave$cones) neg.ll.nonwave &lt;- fir.neg.ll(parms = c(a = a.n, b = b.n, k = k), x = nonwave$dbh, y = nonwave$cones) total.ll &lt;- neg.ll.wave + neg.ll.nonwave return(total.ll) } (fir.full &lt;- optim(f = fir.neg.ll.full, par = c(a.w = 1, b.w = 1, a.n = 1, b.n = 1, k = 1))) ## $par ## a.w b.w a.n b.n k ## 0.4136414 2.1417941 0.2874122 2.3550753 1.5083974 ## ## $value ## [1] 1135.677 ## ## $counts ## function gradient ## 502 NA ## ## $convergence ## [1] 1 ## ## $message ## NULL Lets make a plot to show the different fits. a.w.mle &lt;- fir.full$par[1] b.w.mle &lt;- fir.full$par[2] a.n.mle &lt;- fir.full$par[3] b.n.mle &lt;- fir.full$par[4] par(mfrow = c(1, 2)) # wave populations fit.vals.wave &lt;- fit.vals.non &lt;- double(length = length(dbh.vals)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;w&quot;)) for (i in seq(along = dbh.vals)) { fit.vals.wave[i] &lt;- a.w.mle * dbh.vals[i] ^ b.w.mle } lines(fit.vals.wave ~ dbh.vals, col = &quot;blue&quot;) # non-wave populations plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;non-wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;n&quot;)) for (i in seq(along = dbh.vals)) { fit.vals.non[i] &lt;- a.n.mle * dbh.vals[i] ^ b.n.mle } lines(fit.vals.non ~ dbh.vals, col = &quot;red&quot;) Note that to compute the negative log likelihood for the full model, we compute the negative log likelihood for each population separately, and then sum the two negative log likelihoods. We can see the justification for doing so by writing out the log likelihood function explicitly: \\[\\begin{eqnarray*} \\ln L(a_w, a_n, b_w, b_n, k; \\mathbf{y}) &amp; = &amp; \\ln \\prod_{i \\in \\left\\{w, n \\right\\}} \\prod_{j=1}^{n_i} f(y_{ij}; a_w, a_n, b_w, b_n, k) \\\\ &amp; = &amp; \\sum_{i \\in \\left\\{w, n \\right\\}} \\sum_{j=1}^{n_i} \\ln f(y_{ij}; a_w, a_n, b_w, b_n, k) \\\\ &amp; = &amp; \\sum_{j=1}^{n_w} \\ln f(y_{w,j}; a_w, b_w, k) + \\sum_{j=1}^{n_n} \\ln f(y_{2, n}; a_n, b_n, k) \\end{eqnarray*}\\] Now conduct the likelihood ratio test: (lrt.stat &lt;- 2 * (fir.reduced$value - fir.full$value)) # compute the likelihood ratio test statistic ## [1] 0.6762567 (lrt.pvalue &lt;- pchisq(q = lrt.stat, df = 2, lower.tail = FALSE)) # calculate the p-vlaue ## [1] 0.7131037 The LRT suggests that the full model does not provide a significantly better fit than the reduced model (\\(\\chi^2_2 = 0.676\\), \\(p=0.71\\)). In other words, there is no evidence that the two population types have different relationships between tree size and avearage fecundity. Now compare AIC values for the two models. Because we have already done the LRT, this AIC comparison is for illustration. (aic.reduced &lt;- 2 * fir.reduced$value + 2 * 3) ## [1] 2278.03 (aic.full &lt;- 2 * fir.full$value + 2 * 5) ## [1] 2281.354 (delta.aic &lt;- aic.full - aic.reduced) ## [1] 3.323743 The reduced model is AIC-best, although the \\(\\Delta AIC\\) is only moderately large. We can also fit a Poisson model to these data. Because we have ruled out the need for different models for the two population type, we fit a Poisson model to the data with the two populations pooled together. fir.neg.ll.pois &lt;- function(parms, x, y){ a &lt;- parms[1] b &lt;- parms[2] my.mu &lt;- a * x^b ll.values &lt;- dpois(y, lambda = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.pois &lt;- optim(f = fir.neg.ll.pois, par = c(a = 1, b = 1), x = fir$dbh, y = fir$cones)) ## $par ## a b ## 0.2613297 2.3883860 ## ## $value ## [1] 3161.832 ## ## $counts ## function gradient ## 115 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle.pois &lt;- fir.pois$par[1] b.mle.pois &lt;- fir.pois$par[2] Calculate the AIC for this model: # calculate AIC (aic.pois &lt;- 2 * fir.pois$value + 2 * 2) ## [1] 6327.664 Whoa! The AIC suggests the negative binomial model is an overwhelmingly better fit. Finally, make a plot to compare the two fits: with(fir, plot(cones ~ dbh)) lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) # plot the fit from the NegBin model ## calculate and plot the fit for the Poisson model fit.vals.pois &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals.pois[i] &lt;- a.mle.pois * dbh.vals[i] ^ b.mle.pois } lines(fit.vals.pois ~ dbh.vals, col = &quot;red&quot;) legend(x = 4, y = 280, leg = c(&quot;Neg Bin&quot;, &quot;Poisson&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, bty = &quot;n&quot;) 2.5 Transformable constraints So far, we have not thought much about the numerical optimization routines that R uses to find MLEs. If time allowed, we really should think more deeply about how these routines work. Indeed, Bolker devotes an entire chapter (his chapter 7) to numerical optimization. Because time is short, we wont go that deeply into understanding these methods now, although Bolkers chapter is worth a read if you are so inclined. There is one topic that deserves more of our attention, which is the issue of constriants on the allowable parameter space. (Bolker touches on this in his \\(\\S\\) 7.4.5.) Many times, we write down models with parameters that only make biological sense in a certain range. For example, in the fir data, we know that the parameter \\(a\\) (the average cone production for trees of size \\(x = 1\\)) must be positive. We also know that \\(k\\), the overdispersion parameter in the negative binomial model, must also be positive. However, most numerical optimization routines are not terribly well suited to optimizing over a constrained space. (The presence of constraints is one of the reasons why it is important to initiate numerical optimization routines with reasonalbe starting values.) One exception is the L-BFGS-B method, available in optim, which will permit so-called rectangular constraints. An alternative approach that will work with any numerical optimization scheme is to transform the constraints away. That is, transform the parameterization to a new scale that is unconstrained. Because of the invariance principle of MLEs, these transformations wont change the MLEs that we eventually find, as long as the MLEs are not on the edge of the original, constrained space. To illustrate, consider the fir data again, and consider the negative-binomial model fit to the entire data set, ignoring differences between wave vs. non-wave populations. To transform away the constraints on \\(a\\) and \\(k\\), re-parameterize the model in terms of the logs of both parameters. That is, define \\[\\begin{align*} a^* &amp; = \\ln (a) \\\\ k^* &amp; = \\ln (k) \\\\ \\end{align*}\\] so that the model is now \\[\\begin{align*} \\mu(x) &amp; = \\exp(a^*) \\times x ^ b \\\\ Y &amp; \\sim \\mbox{NB}(\\mu(x), \\exp(k^*)) \\end{align*}\\] Fitting proceeds in the usual way: fir.neg.ll &lt;- function(parms, x, y){ a &lt;- exp(parms[1]) b &lt;- parms[2] k &lt;- exp(parms[3]) my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k, mu = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.reduced &lt;- optim(f = fir.neg.ll, par = c(a = 0, b = 1, k = 0), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## -1.1914367 2.3195050 0.4074672 ## ## $value ## [1] 1136.015 ## ## $counts ## function gradient ## 158 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Back-transforming to the original scale recovers the previous MLEs: (a.mle &lt;- exp(fir.reduced$par[1])) ## a ## 0.3037845 (b.mle &lt;- fir.reduced$par[2]) ## b ## 2.319505 (k.mle &lt;- exp(fir.reduced$par[3])) ## k ## 1.503006 The constraint issue also explains why we received warnings from R when we first found the MLEs for the tadpole predation data in Section 2.2. Another example that one frequently encounters in ecology are parameters that are constrained to lie between 0 and 1, such as a survival probability. A logit (or log odds) transformation will eliminate the constraints on a parameter that lies between 0 and 1. 2.6 The negative binomial distriution, revisited The negative binomial distribution is a funny distribution that is frequently misunderstood by ecologists. In ecology, the negative binomial distribution is typically parameterized by the distributions mean (which we typically write as \\(\\mu\\)) and the overdispersion parameter, almost always written as \\(k\\). In this parameterization, if \\(X\\) has a negative binomial distribution with mean \\(\\mu\\) and overdispersion parameter \\(k\\), then the variance of \\(X\\) is \\[ Var(X) = \\mu + \\frac{\\mu^2}{k} \\] Thus, for fixed \\(\\mu\\), the variance increases as \\(k\\) decreases. As \\(k\\) gets large, the variance approaches \\(\\mu\\), and the negative binomial distribution approaches a Poisson distribution. There are a few occasions in ecology where the overdispersion parameter \\(k\\) has a mechanistic interpretation. In all other cases, though, \\(k\\) is merely a phenomenological descriptor that captures the relationship between the mean and variance for one particular value of \\(\\mu\\). The error that most ecologists make is to assume that a single value of \\(k\\) should prevail across several values of \\(\\mu\\). If \\(k\\) is phenomenological, there is no reason that \\(k\\) should remain fixed as \\(\\mu\\) changes. The fit to the fir data exemplifies this error, as so far we have assumed that one value of \\(k\\) must prevail across all sizes of trees. By assuming that \\(k\\) is fixed, we impose a relationship on the data where the variance must increase quadratically as the mean increases. This may be a reasonable model for the relationship between the variance and the mean, or it may not be. Instead of assuming \\(k\\) constant, another equally viable approach might be to assume that \\(k\\) is a linear function of \\(\\mu\\). In other words, We might set \\(k = \\kappa \\mu\\) for some value of \\(\\kappa\\). In this case, for a given mean \\(\\mu\\), the variance would be \\(\\mu + \\frac{\\mu^2}{\\kappa \\mu} = \\mu \\left(1 + \\frac{1}{\\kappa}\\right)\\), so that the variance would increase linearly as the mean increases. We can try fitting this alternative model to the fir tree data, again pooling wave and non-wave populations together. fir.alt.neg.ll &lt;- function(parms, x, y){ a &lt;- exp(parms[1]) b &lt;- parms[2] k &lt;- exp(parms[3]) my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k * my.mu, mu = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.alt &lt;- optim(f = fir.alt.neg.ll, par = c(a = 0, b = 1, k = 0), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## -1.008373 2.243545 -3.278311 ## ## $value ## [1] 1128.403 ## ## $counts ## function gradient ## 182 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL (a.mle.alt &lt;- exp(fir.alt$par[1])) ## a ## 0.3648121 (b.mle.alt &lt;- fir.alt$par[2]) ## b ## 2.243545 (k.mle.alt &lt;- exp(fir.alt$par[3])) ## k ## 0.03769186 If we compare the fits graphically, the alternative model doesnt generate a dramatically different fit for the relationship between the average cone production and tree size: fit.vals.alt &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals.alt[i] &lt;- a.mle.alt * dbh.vals[i] ^ b.mle.alt } with(fir, plot(cones ~ dbh)) lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) lines(fit.vals.alt ~ dbh.vals, col = &quot;red&quot;) legend(&quot;topleft&quot;, col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, leg = c(&quot;original&quot;, &quot;alternate&quot;)) However, the two models imply very different relationships between the variance in cone production and tree size. Lets look at the implied relationship between the standard deviation of cone production and tree size: mu.vals &lt;- seq(from = 0, to = max(fit.vals), length = 100) sd.vals.nb1 &lt;- sqrt(mu.vals + mu.vals ^ 2 / k.mle) sd.vals.nb2 &lt;- sqrt(mu.vals * (1 + 1 / k.mle.alt)) plot(mu.vals, sd.vals.nb1, xlab = &quot;mean&quot;, ylab = &quot;SD&quot;, type = &quot;l&quot;, col = &quot;blue&quot;) lines(mu.vals, sd.vals.nb2, col = &quot;red&quot;) legend(&quot;topleft&quot;, col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, leg = c(&quot;original&quot;, &quot;alternate&quot;)) We can calculate the AIC for this alternate parameterization as well: (aic.alt &lt;- 2 * fir.alt$value + 2 * 3) ## [1] 2262.805 Recall that the AIC value for the original fit was 2278.0. Thus the model with the alternative parameterization is considerably better by AIC. "],["bayesian-computation.html", "Chapter 3 Bayesian computation 3.1 Computations with conjugate priors 3.2 JAGS in R 3.3 rstanarm", " Chapter 3 Bayesian computation This chapter of the computing companion will focus solely on the computing aspects of Bayesian computation in R. See the course notes or relevant sections of Bolker for the underlying theory. The landscape of computing tools available to fit Bayesian models is fluid. Here, we will look at three tools currently available: R2jags, which is based on the JAGS (Just Another Gibbs Sampler) platform, rstan, which is based on the computer program Stan (itself based on Hamiltonian Monte Carlo, or HMC), and the recent rstanarm, which seeks to put much of the computational details in the background. (The arm portion of the name rstanarm is an acronym for applied regression modeling.) Throughout, we will be working with two data sets: the horse kick data (again), and a data set that details how the rate at which a cricket chirps depends on the air temperature. The horse kick data are useful in this context because a Gamma distribution is a conjugate prior for Poisson data. Thus, if we use a Gamma prior, then we know the posterior exactly. Therefore, we can compare the approximations provided by stochastic sampling schemes to the known posterior. The cricket data set will be used as an example of a simple linear regression, even though the data hint that the actual relationship between temperature and the rate of chirping is nonlinear. 3.1 Computations with conjugate priors Suppose that we observe an iid random sample \\(X_1, \\ldots, X_n\\) from a Poisson distribution with unknown parameter \\(\\lambda\\). (This is the setting for the horse-kick data.) If we place a Gamma prior with shape parameter \\(a\\) and rate parameter \\(r\\) on \\(\\lambda\\), then the posterior distribution is also Gamma with shape parameter \\(a + \\sum_n X_n\\) and rate parameter \\(r + n\\). In other words, \\[\\begin{align*} \\lambda &amp; \\sim \\mbox{Gamma}(a, r) \\\\ X_1, \\ldots, X_n &amp; \\sim \\mbox{Pois}(\\lambda) \\\\ \\lambda | X_1, \\ldots, X_n &amp; \\sim \\mbox{Gamma}(a + \\sum_n X_n, r + n) \\\\ \\end{align*}\\] In the horse kick data, \\(\\sum_n x_n = 196\\) and \\(n = 280\\). Suppose we start with the vague Gamma prior \\(a=.01\\), \\(r = .01\\) on \\(\\lambda\\). This prior has mean \\(a/r = 1\\) and variance \\(a/r^2 = 100\\). The posterior distribution for \\(\\lambda\\) is then a Gamma with shape parameter \\(a = 196.01\\) and rate parameter \\(280.01\\). We can plot it: horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE, stringsAsFactors = TRUE) l.vals &lt;- seq(from = 0, to = 2, length = 200) plot(l.vals, dgamma(l.vals, shape = 196.01, rate = 280.01), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;&quot;) lines(l.vals, dgamma(l.vals, shape = .01, rate = .01), lty = &quot;dashed&quot;) abline(v = 0.7, col = &quot;red&quot;) legend(&quot;topleft&quot;, leg = c(&quot;prior&quot;, &quot;posterior&quot;), lty = c(&quot;dashed&quot;, &quot;solid&quot;)) The red line shows the MLE, which is displaced slightly from the posterior mode. As a point estimate, we might consider any of the following. The posterior mean can be found exactly as \\(a/r\\) = 0.70001. Alternatively, we might consider the posterior median qgamma(0.5, shape = 196.01, rate = 280.01) ## [1] 0.6988206 Finally, we might conisder the posterior mode: optimize(f = function(x) dgamma(x, shape = 196.01, rate = 280.01), interval = c(0.5, 1), maximum = TRUE) ## $maximum ## [1] 0.6964383 ## ## $objective ## [1] 7.995941 To find a 95% confidence interval, we might consider the central 95% interval: qgamma(c(0.025, 0.975), shape = 196.01, rate = 280.01) ## [1] 0.6054387 0.8013454 A 95% highest posterior density (HPD) interval takes a bit more work: diff.in.pdf &lt;- function(x){ upper &lt;- qgamma(p = x, shape = 196.01, rate = 280.01) lower &lt;- qgamma(p = x - .95, shape = 196.01, rate = 280.01) dgamma(upper, shape = 196.01, rate = 280.01) - dgamma(lower, shape = 196.01, rate = 280.01) } (upper.qtile &lt;- uniroot(diff.in.pdf, interval = c(0.95, 1))$root) ## [1] 0.9722176 (hpd.ci &lt;- qgamma(p = c(upper.qtile - .95, upper.qtile), shape = 196.01, rate = 280.01)) ## [1] 0.6031732 0.7988576 We might also ask questions like: What is the posterior probability that \\(\\lambda &gt; 2/3\\)? These caluclations are straightforward in a Bayesian context, and they make full sense. pgamma(2/3, shape = 196.01, rate = 280.01, lower.tail = FALSE) ## [1] 0.7434032 Thus we would say that there is a 0.743 posterior probability that \\(\\lambda &gt; 2/3\\). As an illustration, note that if we had begun with a more informative prior  say, a gamma distribution with shape parameter \\(a = 50\\) and rate parameter = \\(100\\)  then the posterior would have been more of a compromise between the prior and the information in the data: plot(l.vals, dgamma(l.vals, shape = 196 + 50, rate = 100 + 280), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;&quot;) lines(l.vals, dgamma(l.vals, shape = 50, rate = 100), lty = &quot;dashed&quot;) abline(v = 0.7, col = &quot;red&quot;) legend(&quot;topleft&quot;, leg = c(&quot;prior&quot;, &quot;posterior&quot;), lty = c(&quot;dashed&quot;, &quot;solid&quot;)) 3.2 JAGS in R All of the computational tools that we will examine in this section involve some form of stochastic sampling from the posterior. This computing companion will largely use the default settings, though in real practice the analyst will often have to do considerable work adjusting the settings to obtain a satisfactory approximation. Well use JAGS through R, using the library r2jags. Here is JAGS code to approximate the posterior to \\(\\lambda\\) for the horse kick data, using the vague prior. require(R2jags) ## Loading required package: R2jags ## Warning: package &#39;R2jags&#39; was built under R version 4.1.1 ## Loading required package: rjags ## Warning: package &#39;rjags&#39; was built under R version 4.1.1 ## Loading required package: coda ## Linked to JAGS 4.3.0 ## Loaded modules: basemod,bugs ## ## Attaching package: &#39;R2jags&#39; ## The following object is masked from &#39;package:coda&#39;: ## ## traceplot horse.model &lt;- function() { for (j in 1:J) { # J = 280, number of data points y[j] ~ dpois (lambda) # data model: the likelihood } lambda ~ dgamma (0.01, 0.01) # prior # note that BUGS / JAGS parameterizes # gamma by shape, rate } jags.data &lt;- list(y = horse$deaths, J = length(horse$deaths)) jags.params &lt;- c(&quot;lambda&quot;) jags.inits &lt;- function(){ list(&quot;lambda&quot; = rgamma(0.01, 0.01)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = horse.model, n.chains = 3, n.iter = 5000) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 280 ## Unobserved stochastic nodes: 1 ## Total graph size: 283 ## ## Initializing model Lets take a look at some summary statistics of the fit print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmp6RrURH/model28482dc569e5.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## lambda 0.700 0.050 0.603 0.667 0.698 0.733 0.801 1.002 1500 ## deviance 629.299 1.404 628.310 628.402 628.746 629.602 633.151 1.001 3800 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 1.0 and DIC = 630.3 ## DIC is an estimate of expected predictive error (lower deviance is better). The Rhat values suggest that our chains have converged, as we might hope for such a simple model. We can generate a trace plot using traceplot to inspect convergence visually, but beware that visual assessment of convergence is prone to error. For an rjags object, the raw MCMC samples are stored in BUGSoutput$sims.list. Sometimes it is helpful to analyze these samples directly. For example, with these samples we can estimate other posterior quantities, such as the posterior median of \\(\\lambda\\), or generate a 95% central posterior confidence interval directly: mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) summary(mcmc.output) ## deviance lambda ## Min. :628.3 Min. :0.5295 ## 1st Qu.:628.4 1st Qu.:0.6668 ## Median :628.7 Median :0.6981 ## Mean :629.3 Mean :0.6998 ## 3rd Qu.:629.6 3rd Qu.:0.7327 ## Max. :642.3 Max. :0.8709 median(mcmc.output$lambda) ## [1] 0.6981384 quantile(mcmc.output$lambda, c(.025, .975)) ## 2.5% 97.5% ## 0.6031098 0.8010843 We can also use the lattice package to construct smoothed estimates of the posterior density: require(lattice) ## Loading required package: lattice jagsfit.mcmc &lt;- as.mcmc(jagsfit) densityplot(jagsfit.mcmc) For a more involved example, lets take a look at the simple regression fit to the cricket data. First, well make a plot of the data and fit a SLR model by least squares. cricket &lt;- read.table(&quot;data/cricket.txt&quot;, header = TRUE) cricket.slr &lt;- lm(chirps ~ temperature, data = cricket) summary(cricket.slr) ## ## Call: ## lm(formula = chirps ~ temperature, data = cricket) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.56009 -0.57930 0.03129 0.59020 1.53259 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.30914 3.10858 -0.099 0.922299 ## temperature 0.21193 0.03871 5.475 0.000107 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9715 on 13 degrees of freedom ## Multiple R-squared: 0.6975, Adjusted R-squared: 0.6742 ## F-statistic: 29.97 on 1 and 13 DF, p-value: 0.0001067 plot(chirps ~ temperature, data = cricket) abline(cricket.slr) Now well fit the same model in JAGS, using vague priors for all model parameters cricket.model &lt;- function() { for (j in 1:J) { # J = number of data points y[j] ~ dnorm (mu[j], tau) # data model: the likelihood # note that BUGS / JAGS uses precision # instead of variance mu[j] &lt;- b0 + b1 * x[j] # compute the mean for each observation } b0 ~ dnorm (0.0, 1E-6) # prior for intercept b1 ~ dnorm (0.0, 1E-6) # prior for slope tau ~ dgamma (0.01, 0.01) # prior for tau # note that BUGS / JAGS parameterizes # gamma by shape, rate sigma &lt;- pow(tau, -1/2) # the SD of the residaul errors } jags.data &lt;- list(y = cricket$chirps, x = cricket$temperature, J = nrow(cricket)) jags.params &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;sigma&quot;) jags.inits &lt;- function(){ list(&quot;b0&quot; = rnorm(1), &quot;b1&quot; = rnorm(1), &quot;tau&quot; = runif(1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = cricket.model, n.chains = 3, n.iter = 5000) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 3 ## Total graph size: 70 ## ## Initializing model print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmp6RrURH/model284830a52fb3.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## b0 -0.369 3.355 -7.036 -2.434 -0.386 1.793 6.282 1.001 3500 ## b1 0.213 0.042 0.130 0.186 0.213 0.238 0.297 1.001 3800 ## sigma 1.029 0.228 0.697 0.870 0.991 1.139 1.578 1.001 2400 ## tau 1.074 0.432 0.402 0.771 1.018 1.320 2.057 1.001 2400 ## deviance 42.900 2.799 39.794 40.891 42.127 44.042 50.482 1.001 3600 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.9 and DIC = 46.8 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) The output of the print function gives the quantiles that one would use to calculate a 95% central credible interval. To find a HPD credible interval, we can use the HPDinterval function in the coda library. The coda library contains a variety of routines for post-processing of MCMC ouput. If we simply pass the jagsfit object to the HPDinterval function, it will return an HPD interval for each of the three chains. This isnt what we want, so well extract the raw MCMC samples first, and then coerce them to a data frame. mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) summary(mcmc.output) ## b0 b1 deviance sigma ## Min. :-20.4886 Min. :0.05251 Min. :39.58 Min. :0.5433 ## 1st Qu.: -2.4338 1st Qu.:0.18642 1st Qu.:40.89 1st Qu.:0.8702 ## Median : -0.3861 Median :0.21264 Median :42.13 Median :0.9909 ## Mean : -0.3693 Mean :0.21268 Mean :42.90 Mean :1.0289 ## 3rd Qu.: 1.7932 3rd Qu.:0.23840 3rd Qu.:44.04 3rd Qu.:1.1390 ## Max. : 12.0984 Max. :0.45759 Max. :61.95 Max. :2.3275 ## tau ## Min. :0.1846 ## 1st Qu.:0.7708 ## Median :1.0185 ## Mean :1.0737 ## 3rd Qu.:1.3205 ## Max. :3.3880 Now well coerce the data frame mcmc.output to an MCMC object, and pass it to HPDinterval: HPDinterval(as.mcmc(mcmc.output)) ## lower upper ## b0 -7.0368111 6.2662717 ## b1 0.1320111 0.2978445 ## deviance 39.5819131 48.4761928 ## sigma 0.6570863 1.4854675 ## tau 0.3440335 1.9619370 ## attr(,&quot;Probability&quot;) ## [1] 0.9498667 One of the merits of the Bayesian approach is that the posterior samples provide an immediate tool for propagating uncertainty to (possibly derived) quantities of interest. We can summarize the uncertainty in the regression fit graphically by randomly sampling a subset of these samples (say, 100 of them) and using them to plot a collection of regression lines: plot(chirps ~ temperature, data = cricket, type = &quot;n&quot;) # we&#39;ll add the points later so that they lie on top of the lines, # instead of the other way around subset.samples &lt;- sample(nrow(mcmc.output), size = 100) for(i in subset.samples) { with(mcmc.output, abline(a = b0[i], b = b1[i], col = &quot;deepskyblue&quot;, lwd = 0.25)) } with(cricket, points(chirps ~ temperature)) We can also propagate the uncertainty to estimate, say, the posterior distribution for the value of the regression line when the temperature is 85 F. This quantifies the uncertainty in the average number of chirps at this temperature. (We can think of it as a vertical slice through the above plot.) avg.chirps.85 &lt;- with(mcmc.output, b0 + b1 * 85) summary(avg.chirps.85) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.20 17.50 17.71 17.71 17.92 19.09 quantile(avg.chirps.85, probs = c(.025, 0.975)) ## 2.5% 97.5% ## 17.03160 18.38473 We could use the density function to get a quick idea of the shape of the distribution: plot(density(avg.chirps.85)) Thus, we might say that the posterior mean for the average number of chirps at 85 F is 17.71, and a central 95% credible interval is (17.03, 18.38). Finally, we can use the posterior samples to estimate the uncertainty in a future observation. When we use a posterior distribution to estimate the distribution of a future observation, we refer to it as a posterior predictive distribution. The posterior predictive distribution must also include the error around the regression line. We can estimate the posterior predictive distribution as follows. Suppose we denote sample \\(i\\) from the posterior as \\(\\beta_{0, i}\\), \\(\\beta_{1, i}\\), and \\(\\sigma_i\\). Then for each posterior sample we will generate a new hypothetical observation \\(y_i^\\star\\) by sampling from a Gaussian distribution with mean equal to ${0,i} + {1,i} x $ and standard deviation \\(\\sigma_i\\), where \\(x = 85\\). The distribution of the \\(y_i^*\\)s then gives the posterior predictive distribution that we seek. n.sims &lt;- nrow(mcmc.output) new.errors &lt;- with(mcmc.output, rnorm(n.sims, mean = 0, sd = sigma)) new.chirps.85 &lt;- with(mcmc.output, b0 + b1 * 85) + new.errors plot(density(new.chirps.85)) summary(new.chirps.85) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 13.55 16.96 17.65 17.68 18.38 23.95 quantile(new.chirps.85, probs = c(.025, 0.975)) ## 2.5% 97.5% ## 15.51829 19.86276 Thus, the posterior predictive distribution has a central 95% credible interval of (15.52, 19.86). Although it hasnt caused any difficulty here, the slope and intercept are strongly negatively correlated in the posterior. We can visualize this posterior correlation: library(hexbin) ## Warning: package &#39;hexbin&#39; was built under R version 4.1.1 library(RColorBrewer) rf &lt;- colorRampPalette(rev(brewer.pal(11, &#39;Spectral&#39;))) with(jagsfit$BUGSoutput$sims.list, hexbinplot(b1 ~ b0, colramp = rf)) We can estimate the posterior correlation between the intercept and the slope by accessing the raw MCMC samples cor(mcmc.output[, -c(3:4)]) ## b0 b1 tau ## b0 1.000000000 -0.996663525 0.002187358 ## b1 -0.996663525 1.000000000 -0.003298596 ## tau 0.002187358 -0.003298596 1.000000000 Thus we estimate that the intercept and slope have a posterior correlation of -0.997. We could make life easier on ourselves by centering the predictor and trying again: cricket$temp.ctr &lt;- cricket$temperature - mean(cricket$temperature) jags.data &lt;- list(y = cricket$chirps, x = cricket$temp.ctr, J = nrow(cricket)) jags.params &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;sigma&quot;) jags.inits &lt;- function(){ list(&quot;b0&quot; = rnorm(1), &quot;b1&quot; = rnorm(1), &quot;tau&quot; = runif(1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = cricket.model, n.chains = 3, n.iter = 5000) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 3 ## Total graph size: 70 ## ## Initializing model print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmp6RrURH/model284833fb342a.txt&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## b0 16.653 0.281 16.090 16.474 16.655 16.834 17.217 1.001 3800 ## b1 0.211 0.042 0.130 0.184 0.210 0.238 0.294 1.001 3800 ## sigma 1.032 0.229 0.705 0.875 0.993 1.147 1.590 1.001 3800 ## tau 1.064 0.419 0.396 0.760 1.014 1.308 2.013 1.001 3800 ## deviance 42.949 2.787 39.796 40.902 42.233 44.154 49.996 1.001 3800 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule, pD = var(deviance)/2) ## pD = 3.9 and DIC = 46.8 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) The posteriors for the intercept and slope are now uncorrelated: library(hexbin) library(RColorBrewer) rf &lt;- colorRampPalette(rev(brewer.pal(11, &#39;Spectral&#39;))) with(jagsfit$BUGSoutput$sims.list, hexbinplot(b1 ~ b0, colramp = rf)) mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) cor(mcmc.output[, -c(3:4)]) ## b0 b1 tau ## b0 1.000000000 -0.017817161 -0.001781149 ## b1 -0.017817161 1.000000000 0.003805496 ## tau -0.001781149 0.003805496 1.000000000 3.3 rstanarm The rstanarm package is a recent set of routines that seeks to provide a user-friendly front end to Bayesian analysis with Stan. Specifically, rstanarm provides functions for fitting standard statistical models that are meant to mimic the analogous fitting functions in R. For example, the basic routine for fitting linear models in R is lm; rstanarm provides a function stan_lm that strives to have the same functionality and interface as lm, albeit using Stan under the hood to generate Bayesian inference. (That said, the main workhorse function in rstanarm for model fitting is stan_glm, which attempts to mimic the native R function glm for fitting generalized linear models. Separately, the developers of rstanarm have taken the not unreasonable stance that generalized linear models should supplant general linear models as the analysts default approach to model fitting.) To provide functionality that is similar to Rs native model-fitting routines, the functions in rstanarm make a number of operational decisions behind the scenes. Most notably, the model fitting routines in rstanarm will select default priors and default HMC parameters. While these defaults can always be modified by the analyst, the implementation of software that chooses priors by default is radical. First, the developers of rstanarm have their own particular view about what the role of the prior should be in data analysis. While their view is a considered one, by no means does it reflect a consensus that extends beyond the developers of the software. If you use rstanarms routines out of the box, you are accepting this view as your own if you do not specify the priors yourself. Second, as best I understand, the methods by which rstanarm chooses default priors still appear to be in some flux. That means that future versions of rstanarm may supply different default priors than those that are supplied today. As a result, the behavior of rstanarm today may differ from its behavior tomorrow, if you use the default priors. All that said, here is how you might use rstanarm to fit the simple regression to the cricket data: require(rstanarm) ## Loading required package: rstanarm ## Loading required package: Rcpp ## This is rstanarm version 2.21.1 ## - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors! ## - Default priors may change, so it&#39;s safest to specify priors, even if equivalent to the defaults. ## - For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()) stanarm.cricket.fit &lt;- stan_glm(chirps ~ temp.ctr, data = cricket, family = gaussian, seed = 1) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.148 seconds (Warm-up) ## Chain 1: 0.214 seconds (Sampling) ## Chain 1: 0.362 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.175 seconds (Warm-up) ## Chain 2: 0.113 seconds (Sampling) ## Chain 2: 0.288 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.139 seconds (Warm-up) ## Chain 3: 0.129 seconds (Sampling) ## Chain 3: 0.268 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.171 seconds (Warm-up) ## Chain 4: 0.125 seconds (Sampling) ## Chain 4: 0.296 seconds (Total) ## Chain 4: Note that rstanarm has made a variety of decisions about how many chains to run, how long to run them, etc. We can obtain a summary of the model fit by the print command: print(stanarm.cricket.fit, digits = 3) ## stan_glm ## family: gaussian [identity] ## formula: chirps ~ temp.ctr ## observations: 15 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) 16.648 0.257 ## temp.ctr 0.210 0.041 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 1.008 0.198 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg There are a few parts of this output that deserve comment. First, this summary reports the posterior median of the parameters instead of the posterior mean. Second, the authors of rstanarm have made the curious decision to replace the posterior standard deviation (itself the Bayesian counterpart to the frequentists standard error) with someting they call MAD SD. This takes a bit of explanation. The MAD part stands for median absolute deviation. It is the median of the absolute deviations of the posterior samples from the posterior median. In other words, if we have a generic parameter \\(\\theta\\) and label its posterior samples as \\(\\theta_1, \\theta_2, \\ldots, \\theta_n\\), then the MAD of \\(\\theta\\) is \\[ \\mathrm{median}_i(| \\theta_i - \\mathrm{median}_i(\\theta_i) |) \\] According to the authors of rstanarm, Because we are so used to working with standard deviations, when we compute the median absolute deviation, we then rescale it by multiplying by 1.483, which reproduces the standard deviation in the special case of the normal distribution. We call this the mad sd. In other words, the MAD SD is a measure of posterior uncertainty that is meant to be comparable to the posterior standard deviation. (The authors of rstanarm clearly must think this is a more desirable estimate of the posterior uncertainty than the posterior standard deviation; though their reasoning here is not immediately clear to me.) If we want to compute our own summary statistics, we can extract the MCMC samples from the stam_glm fit using the as.matrix command: mcmc.sims &lt;- as.matrix(stanarm.cricket.fit) summary(mcmc.sims) ## (Intercept) temp.ctr sigma ## Min. :15.65 Min. :0.02782 Min. :0.5582 ## 1st Qu.:16.48 1st Qu.:0.18191 1st Qu.:0.8869 ## Median :16.65 Median :0.21023 Median :1.0079 ## Mean :16.65 Mean :0.20955 Mean :1.0457 ## 3rd Qu.:16.83 3rd Qu.:0.23740 3rd Qu.:1.1627 ## Max. :17.68 Max. :0.36177 Max. :2.3400 We might, for example, then use this output to find the posterior standard deviation of each of the parameters, or to find central 95% credible intervals: apply(mcmc.sims, 2, sd) ## (Intercept) temp.ctr sigma ## 0.27003507 0.04262539 0.22262375 apply(mcmc.sims, 2, function(x) quantile(x, c(0.025, 0.975))) ## parameters ## (Intercept) temp.ctr sigma ## 2.5% 16.10111 0.1240071 0.7171401 ## 97.5% 17.17942 0.2906848 1.5816217 Compare these values to the posterior standard deviations and 95% central credible intervals reported in the JAGS fit. "],["smooth-regression.html", "Chapter 4 Smooth regression 4.1 Loess smoothers 4.2 Splines 4.3 Generalized additive models (GAMs)", " Chapter 4 Smooth regression 4.1 Loess smoothers We will illustrate LOESS smoothers with the bioluminescence data found in the ISIT data set. These data can be found by visiting the webpage for the book ``Mixed Effects Models and Extensions in Ecology with R by Zuur et al. (2009). A link to this webpage appears on the course website. ## download the data from the book&#39;s website isit &lt;- read.table(&quot;data/ISIT.txt&quot;, head = T) ## extract the data from station 16 st16 &lt;- subset(isit, Station == 16) ## retain just the variables that we want, and rename st16 &lt;- st16[, c(&quot;SampleDepth&quot;, &quot;Sources&quot;)] names(st16) &lt;- c(&quot;depth&quot;, &quot;sources&quot;) with(st16, plot(sources ~ depth)) Fit a loess smoother using the factory settings: st16.lo &lt;- loess(sources ~ depth, data = st16) summary(st16.lo) ## Call: ## loess(formula = sources ~ depth, data = st16) ## ## Number of Observations: 51 ## Equivalent Number of Parameters: 4.33 ## Residual Standard Error: 4.18 ## Trace of smoother matrix: 4.73 (exact) ## ## Control settings: ## span : 0.75 ## degree : 2 ## family : gaussian ## surface : interpolate cell = 0.2 ## normalize: TRUE ## parametric: FALSE ## drop.square: FALSE Plot the fit, this takes a little work depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(object = st16.lo, newdata = depth.vals, se = TRUE) with(st16, plot(sources ~ depth)) lines(x = depth.vals, y = st16.fit$fit, col = &quot;blue&quot;) # add 95% error bars lines(x = depth.vals, y = st16.fit$fit + st16.fit$se.fit * qt(p = .975, df = st16.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - st16.fit$se.fit * qt(p = .975, df = st16.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Examine the residuals: ## see what the fit returns; maybe the residuals are already there names(st16.lo) # they are! ## [1] &quot;n&quot; &quot;fitted&quot; &quot;residuals&quot; &quot;enp&quot; &quot;s&quot; &quot;one.delta&quot; ## [7] &quot;two.delta&quot; &quot;trace.hat&quot; &quot;divisor&quot; &quot;robust&quot; &quot;pars&quot; &quot;kd&quot; ## [13] &quot;call&quot; &quot;terms&quot; &quot;xnames&quot; &quot;x&quot; &quot;y&quot; &quot;weights&quot; plot(st16.lo$residuals ~ st16$depth) abline(h = 0, lty = &quot;dotted&quot;) Lets look at how changing the span changes the fit. Well write a custom function to fit a LOESS curve, and then call the function with various values for the span. PlotLoessFit &lt;- function(x, y, return.fit = FALSE, ...){ # Caluclates a loess fit with the &#39;loess&#39; function, and makes a plot # # Args: # x: predictor # y: response # return.fit: logical # ...: Optional arguments to loess # # Returns: # the loess fit my.lo &lt;- loess(y ~ x, ...) x.vals &lt;- seq(from = min(x), to = max(x), length = 100) my.fit &lt;- predict(object = my.lo, newdata = x.vals, se = TRUE) plot(x, y) lines(x = x.vals, y = my.fit$fit, col = &quot;blue&quot;) lines(x = x.vals, y = my.fit$fit + my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = x.vals, y = my.fit$fit - my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) if (return.fit) { return(my.lo) } } Now well call the function several times, each time chanigng the value of the span argument to the loess function: PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.5) PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.25) PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.1) Lets try a loess fit with a locally linear regression: PlotLoessFit(x = st16$depth, y = st16$sources, span = 0.25, degree = 1) 4.2 Splines Well use the gam function in the mgcv package to fit splines and additive models. The name of the package is an acronym for Mixed GAM Computation Vehicle. GAM is an acronym for Generalized Additive Model. Warning. I do not understand much of the functionality of mgcv::gam. What follows is my best guess of how the procedure works. The code below fits a regression spline to the bioluminescence data. Actually, the code fits an additive model with the spline as the only predictor. We will say more about additive models later. For now, it is sufficient to think about an additive model as a type of regression in which the linear effect of the predictor has been replaced by a spline. In other words, in terms of a word equation, the model can be represented as \\[ \\mbox{response = intercept + spline + error} \\] The s() component of the model formula designates a spline, and specifies details about the particular type of spline to be fit. The fx = TRUE component of the formula indicates that the amount of smoothing is fixed. The default value for the fx argument is fx = FALSE, in which case the amount of smoothing is determined by (generalized) cross-validation. When fx = TRUE, the parameter k determines the dimensionality (degree of flexibility) of the spline. Larger values of k correspond to greater flexibility, and a less smooth fit. I think that the number of knots is \\(k-4\\), such that setting \\(k=4\\) fits a familiar cubic polynomial with no knots. Setting \\(k=5\\) then fits a regression spline with one knot, etc. I have not been able to figure out where the knots are placed. In any case, well fit a regression spline with two knots: library(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-35. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. st16.rspline &lt;- mgcv::gam(sources ~ s(depth, k = 6, fx = TRUE), data = st16) plot(st16.rspline, se = TRUE) Note that the plot includes only the portion of the model attributable to the covariate effect. This is because we have actually fit an additive model (e.g., a GAM). The plot shows only the spline component, which thus does not include the intercept. To visualize the fit, well need to do a bit more work. with(st16, plot(sources ~ depth)) st16.fit &lt;- predict(st16.rspline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st16.fit$fit) ## add +/- 2 SE following Zuur; this is only approximate. ## should probably use a critical value from a t-dist with n - edf df, that is, 51 - 5 = 46 df lines(x = depth.vals, y = st16.fit$fit + 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) We see that this particular fit is not flexible enough to capture the trend in luminescence at low depth. Lets take a look at the information produced by a call to summary: summary(st16.rspline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sources ~ s(depth, k = 6, fx = TRUE) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4771 0.5858 21.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 5 5 122.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.924 Deviance explained = 93.2% ## GCV = 19.837 Scale est. = 17.503 n = 51 This summary requires a bit more explanation as well. In this GAM, the spline component of the model effectively creates a set of new predictor variables. A regression spline with \\(x\\) knots requires \\(x+3\\) new regression predictors to fit the spline. In this fit, there are two knots, so the spline requires 5 new predictor variables. Because the predictors are determined in advance with regression splines, we can use the usual theory of \\(F\\)-tests from regression to assess the statistical significance of the spline terms. In the section of the output labeled Approximate significance of smooth terms, we see that these 5 predictors together provide a significantly better fit than a model that does not include the spline. I believe this test is actually exact. I think that it is labeled approximate because the default behavior of mgcv::gam is to fit a smoothing spline, for which the test is indeed only approximate. Well discuss this more when we study a smoothing spline fit. Now well fit and plot a smoothing spline. A smoothing spline differs from a regression spline by using generalized cross-validation to determine the appropriate smoothness. st16.spline &lt;- mgcv::gam(sources ~ s(depth), data = st16) plot(st16.spline, se = TRUE) # note that the plot does not include the intercept Again, we make a plot that includes both the points and the fit with(st16, plot(sources ~ depth)) st16.fit &lt;- predict(st16.spline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st16.fit$fit) ## add +/- 2 SE following Zuur; this is only approximate. ## should probably use a critical value from a t-dist with n - edf df, that is, 51 - 9.81 = 41.19 df lines(x = depth.vals, y = st16.fit$fit + 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st16.fit$fit - 2 * st16.fit$se.fit, lty = &quot;dashed&quot;) Lets ask for a summary: summary(st16.spline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## sources ~ s(depth) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.4771 0.3921 31.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 8.813 8.99 158.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.966 Deviance explained = 97.2% ## GCV = 9.7081 Scale est. = 7.8402 n = 51 Note especially the edf component in the Approximate significance of smooth terms section. The label edf stands for effective degrees of freedom. We can think of the edf as the effective number of new predictors that have been added to the model to accommodate the spline. For a smoothing spline, the number and values of the newly created predictors are determined by fitting the model to the data. Because the predictors are calculated in this way, the usual theory of \\(F\\)-testing does not apply. This is why the \\(F\\)-test shown for the smoothing spline is labeled as approximate. Find the AIC for the smoothing spline fit: AIC(st16.spline) ## [1] 260.4811 Heres a small detail. Notice that the syntax of the call to predict is slightly different when making a prediction for a loess object vs. making a prediction for a gam object (which the spline fit is). For a call to predict with a loess object, the new predictor values can be provided in the form of a vector. So, we were able to use depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(object = st16.lo, newdata = depth.vals, se = TRUE) However, for a call to predict with a gam object, the new predictor values must be provided in the form of a new data frame, with variable names that match the variables in the gam model. So, to get predicted values for the spline fit, we needed to use the more cumbersome depth.vals &lt;- with(st16, seq(from = min(depth), to = max(depth), length = 100)) st16.fit &lt;- predict(st16.spline, newdata = data.frame(depth = depth.vals), se = TRUE) 4.3 Generalized additive models (GAMs) Generalized additive models replace the usual linear terms that appear in multiple regression models with splines. That is, suppose we seek to model the relationship between a response \\(y\\) and two predictors, \\(x_1\\) and \\(x_2\\). A standard regression model without polynomial effects or interactions would be written as \\[ y = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + \\varepsilon \\] where \\(\\varepsilon\\) is assumed to be an iid Gaussian random variate with variance \\(\\sigma^2_\\varepsilon\\). This is an additive model, in the sense that the combined effects of the two predictors equal the sum of their individual effects. A generalized additive model (GAM) replaces the individual regression terms with splines. Continuing with the generic example, a GAM would instead model the effects of the two predictors as \\[ y = \\beta_0 + s(x_1) +s(x_2) + \\varepsilon \\] where \\(s(\\cdot)\\) represents a spline. We continue to assume that, conditional on the covariate effects, the responses are normally distributed with constant variance \\(\\sigma^2_\\varepsilon\\). We will illustrate additive modeling using the bird data found in Appendix A of Zuur et al. (2009). Zuur et al. report that these data originally appeared in Loyn (1987) and were featured in Quinn &amp; Keough (2002)s text. Zuur et al. describe these data in the following way: Forest bird densities were measured in 56 forest patches in south-eastern Victoria, Australia. The aim of the study was to relate bird densities to six habitat variables; size of the forest patch, distance to the nearest patch, distance to the nearest larger patch, mean altitude of the patch, year of isolation by clearing, and an index of stock grazing history (1 = light, 5 = intensive). We first read the data and perform some light exploratory analysis and housekeeping. rm(list = ls()) require(mgcv) bird &lt;- read.table(&quot;data/Loyn.txt&quot;, head = T) summary(bird) ## Site ABUND AREA DIST ## Min. : 1.00 Min. : 1.50 Min. : 0.10 Min. : 26.0 ## 1st Qu.:14.75 1st Qu.:12.40 1st Qu.: 2.00 1st Qu.: 93.0 ## Median :28.50 Median :21.05 Median : 7.50 Median : 234.0 ## Mean :28.50 Mean :19.51 Mean : 69.27 Mean : 240.4 ## 3rd Qu.:42.25 3rd Qu.:28.30 3rd Qu.: 29.75 3rd Qu.: 333.2 ## Max. :56.00 Max. :39.60 Max. :1771.00 Max. :1427.0 ## LDIST YR.ISOL GRAZE ALT ## Min. : 26.0 Min. :1890 Min. :1.000 Min. : 60.0 ## 1st Qu.: 158.2 1st Qu.:1928 1st Qu.:2.000 1st Qu.:120.0 ## Median : 338.5 Median :1962 Median :3.000 Median :140.0 ## Mean : 733.3 Mean :1950 Mean :2.982 Mean :146.2 ## 3rd Qu.: 913.8 3rd Qu.:1966 3rd Qu.:4.000 3rd Qu.:182.5 ## Max. :4426.0 Max. :1976 Max. :5.000 Max. :260.0 # get rid of the &#39;Site&#39; variable; it is redundant with the row label bird &lt;- bird[, -1] # log-transform area, distance, ldistance, to remove right-skew bird$L.AREA &lt;- log(bird$AREA) bird$L.DIST &lt;- log(bird$DIST) bird$L.LDIST &lt;- log(bird$LDIST) # change YR.ISOL to years since isolation (study was published in 1987) bird$YR.ISOL &lt;- 1987 - bird$YR.ISOL # keep the only the variables we want bird &lt;- bird[, c(&quot;ABUND&quot;, &quot;L.AREA&quot;, &quot;L.DIST&quot;, &quot;L.LDIST&quot;, &quot;YR.ISOL&quot;, &quot;ALT&quot;, &quot;GRAZE&quot;)] summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 ## Median :24.50 Median :140.0 Median :3.000 ## Mean :37.25 Mean :146.2 Mean :2.982 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 ## Max. :97.00 Max. :260.0 Max. :5.000 Our first attempt at a GAM will entertain smoothing splines for all of the continuous predictors in the model. We will use a linear term for GRAZE because there are too few unique values to support a smooth term: bird.gam1 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + s(ALT), data = bird) summary(bird.gam1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + ## s(ALT) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.4443 2.7798 9.153 9.42e-12 *** ## GRAZE -1.9885 0.8968 -2.217 0.0318 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.446 3.089 12.635 3.98e-06 *** ## s(L.DIST) 3.693 4.559 0.855 0.461 ## s(L.LDIST) 1.000 1.000 0.386 0.538 ## s(YR.ISOL) 1.814 2.238 1.231 0.262 ## s(ALT) 1.000 1.000 0.629 0.432 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.72 Deviance explained = 77.6% ## GCV = 40.987 Scale est. = 32.238 n = 56 The output reports the partial regression coefficient for the lone quantitative predictor GRAZE, and approximate significance tests for the smooth terms for each of the other predictors. We can visualize these smooth terms with a call to plot: plot(bird.gam1) In the interest of time, we take a casual approach to variable selection here. Well drop smooth terms that are clearly not significant to obtain: bird.gam2 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + GRAZE, data = bird) summary(bird.gam2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + GRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.400 2.201 12.903 &lt; 2e-16 *** ## GRAZE -2.980 0.686 -4.344 6.56e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.284 2.903 13.18 3.4e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.68 Deviance explained = 69.9% ## GCV = 39.992 Scale est. = 36.932 n = 56 plot(bird.gam2) Note that the GRAZE variable is currently treated as a numerical predictor. Well try fitting a model with GRAZE as a factor. First well create a new variable that treats GRAZE as a factor. Well use the summary command to confirm that the new variable fGRAZE is indeed a factor. bird$fGRAZE &lt;- as.factor(bird$GRAZE) summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE fGRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 1:13 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 2: 8 ## Median :24.50 Median :140.0 Median :3.000 3:15 ## Mean :37.25 Mean :146.2 Mean :2.982 4: 7 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 5:13 ## Max. :97.00 Max. :260.0 Max. :5.000 Now well proceed to fit the model bird.gam3 &lt;- gam(ABUND ~ s(L.AREA) + fGRAZE, data = bird) plot(bird.gam3) summary(bird.gam3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + fGRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.727275 1.944080 11.691 1.11e-15 *** ## fGRAZE2 0.006623 2.845343 0.002 0.998152 ## fGRAZE3 -0.660124 2.585878 -0.255 0.799592 ## fGRAZE4 -2.170994 3.050736 -0.712 0.480122 ## fGRAZE5 -11.913966 2.872911 -4.147 0.000136 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.761 3.478 11.67 4.71e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.723 Deviance explained = 75.7% ## GCV = 37.013 Scale est. = 31.883 n = 56 To formally compare the models with GRAZE as a numerical vs. categorical predictor, well have to use AIC. We cant use an \\(F\\)-test here because we have used smoothing splines to capture the effect of L.AREA. Thus, the models are not nested. (If we had used regression splines for L.AREA, then the models would have been nested.) We can extract the AICs for these models by a simple call to the AIC function. AIC(bird.gam2) ## [1] 367.1413 AIC(bird.gam3) ## [1] 361.9655 We can see the contrasts used to incorporate the factor fGRAZE in the model by a call to contrasts: with(bird, contrasts(fGRAZE)) ## 2 3 4 5 ## 1 0 0 0 0 ## 2 1 0 0 0 ## 3 0 1 0 0 ## 4 0 0 1 0 ## 5 0 0 0 1 The output here is somewhat opaque because the levels of fGRAZE are 1, 2, \\(\\ldots\\), 5. The output of the call to contrasts shows each of the newly created indicator variables as a column. For example, the first column shows that the predictor named fGRAZE2 takes the value of 1 when the variable fGRAZE equals 2, and is 0 otherwise. Fit an additive model with only a smooth effect of L.AREA, in order to show residuals vs. GRAZE: bird.gam4 &lt;- gam(ABUND ~ s(L.AREA), data = bird) plot(x = bird$GRAZE, y = bird.gam4$residuals) abline(h = 0, lty = &quot;dashed&quot;) Both the plot and the model output suggest that the effect of grazing is primarily due to lower bird abundance in the most heavily grazed category. To conclude, well conduct a formal test of whether the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE. In this case, we have to use regression splines for the smooth effect of L.AREA. Well use regression splines without any internal knots, (which are actually not splines at all, just a cubic trend) because the effect of log area seems to be reasonably well captured by a cubic trend anyway: bird.gam5 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE, data = bird) bird.gam6 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE, data = bird) anova(bird.gam5, bird.gam6, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE ## Model 2: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 51 1869.0 ## 2 48 1543.1 3 325.93 3.3796 0.02565 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both AIC and the \\(F\\)-test suggest that the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE (\\(F_{3,48} = 3.38, p = 0.026\\)). As a final note, Zuur et al. (p.550) observe that the non-linear L.AREA effect is mainly due to two large patches. It would be useful to sample more of this type of patch in the future. (Note the rug plots in any of the plots of the area effect above.) "]]
