[["index.html", "Applied statistical analysis of non-normal and/or correlated data Chapter 1 Maximum likelihood estimation 1.1 Mathematical basics 1.2 Horse-kick data 1.3 Pulse rate data 1.4 Tadpole data 1.5 Transformable constraints", " Applied statistical analysis of non-normal and/or correlated data Kevin Gross 2025-10-24 Chapter 1 Maximum likelihood estimation The likelihood function is the mathematical object that underlies many of the methods that we will study in this course. In this chapter, we will study the properties of the likelihood function for some simple models and data sets. We will see that the likelihood can be used to generate parameter estimates and associated measures of uncertainty (e.g., standard errors and confidence intervals). For most of the methods that we study later in this course, we will use software in which someone else has written code to analyze the likelihood function; thus we won’t have to worry about coding the likelihood function ourselves. However, it is helpful to know how to derive and analyze a likelihood function when needed, because likelihood analysis is flexible and can often be applied in specialized situations where code for a specific analysis may not already exist. 1.1 Mathematical basics The mathematical expression for a likelihood function is identical to the mathematical expression that one would use to find the probability mass or density associated with a particular value of a random variable. For example, suppose that we have a very simple data set that consists of only one observation from a Poisson distribution. Let \\(X\\) denote the value of the single data point, and let \\(\\lambda\\) denote the parameter of the Poisson distribution. In a probability class, we learn that we can find the probability mass associated with any particular value of \\(X\\) using the formula \\[ \\mathrm{Pr}\\!\\left\\{X=x; \\lambda\\right\\} = \\dfrac{e^{-\\lambda} \\lambda^x}{x!}. \\] In R, we can access this probability mass function using the dpois function. For example, if we wanted to find the probability mass associated with \\(X=1\\) when \\(\\lambda = 1.5\\), we could use dpois(x = 1, lambda = 1.5) ## [1] 0.3346952 In likelihood analysis, we use the same mathematical expression for the probability mass function (pmf) of a data set, but we change our perspective. Instead of regarding the parameter as a known quantity and computing the probability associated with various possible values for the data, in likelihood analysis we regard the data as the known quantity and evaluate the same mathematical expression for different parameter values. In notation, this logic translates into an expression that we can write as \\[ \\mathcal{L}\\!\\left(\\lambda; x\\right) = \\mathrm{Pr}\\!\\left\\{X=x; \\lambda\\right\\}. \\] where we have used \\(\\mathcal{L}\\!\\left(\\lambda; x\\right)\\) to denote the likelihood function for \\(\\lambda\\) when we have a data set with value \\(x\\). The expression above seems strange, because nothing seems to be happening; we are simply taking the same mathematical expression and calling it two different things depending on the context. But that is all there is to it, at least with regard to constructing the likelihood function. Because a likelihood function uses the same mathematical formulas as a probability mass function, we can use the same functions that R provides for computing probability masses for discretely valued data (or probability densities for continuously valued data) to compute the a likelihood function. Let’s return to our simple example of observing a single observation from a Poisson distribution. Suppose that observation is \\(X=2\\). We can use the dpois function to evaluate the likelihood for this single observation. For example, we can evaluate the likelihood at \\(\\lambda = 1.5\\): dpois(x = 2, lambda = 1.5) ## [1] 0.2510214 Or we could evaluate the likelihood at \\(\\lambda = 2\\) or \\(\\lambda = 2.5\\): dpois(x = 2, lambda = c(2, 2.5)) ## [1] 0.2706706 0.2565156 Now let’s evaluate the likelihood at a sequence of \\(\\lambda\\) values: my.lhood &lt;- function(lambda) dpois(x = 2, lambda = lambda) curve(my.lhood, from = 0, to = 5, xlab = expression(lambda), ylab = &quot;Likelihood&quot;) We might guess that the likelihood is maximized at \\(\\lambda = 2\\). We’d be right, as the plot below suggests. curve(my.lhood, from = 0, to = 5, xlab = expression(lambda), ylab = &quot;Likelihood&quot;) abline(v = 2, col = &quot;red&quot;) 1.2 Horse-kick data Most real data sets contain more than a single observation. Here is a data set that we can use to illustrate maximum likelihood estimation with a single parameter. Famously, Ladislaus van Bortkewitsch (1868 – 1931) published how many members of the Prussian army were killed by horse kicks in each of 20 years, for each of 14 army corps. In this analysis, we will ignore both the temporal structure and the grouping among corps and treat the data as just a simple random sample1 from a Poisson distribution with \\(n=280\\) data points. As a caveat, these data are often used to illustrate the Poisson distribution, as we will use them. They match the Poisson distribution more neatly than we might expect for most data sets. First import the data. Note that the path name used here is specific to the file directory that was used to create this file. The path name that you use will likely differ. horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) Ask for a summary of the data to make sure the data have been imported correctly. summary(horse) ## year corps deaths ## Min. :1875 Length:280 Min. :0.0 ## 1st Qu.:1880 Class :character 1st Qu.:0.0 ## Median :1884 Mode :character Median :0.0 ## Mean :1884 Mean :0.7 ## 3rd Qu.:1889 3rd Qu.:1.0 ## Max. :1894 Max. :4.0 We can also learn about the data by asking to see the first few records using the head command head(horse) ## year corps deaths ## 1 1875 GC 0 ## 2 1876 GC 2 ## 3 1877 GC 2 ## 4 1878 GC 1 ## 5 1879 GC 0 ## 6 1880 GC 0 or we can see the last few records using the tail command: tail(horse) ## year corps deaths ## 275 1889 C15 2 ## 276 1890 C15 2 ## 277 1891 C15 0 ## 278 1892 C15 0 ## 279 1893 C15 0 ## 280 1894 C15 0 Another useful function to keep in mind is the str function which tells you about the [str]ucture of an R object: str(horse) ## &#39;data.frame&#39;: 280 obs. of 3 variables: ## $ year : int 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 ... ## $ corps : chr &quot;GC&quot; &quot;GC&quot; &quot;GC&quot; &quot;GC&quot; ... ## $ deaths: int 0 2 2 1 0 0 1 1 0 3 ... Let’s plot a histogram of the values: hist(horse$deaths, breaks = seq(from = min(horse$deaths) - 0.5, to = max(horse$deaths) + 0.5, by = 1)) 1.2.1 Calculate and plot the log-likelihood function The first step in likelihood analysis is to construct the likelihood function. The likelihood function is given by the same mathematical expression as the expression for the joint probability mass function of the data. This joint pmf should follow from our probability model for the data. In this case, we will assume that the data are an iid sample from a Poisson distribution with parameter \\(\\lambda\\). Denoting the random sample as \\(X_1, X_2, \\ldots, X_n\\), we might write this model as \\[ X_i \\stackrel{\\text{iid}}{\\sim} \\mathrm{Pois}(\\lambda). \\] To make the notation a bit easier, we’ll write the entire data set as a vector \\(\\mathbf{X} = \\left[ X_1 \\; X_2 \\; \\cdots \\; X_n\\right]^T\\), where we use uppercase \\(\\mathbf{X}\\) to denote the unobserved random vector and lowercase \\(\\mathbf{x}\\) to denote a single realization of \\(\\mathbf{X}\\). The likelihood function is then given by \\[\\begin{align*} \\mathcal{L}(\\lambda; \\mathbf{x}) &amp; = \\mathrm{Pr}\\!\\left\\{\\mathbf{X} = \\mathbf{x}; \\lambda\\right\\} \\\\ &amp; = \\mathrm{Pr}\\!\\left\\{X_1 = x_1, X_2 = x_2, \\ldots X_n = x_n; \\lambda\\right\\} \\\\ &amp; = \\mathrm{Pr}\\!\\left\\{X_1 = x_1; \\lambda\\right\\} \\times \\mathrm{Pr}\\!\\left\\{X_2 = x_2; \\lambda\\right\\} \\times \\cdots \\times \\mathrm{Pr}\\!\\left\\{X_n = x_n; \\lambda\\right\\} \\\\ &amp; = \\prod_{i=1}^n \\mathrm{Pr}\\!\\left\\{X_i = x_i; \\lambda\\right\\}. \\end{align*}\\] The third equality above follows from the independence of the data points. Here’s a function that we can use to compute likelihood for any particular value of \\(\\lambda\\) for the horse data: horse.lhood &lt;- function(my.lambda){ ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda) prod(ll.vals) } Notice, however, that the values of the likelihood are very close to zero, even for reasonable choices of \\(\\lambda\\): horse.lhood(0.75) ## [1] 2.275728e-137 Small values of the likelihood create the risk of numerical underflow. Numerical underflow occurs when positive numbers become too close to zero for the computer to perform sufficiently precise calculations.2 To prevent numerical underflow, we’ll work on the log-likelihood instead of the likelihood itself. Throughout these notes, we’ll use lowercase \\(\\ell = \\ln \\mathcal{L}\\) to denote the log likelihood. Note that when we use the log likelihood, the product of the marginal pmfs above becomes a sum: \\[\\begin{align*} \\ell(\\lambda; \\mathbf{x}) &amp; = \\ln \\prod_{i=1}^n \\mathrm{Pr}\\!\\left\\{X_i = x_i; \\lambda\\right\\} \\\\ &amp; = \\sum_{i=1}^n \\ln \\mathrm{Pr}\\!\\left\\{X_i = x_i; \\lambda\\right\\} \\end{align*}\\] Let’s create a function that calculates the log-likelihood for a value of \\(\\lambda\\): horse.ll &lt;- function(my.lambda){ ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) sum(ll.vals) } We can use this function to calculate the log-likelihood for any value of \\(\\lambda\\), such as \\(\\lambda = 1\\): horse.ll(1) ## [1] -328.2462 Let’s calculate the log-likelihood for many values of \\(\\lambda\\), in preparation for making a plot. We’ll use a loop here, and not worry about vectorization. # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.01, to = 2.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.ll(lambda.vals[i.lambda]) } Now plot the log-likelihood values vs. the values of \\(\\lambda\\): plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;log likelihood&quot;, type = &quot;l&quot;) abline(v = 0.7, col = &quot;red&quot;) 1.2.2 Find the MLE numerically using ‘optimize’ Bolker’s book illustrates numerical optimization using the optim function. The R documentation recommends using optimize for one-dimensional optimization, and optim for optimizing a function in several dimensions. So, we will use optimize here. We will enclose the entire call to optimize in parentheses so that the output is dumped to the command line in addition to being stored as horse.mle. (horse.mle &lt;- optimize(f = horse.ll, interval = c(0.1, 2), maximum = TRUE)) ## $maximum ## [1] 0.7000088 ## ## $objective ## [1] -314.1545 The optimize function returns a ‘list’. A list is an R object that contains components of different types. The numerically calculated MLE is \\(\\hat{\\lambda} \\approx 0.7\\). The ‘objective’ component of horse.mle gives the value of the log-likelihood at that point. 1.3 Pulse rate data The data set pulse.csv contains the heights (in cm) and resting pulse rates (in beats per minute) of 43 graduate students at NCSU. We will use the pulse-rate data as an example to illustrate estimating the mean and variance of a Gaussian distribution from a simple random sample. The purpose of this example is two-fold: first, to illustrate maximum-likelihood estimation with more than one parameter, and second, to illustrate an important result about the MLE of the variance for normally distributed data. pulse &lt;- read.csv(&quot;data/pulse.csv&quot;, head = T) Inspect the data to make sure they have been imported correctly. summary(pulse) ## height rate ## Min. :152.0 Min. : 52 ## 1st Qu.:163.0 1st Qu.: 66 ## Median :168.0 Median : 72 ## Mean :168.2 Mean : 72 ## 3rd Qu.:173.0 3rd Qu.: 78 ## Max. :185.0 Max. :100 head(pulse) ## height rate ## 1 152 68 ## 2 173 68 ## 3 165 82 ## 4 160 60 ## 5 168 74 ## 6 170 80 For the sake of illustration, we will estimate the mean and variance of the normal distribution using the optim function in R. First, we write a function to calculate the log likelihood. pulse.ll &lt;- function(m, v){ ll.vals &lt;- dnorm(pulse$rate, mean = m, sd = sqrt(v), log = TRUE) sum(ll.vals) } Note that R’s function for the pdf of a normal distribution — dnorm — is parameterized by the mean and standard deviation (SD) of the normal distribution. Although it would be just as easy to find the MLE of the standard deviation \\(\\sigma\\), for the sake of illustration, we will seek the MLE of the variance, \\(\\sigma^2\\). (It turns out that, if we write the MLE of the standard deviation as \\(\\hat{\\sigma}\\) and the MLE of the variance as \\(\\hat{\\sigma}^2\\), then \\(\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}\\). This is an example of the invariance property of MLEs.) We can use our function to calculate the likelihood for any choice of mean and variance. For example, let’s try \\(\\mu = 60\\) and \\(\\sigma^2 = 100\\). pulse.ll(m = 60, v = 100) ## [1] -189.6155 We want to maximize the likelihood using optim. Unfortuantely, optim is a little finicky. To use optim, we have to re-write our function pulse.ll so that the parameters to be estimated are passed to the function as a single vector. Also, by default, optim performs minimization instead of maximization. We can change this behavior when we call optim. Alternatively, we can just re-define the function to return the negative log likelihood. pulse.neg.ll &lt;- function(pars){ m &lt;- pars[1] v &lt;- pars[2] ll.vals &lt;- dnorm(pulse$rate, mean = m, sd = sqrt(v), log = TRUE) -sum(ll.vals) } Now we can use optim: (pulse.mle &lt;- optim(par = c(60, 100), # starting values, just a ballpark guess fn = pulse.neg.ll)) ## $par ## [1] 71.99897 93.65332 ## ## $value ## [1] 158.6099 ## ## $counts ## function gradient ## 51 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Note that the MLE of the variance is \\[ \\hat{\\sigma}^2 = \\frac{\\sum_i (x_i - \\bar{x})}{n}. \\] Let’s verify this by calculating the same quantity at the command line: residuals &lt;- with(pulse, rate - mean(rate)) ss &lt;- sum(residuals^2) n &lt;- nrow(pulse) ss / n ## [1] 93.62791 Compare this to the answer given by var, and to the more usual calculation of the variance as \\[ s^2 = \\frac{\\sum_i (x_i - \\bar{x})}{n-1}. \\] (var.usual &lt;- ss / (n - 1)) ## [1] 95.85714 var(pulse$rate) ## [1] 95.85714 One main take-home of this example is that when we use maximum likelihood to estimate variances for normally distributed data, the MLE is biased low. In other words, it underestimates the true variance. When we study hierarchical models later in the semester, we will regularly find ourselves estimating variances for normally distributed effects, and will have to deal with the consequences of the fact that the MLEs of these variances are biased low. For models with 2 parameters, we can visualize the likelihood surface with a contour plot. To do so, the first step is to define a lattice of values at which we want to calculate the log-likelihood. We’ll do so by defining vectors for \\(\\mu\\) and \\(\\sigma^2\\): m.vals &lt;- seq(from = 60, to = 80, by = 0.5) v.vals &lt;- seq(from = 70, to = 125, by = 0.5) Now we will define the matrix that will store the values of the log-likelihood for each combination of \\(\\mu\\) and \\(\\sigma^2\\) in the lattice shown above. ll.vals &lt;- matrix(nrow = length(m.vals), ncol = length(v.vals)) Next, we will write a nested loop that cycles through the lattice points, calculates the log-likelihood for each, and stores the value of the log likelihood in the matrix ll.vals that we just created. for (i.m in 1:length(m.vals)) { for(i.v in 1:length(v.vals)) { ll.vals[i.m, i.v] &lt;- pulse.ll(m = m.vals[i.m], v = v.vals[i.v]) } } Now we will use the contour function to build the contour plot, and then add a red dot for the MLE. contour(x = m.vals, y = v.vals, z = ll.vals, nlevels = 100, xlab = expression(mu), ylab = expression(sigma^2)) # show the MLE points(x = pulse.mle$par[1], y = pulse.mle$par[2], col = &quot;red&quot;) 1.4 Tadpole data Finally, we’ll take a look at the data from the functional response experiment of Vonesh and Bolker (2005), described in section 6.3.1.1 of Bolker’s book. This is another example of using likelihood to estimate parameters in a two-parameter model. This example differs from the previous two examples because we won’t assume that the data constitute a simple random sample from some known distribution like the Gaussian or Poisson distribution. Instead, we’ll build a somewhat more customized model for these data that incorporates some ecological ideas. This process of building a customized model is more typical of how one would analyze a “real” data set. First, we’ll read in the data and explore them in various ways. library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) summary(frog) ## Initial Killed ## Min. : 5.00 Min. : 1.00 ## 1st Qu.: 13.75 1st Qu.: 5.75 ## Median : 25.00 Median :10.00 ## Mean : 38.12 Mean :13.25 ## 3rd Qu.: 56.25 3rd Qu.:18.75 ## Max. :100.00 Max. :35.00 head(frog) ## Initial Killed ## 1 5 1 ## 2 5 2 ## 3 10 5 ## 4 10 6 ## 5 15 10 ## 6 15 9 plot(Killed ~ Initial, data = frog) Following Bolker, we’ll assume that the number of individuals killed takes a binomial distribution, where the number of trials equals the initial tadpole density, and the probability that a tadpole is killed is given by the expression \\[ p_i = \\dfrac{a}{1 + a h N_i}. \\] The two parameters to estimate are \\(a\\), which we interpret as the attack rate when the prey density is low, and \\(h\\), which is the handling time. This model is motivated by the so-called “Type II” functional response of predator-prey ecology, in which the prey consumption rate saturates as prey density grows. In this case, using the Type II functional curve for these data is a pedagogical simplification, because as Vonesh and Bolker (2005) observe, prey densities declined over the two weeks of the experiment. A more appropriate analysis, and one that Vonesh and Bolker (2005) pursue in their paper, takes account of the declining prey densities. For the purposes of this example, though, we’ll ignore this aspect of the analysis (as Bolker (2008) does) and fit the data assuming that the probability of predation is given by the Type II functional response. If we write the number of individuals killed in each trial as \\(Y_i\\), The full model can then be written as \\[\\begin{align*} Y_i &amp; \\sim \\mathrm{Binom}\\left(p_i, N_i \\right) \\\\ p_i &amp; = \\dfrac{a}{1 + a h N_i}. \\end{align*}\\] We’ll first construct the negative log-likelihood function. # negative log-likelihood, for use with optim frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } Now we’ll find the MLE using optim (frog.mle &lt;- optim(par = c(0.5, 1/40), fn = frog.neg.ll)) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced ## $par ## [1] 0.52592567 0.01660454 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 59 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Why did this code produce warnings? Should we care? See section 1.5 below. Let’s extract the MLEs and add a fitted to our data plot. We’ll plot the data and overlay a fitted line. a.mle &lt;- frog.mle$par[1] h.mle &lt;- frog.mle$par[2] # add a line to our plot to show the fitted curve plot(Killed ~ Initial, data = frog) init.values &lt;- with(frog, seq(from = min(Initial), to = max(Initial), length = 100)) pred.values &lt;- a.mle * init.values / (1 + a.mle * h.mle * init.values) lines(x = init.values, y = pred.values, col = &quot;red&quot;) Finally, we’ll plot the likelihood contours. # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) Note that, in contrast to the pulse-rate data, here the likelihood contours form regions whose major axes are not parallel to the parameter axes. We’ll reflect on the implications of this shape in the next section. 1.5 Transformable constraints So far, we have not thought much about the numerical optimization routines that R uses to find MLEs. If time allowed, we really should think more deeply about how these routines work. Indeed, Bolker devotes an entire chapter (his chapter 7) to numerical optimization. Because time is short, we won’t go that deeply into understanding these methods now, although Bolker’s chapter is worth a read if you are so inclined. There is one topic that deserves more of our attention, which is the issue of constriants on the allowable parameter space. (Bolker touches on this in his \\(\\S\\) 7.4.5.) Many times, we write down models with parameters that only make biological sense in a certain range. For example, in the horse-kick data, we know that \\(\\lambda\\) must be positive. However, most numerical optimization routines are not terribly well suited to optimizing over a constrained space. (The presence of constraints is one of the reasons why it is important to initiate numerical optimization routines with reasonable starting values.) One exception is the L-BFGS-B method, available in optim, which will permit so-called rectangular constraints. An alternative approach that will work with any numerical optimization scheme is to transform the constraints away. That is, transform the parameterization to a new scale that is unconstrained. Because of the invariance principle of MLEs, these transformations won’t change the MLEs that we eventually find, as long as the MLEs are not on the edge of the original, constrained space. To illustrate, consider the parameters in the tadpole predation data again. Clearly, both the parameters \\(a\\) and \\(h\\) must be positive. However, there is another constraint on \\(a\\). The probability that a tadpole is eaten must be \\(\\leq 1\\), and thus we must have \\(a \\leq 1 +ahN\\) for all \\(N\\). A little algebra shows that this is equivalent to \\[ a \\leq \\dfrac{1}{1-hN}. \\] The right-hand side above is increasing in \\(N\\), and thus if the above expression is to be true for all \\(N&gt;0\\), then we must have \\(a \\leq 1\\).3 Thus we will illustrate two handy transformations here. For parameters that are constrained to lie on the unit interval, such as \\(a\\) in this case, we can re-define the model in terms of the logit (or log-odds) of \\(a\\). In fact, the logit transformation will work for any parameter that is constrained to lie on an interval; we just have to rescale the transformation accordingly. For a parameter that is constrained to be positive, such as \\(h\\), we can use the log transformation. That is, define \\[\\begin{align*} a^* &amp; = \\ln \\left(\\dfrac{a}{1-a}\\right) \\\\ k^* &amp; = \\ln (k). \\\\ \\end{align*}\\] Fitting proceeds in the usual way. Note that we supply starting values on the transformed scale, and that we invert the transformation before evaluating the likelihood: logit &lt;- function(p) log(p / (1 - p)) invLogit &lt;- function(x) exp(x) / (1 + exp(x)) # negative log-likelihood, for use with optim frog.neg.ll &lt;- function(params){ a &lt;- invLogit(params[1]) h &lt;- exp(params[2]) prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- with(frog, dbinom(Killed, size = Initial, prob = prob.vals, log = TRUE)) -1 * sum(ll.vals) } (frog.mle &lt;- optim(par = c(logit(0.5), log(1/60)), fn = frog.neg.ll)) ## $par ## [1] 0.1038349 -4.0981694 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 43 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Note that we no longer receive the warnings that we had received previously, because now we are guaranteed that the model will yield probabilities between 0 and 1. Back-transforming to the original scale recovers the previous MLEs: (a.mle &lt;- invLogit(frog.mle$par[1])) ## [1] 0.5259354 (h.mle &lt;- exp(frog.mle$par[2])) ## [1] 0.01660304 Bibliography Bolker, Benjamin M. 2008. Ecological Models and Data in R. Princeton University Press. Vonesh, James R, and Benjamin M Bolker. 2005. “Compensatory Larval Responses Shift Trade-Offs Associated with Predator-Induced Hatching Plasticity.” Ecology 86 (6): 1580–91. Recall that when we refer to a data set as a “simple random sample”, we mean that the data are independent and identically distributed. That is, they are independent draws from the same underlying probability distribution.↩︎ In R, we can find the smallest positive value that meets the IEEE technical standard using .Machine$double.xmin. Although R may show answers for computations involving values less than this value, those computations are not trustworthy and may generate strange results.↩︎ The upper bound on \\(a\\) is odd and possibly confusing. In standard predation models, there is no upper bound on \\(a\\). The reason why we have an upper bound on \\(a\\) in this case is because we’ve taken the usual functional expression for Type II predation — which gives a predation rate — and re-interpreted it as the number of tadpoles eaten over a finite time interval. This was probably never a great idea to begin with, and now we are seeing the cost. The \\(a \\leq 1\\) constraint arises in this case because the number of tadpoles eaten in this experiment can’t exceed the number that were originally placed in the experimental mesocosm. This emphasizes that we’re really only using the Type II functional response here for pedagogical convenience. If we really wanted to learn something about the underlying predation rate, then we have to account for the prey depletion over the course of the experiment, as Vonesh and Bolker (2005) do in their study.↩︎ "],["beyondML.html", "Chapter 2 Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function 2.1 Confidence intervals for single parameters 2.2 Confidence regions, profile likelihoods, and associated univariate intervals 2.3 Quadratic approximations to confidence intervals and regions 2.4 Comparing models: Likelihood ratio test and AIC 2.5 The negative binomial distriution, revisited", " Chapter 2 Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function Likelihood can be used for more than simply isolating the MLE. The likelihood can also be used to generate confidence intervals for single parameters, or confidence regions for several parameters. We’ll start by using the horse-kick data to see how to generate a confidence interval for a single parameter, and then move on to considering models with more than one parameter. 2.1 Confidence intervals for single parameters Likelihood regions for parameters can be found using upper contour sets of the log-likelihood function (or, equivalently, using lower contour sets of the negative log-likelihood function). An upper contour set consists of all values of the parameter(s) at which the log-likelihood is no less than a certain value. Even though this definition seems a bit wordy, the intuition is straightforward: if the log-likelihood measures the goodness of fit, then we just want to select the parameter values that correspond to a fit that is at least as good as a certain threshold. We’ll start by constructing a confidence interval for \\(\\lambda\\) with the horse-kick data. horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) horse.neg.ll &lt;- function(my.lambda) { ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) -1 * sum(ll.vals) } # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.5, to = 1.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.neg.ll(lambda.vals[i.lambda]) } plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) To find an asymptotic confidence interval for \\(\\lambda\\) with confidence level \\(100 \\times (1-\\alpha)\\%\\), we want to find all the values of \\(\\lambda\\) for which the negative log-likelihood is no greater than \\(\\frac{1}{2}\\chi^2_1(1-\\alpha)\\) larger than the negative log-likelihood at the MLE.4 By \\(\\chi^2_1(1-\\alpha)\\), we mean \\(1-\\alpha\\) quantile of a \\(\\chi^2_1\\) distribution, which can be found with the function qchisq in R. The code below uses the function uniroot to find the upper and lower bounds of a 95% CI for \\(\\lambda\\). cutoff.ll &lt;- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 # recreate the plot and add a line plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) # use uniroot to find the confidence bounds precisely my.function &lt;- function(my.lambda){ horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda) } (lower &lt;- uniroot(f = my.function, interval = c(0.6, 0.7))) ## $root ## [1] 0.6065198 ## ## $f.root ## [1] -3.556854e-05 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 (upper &lt;- uniroot(f = my.function, interval = c(0.7, 0.9))) ## $root ## [1] 0.8026265 ## ## $f.root ## [1] -0.0001007316 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 As an alternative programming style, we could have defined the objective function on the fly without bothering to create my.function. (lower &lt;- uniroot(f = function(x) horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(x) , interval = c(0.6, 0.7))) ## $root ## [1] 0.6065198 ## ## $f.root ## [1] -3.556854e-05 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 Let’s recreate the plot and add vertical lines to indicate the confidence interval. plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;) abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = c(lower$root, upper$root), col = &quot;red&quot;) # clean up the workspace rm(list = ls()) Thus, the 95% CI for \\(\\lambda\\) is \\((0.607, 0.803)\\). There are two important caveats about the CIs constructed from the likelihood function in this way. First, the coverage is asymptotic, which means that the actual coverage is only guaranteed to match the nominal coverage (e.g., the 95% value) in the limit as the volume of data grows large. As Bolker (p. 194) notes, though, analysts use these asymptotic CIs “very freely”. Secondly, the CI is only valid if the MLE lies in the interior of its range of allowable values. Said the other way, the CI isn’t valid if the MLE lies at the edge of the parameter’s allowable values. We’ll have to worry about this most when constructing likelihood-based CIs for variances. To foreshadow, in mixed models we sometimes encounter a variance whose MLE is 0 — its smallest allowable value. In those case, we’ll have to modify the method detailed here to get a valid CI. Here’s a bit of the theory behind the result above for generating asymptotic CIs from the likelihood function. This is only a sketch of the theory (as it doesn’t justify the key step); for a more complete explanation, Bolker (2008) suggests consulting Kendall and Stuart (1979). Consider a model with \\(k\\) parameters, and write those parameters generically as \\(\\theta_1, \\theta_2, \\ldots, \\theta_k\\). Write the likelihood as \\(\\mathcal{L}(\\theta_1,\\ldots,\\theta_k)\\), and write the MLEs as \\(\\hat{\\theta_1}\\), etc., in the usual way. Now consider a subset of \\(r \\leq k\\) of the parameters — and we might as well write these as the first \\(r\\) parameters, \\(\\theta_1, \\ldots, \\theta_r\\) — and fix these at any particular value, and proceed to maximize the likelihood with respect to the remaining parameters. Write the values of the remaining parameters that maximize the likelihood as \\(\\tilde{\\theta}_{r+1}, \\ldots, \\tilde{\\theta}_k\\). (It’s common to call these values the restricted MLEs, because they maximize the likelihood restricted to the values \\(\\theta_1, \\ldots, \\theta_r\\), but in using this terminology we should not confuse these with REML estimates, which are yet to come and are a separate thing.) In other words, \\[ \\tilde{\\theta}_{r+1}, \\ldots, \\tilde{\\theta}_k = \\mathop{\\mathrm{arg\\,max}}_{\\theta_{r+1},\\ldots,\\theta_{k}} \\mathcal{L}(\\theta_1, \\ldots, \\theta_r, \\theta_{r+1},\\ldots, \\theta_k). \\] Now (and this is the key step that we’ll just assert here) it can be shown that, asymptotically (e.g., in the limit as \\(n\\) becomes large) \\[ 2 \\ln \\dfrac{\\mathcal{L}(\\hat{\\theta}_1,\\ldots,\\hat{\\theta}_k)}{\\mathcal{L}(\\theta_1, \\ldots, \\theta_r, \\tilde{\\theta}_{r+1},\\ldots, \\tilde{\\theta}_k)} \\sim \\chi^2_r. \\] From here, it’s simple algebra to re-express the above in terms of the negative log likelihood to yield \\[ 2 \\times \\left[-\\ell(\\theta_1, \\ldots, \\theta_r, \\tilde{\\theta}_{r+1},\\ldots, \\tilde{\\theta}_k) - (-\\ell(\\hat{\\theta}_1,\\ldots,\\hat{\\theta}_k)) \\right] \\sim \\chi^2_r \\] from which the needed result follows. 2.2 Confidence regions, profile likelihoods, and associated univariate intervals With a 2-parameter model, we can plot a confidence region directly. First some housekeeping to get started: library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } (frog.mle &lt;- optim(par = c(0.5, 1/60), fn = frog.neg.ll)) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced ## $par ## [1] 0.52585566 0.01660104 ## ## $value ## [1] 46.72136 ## ## $counts ## function gradient ## 61 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle &lt;- frog.mle$par[1] h.mle &lt;- frog.mle$par[2] # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;, pch = 16) Equipped with the contour plot, graphing the appropriate confidence region is straightforward. cut.off &lt;- frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2) # recreate the plot and add a line for the 95% confidence region contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;) contour(x = a.vals, y = h.vals, z = ll.vals, levels = cut.off, add = TRUE, col = &quot;red&quot;, lwd = 2) 2.2.1 Profile likelihoods There are several drawbacks to confidence regions. First, while a two-dimensional confidence region can be readily visualized, it is hard to summarize or describe. Second, and more importantly, most models have more than two parameters. In these models, a confidence region would have more than 2 dimensions, and thus would be impractical to visualize. Thus it is helpful, or even essential, to be able to generate univariate confidence intervals for single parameters from high-dimensional likelihoods. There are two common approaches for generating univariate confidence intervals from a high-dimensional likelihood surface. The first method is called slicing. As the name perhaps suggests, a slice likelihood function is computed by allowing one or more parameter(s) of interest to vary while holding the other parameter(s) fixed at their MLEs. The slice likelihood then tells us about how the quality of the fit changes as we vary our parameter(s) of interest. In notation, write our parameter(s) of interest as \\(\\theta\\) and write the other parameter(s) in the model as \\(\\phi\\). (Here, we’ll let both \\(\\theta\\) and \\(\\phi\\) denote either single parameters or vectors of several parameters, as the situation requires. Doing this allows us to cut down on the notation.) Write the full likelihood function as \\(\\mathcal{L}(\\theta, \\phi)\\). The slice likelihood for \\(\\theta\\), which we might write as \\(\\mathcal{L}_s(\\theta)\\), is \\[\\begin{equation} \\mathcal{L}_s(\\theta) = \\mathcal{L}(\\theta, \\hat{\\phi}) \\end{equation}\\] where \\(\\hat{\\phi}\\) is the MLE of \\(\\phi\\). The advantage to the slice likelihood is that it is easy to compute. To write a slice-likelihood function for \\(a\\) in the tadpole data, all we need is slice.ll &lt;- function(a) frog.neg.ll(c(a, h.mle)) Now we can plot the slice likelihood for different values of \\(a\\). a.values &lt;- seq(from = 0.3, to = 0.8, by = 0.01) a.slice &lt;- double(length = length(a.values)) for (i in 1:length(a.values)) a.slice[i] &lt;- slice.ll(a.values[i]) plot(x = a.values, y = a.slice, xlab = &quot;a&quot;, ylab = &quot;negative slice log-likelihood&quot;, type = &quot;l&quot;) The downside to a slice likelihood is that as we vary our parameter(s) of interest, we might be able to improve the fit by allowing the other parameters in the model to adjust. A profile likelihood tells us how the fit changes as we vary our parameter(s) of interest, while allowing the other parameters in the model to adjust. In other words, the profile likelihood tells us about the best fit available for a particular value of our parameter(s) of interest (which isn’t necessarily found by keeping the other model parameters fixed at their MLEs, as the slice likelihood does). Computing a profile likelihood takes a little more work, but it generally provides a more accurate understanding of how our parameter(s) of interest affects the fit. Continuing with our notation above, the profile likelihood for \\(\\theta\\), which we might write as \\(\\mathcal{L}_p(\\theta)\\), is \\[\\begin{equation} \\mathcal{L}_p(\\theta) = \\max_{\\phi} \\, \\mathcal{L}(\\theta, \\phi). \\end{equation}\\] The code below calculates the (negative) profile log-likelihood for \\(a\\) in the tadpole data. profile.ll &lt;- function(my.a) { # Calculate the minimum log likelihood value for a given value of a, the attack rate my.ll &lt;- function(h) frog.neg.ll(c(my.a, h)) my.profile &lt;- optimize(f = my.ll, interval = c(0, 0.03), maximum = FALSE) my.profile$objective } # plot the profile likelihood vs. a a.profile &lt;- double(length = length(a.values)) for (i in 1:length(a.values)) a.profile[i] &lt;- profile.ll(a.values[i]) plot(x = a.values, y = a.profile, xlab = &quot;a&quot;, ylab = &quot;profile negative log-likelihood&quot;, type = &quot;l&quot;) Notice that the profile likelihood is shallower than the slice likelihood. We can find a confidence region for \\(\\theta\\) using the upper contour sets of the profile likelihood, in just the same way as we did with the full likelihood. The code below computes a profile-based confidence interval for the attack rate \\(a\\) in the tadpole data. # Now follow the same steps as before to find the profile 95% CI cut.off &lt;- profile.ll(a.mle) + qchisq(0.95, df = 1) / 2 (lower &lt;- uniroot(f = function(x) cut.off - profile.ll(x) , interval = c(0.3, a.mle))) ## $root ## [1] 0.4024268 ## ## $f.root ## [1] -0.0001303772 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 (upper &lt;- uniroot(f = function(x) cut.off - profile.ll(x) , interval = c(a.mle, 0.8))) ## $root ## [1] 0.6824678 ## ## $f.root ## [1] -9.763258e-06 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 plot(x = a.values, y = a.profile, xlab = &quot;a&quot;, ylab = &quot;negative log-likelihood&quot;, type = &quot;l&quot;) abline(v = c(lower$root, upper$root), col = &quot;blue&quot;) abline(h = cut.off, col = &quot;blue&quot;, lty = &quot;dashed&quot;) So, the 95% profile CI for \\(a\\) is (0.402, 0.682). 2.3 Quadratic approximations to confidence intervals and regions Likelihood profiling provides a straightforward way to summarize a high-dimensional confidence region by univariate confidence intervals. However, these profile intervals can still involve quite a bit of computation. Further, they are not able to capture possible correlations among parameter estimates, which are revealed in (two-dimensional) confidence regions. (Recall the shape of the joint confidence region for the parameters \\(a\\) and \\(h\\) in the tadpole data.) Locally quadratic approximations provide a way to approximate the (already approximate) univariate confidence intervals and bivariate confidence regions using only information about the curvature of the likelihood surface at the MLE. There’s some remarkable theory behind the idea of approximating a log-likelihood with a quadratic function. Asymptotically, (that is, as the number of data points grows large), the sampling distribution of an MLE approaches a Gaussian distribution (with the usual caveat that the MLE can’t be on the edge of the allowable parameter space). But the log-likelihood of the parameters in a Gaussian distribution is exactly quadratic. Therefore, as the number of data points grows large, the log-likelihood function of any model will come to resemble a quadratic function in the neighborhood of the MLE. We’ll first start by revisiting the horse-kick data again. Of course, with the more precise \\(\\chi^2\\) based confidence interval in hand, there is no reason to seek an approximation. But doing so allows us to illustrate the calculations involved, and to see how well the approximation fares in this case. First some housekeeping to read the data into memory, etc. # clean up rm(list = ls()) # read in the data horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE) horse.neg.ll &lt;- function(my.lambda) { ll.vals &lt;- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE) -1 * sum(ll.vals) } # use uniroot to find the confidence bounds precisely my.function &lt;- function(my.lambda){ horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda) } lower &lt;- uniroot(f = my.function, interval = c(0.6, 0.7)) upper &lt;- uniroot(f = my.function, interval = c(0.7, 0.9)) To proceed with the approximation, we need to calculate the second derivative of the log-likelihood at the MLE. We’ll rely on the hessian routine in the numDeriv package to calculate this second derivative for us. However, it’s worth noting that we can approximate the second derivative numerically by the method of finite differences. This method only requires to additional evaluations of the likelihood function! ## this function finds the second derivative at the MLE by finite differences second.deriv &lt;- function(delta.l) { (horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2 } (horse.D2 &lt;- second.deriv(1e-04)) ## [1] 400 # see how the answer changes if we change delta second.deriv(1e-05) ## [1] 400.0003 Let’s compare this answer to the answer obtained by numDeriv::hessian. numDeriv::hessian(func = horse.neg.ll, x = 0.7) ## [,1] ## [1,] 400 The approximate standard error of \\(\\hat{\\lambda}\\) is the square root of the inverse of the second derivative of the likelihood function. (lambda.se &lt;- sqrt(1 / horse.D2)) ## [1] 0.05 Now we can approximate the 95% confidence interval by using critical values from a standard normal distribution. (lower.approx &lt;- 0.7 - qnorm(.975) * lambda.se) ## [1] 0.6020018 (upper.approx &lt;- 0.7 + qnorm(.975) * lambda.se) ## [1] 0.7979982 Compare the approximation to the “exact” values lower$root ## [1] 0.6065198 upper$root ## [1] 0.8026265 Make a plot # create a vector of lambda values using the &#39;seq&#39;uence command lambda.vals &lt;- seq(from = 0.5, to = 1.0, by = 0.01) # create an empty vector to store the values of the log-likelihood ll.vals &lt;- double(length = length(lambda.vals)) # use a loop to find the log-likelihood for each value in lambda.vals for (i.lambda in 1:length(lambda.vals)) { ll.vals[i.lambda] &lt;- horse.neg.ll(lambda.vals[i.lambda]) } plot(ll.vals ~ lambda.vals, xlab = &quot;lambda&quot;, ylab = &quot;negative log likelihood&quot;, type = &quot;l&quot;, col = &quot;red&quot;) approx.ll &lt;- function(lambda) horse.neg.ll(0.7) + (1/2) * (lambda - 0.7)^2 * horse.D2 curve(approx.ll, from = min(lambda.vals), to = max(lambda.vals), add = TRUE, col = &quot;blue&quot;) ################################### ## Now find the confidence interval, and plot it #################################### cutoff.ll &lt;- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 # add a line to the plot abline(h = cutoff.ll, col = &quot;red&quot;, lty = &quot;dashed&quot;) abline(v = c(lower$root, upper$root), col = &quot;red&quot;) abline(v = c(lower.approx, upper.approx), col = &quot;blue&quot;) legend(x = 0.65, y = 326, leg = c(&quot;exact&quot;, &quot;approximate&quot;), pch = 16, col = c(&quot;red&quot;, &quot;blue&quot;), bty = &quot;n&quot;) # clean up rm(list = ls()) Notice that the full \\(\\chi^2\\)-based confidence intervals capture the asymmetry in the information about \\(\\lambda\\). The intervals based on the quadratic approximation are symmetric. Now, use the quadratic approximation to find standard errors for \\(\\hat{a}\\) and \\(\\hat{h}\\) in the tadpole predation data. The first part is preparatory work from old classes. library(emdbook) data(&quot;ReedfrogFuncresp&quot;) # rename something shorter frog &lt;- ReedfrogFuncresp rm(ReedfrogFuncresp) frog.neg.ll &lt;- function(params){ a &lt;- params[1] h &lt;- params[2] prob.vals &lt;- a / (1 + a * h * frog$Initial) ll.vals &lt;- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE) -1 * sum(ll.vals) } frog.mle &lt;- optim(par = c(0.5, 1/60), fn = frog.neg.ll) ## Warning in dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = ## TRUE): NaNs produced (a.mle &lt;- frog.mle$par[1]) ## [1] 0.5258557 (h.mle &lt;- frog.mle$par[2]) ## [1] 0.01660104 Now find the hessian: (D2 &lt;- numDeriv::hessian(func = frog.neg.ll, x = c(a.mle, h.mle))) ## [,1] [,2] ## [1,] 616.5606 -7394.263 ## [2,] -7394.2628 130640.685 The matrix inverse of the hessian is the variance-covariance matrix of the parameters. Note that R uses the function solve to find the inverse of a matrix. # invert to get var-cov matrix (var.matrix &lt;- solve(D2)) ## [,1] [,2] ## [1,] 0.0050493492 2.857932e-04 ## [2,] 0.0002857932 2.383048e-05 We can use the handy cov2cor function to convert the variance matrix into a correlation matrix: cov2cor(var.matrix) ## [,1] [,2] ## [1,] 1.0000000 0.8238872 ## [2,] 0.8238872 1.0000000 Note the large correlation between \\(\\hat{a}\\) and \\(\\hat{h}\\). There are a few paths that we can take from here. It turns out that probability contours of a bivariate normal distribution are ellipses. So, we can use the ellipse::ellipse function to find (say) an approximate 95% confidence region for \\((a,h)\\). (There is some clever math behind this computation, but that math is beside the point here, so we’ll just use the available tools in R). ## Loading required package: ellipse ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs approx.95 &lt;- ellipse::ellipse(var.matrix, center = c(a.mle, h.mle), level = 0.95, npoints = 200) Now we’ll recreate our contour plot of the negative log likelihood, show the exact 95% confidence region, and overlay the approximate 95% confidence ellipse. # plot negative likelihood contours a.vals &lt;- seq(from = 0.3, to = 0.75, by = 0.01) h.vals &lt;- seq(from = 0.001, to = 0.03, by = 0.001) ll.vals &lt;- matrix(nrow = length(a.vals), ncol = length(h.vals)) for (i.a in 1:length(a.vals)) { for(i.h in 1:length(h.vals)) { ll.vals[i.a, i.h] &lt;- frog.neg.ll(c(a.vals[i.a], h.vals[i.h])) } } contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100, xlab = &quot;a&quot;, ylab = &quot;h&quot;) points(x = a.mle, y = h.mle, col = &quot;red&quot;, pch = 16) cut.off &lt;- frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2) contour(x = a.vals, y = h.vals, z = ll.vals, levels = cut.off, add = TRUE, col = &quot;red&quot;, lwd = 2) # add the approximate confidence ellipse lines(approx.95, col = &quot;blue&quot;, lwd = 2) Compare this with Fig. 6.13 in Bolker. We can also use the curvature to compute approximate standard errors for both parameters, as simply the square roots of the diagaonal elements of the variance-covariance matrix. (a.se &lt;- sqrt(var.matrix[1, 1])) ## [1] 0.07105877 (h.se &lt;- sqrt(var.matrix[2, 2])) ## [1] 0.004881647 Let’s use the (approximate) standard error of \\(\\hat{a}\\) to calculate an (approximate) 95% confidence interval: (ci.approx &lt;- a.mle + qnorm(c(0.025, .975)) * a.se) ## [1] 0.3865830 0.6651283 Recall that the 95% confidence interval we calculated by the profile likelihood was \\((0.402, 0.682)\\). So the quadratic approximation has gotten the width of the interval more or less correct, but it has fared less well at capturing the asymmetry of the interval. 2.4 Comparing models: Likelihood ratio test and AIC Obtaining a parsimonious statistical description of data often requires arbitrating between competing model fits. Likelihood provides two tools for comparing models: likelihood ratio tests (LRTs) and information criteria. Of the latter, the best known information criterion is due to Akaike, and takes the name AIC. (Akaike didn’t name AIC after himself; he used AIC to refer to “An information criterion”. In his honor, the acronym is now largely taken to stand for “Akaike’s information criterion”.) LRTs and information criteria have complementary strengths and weaknesses. LRTs are direct, head-to-head comparisons of nested models. By “nested”, we mean that one model can be obtained as a special case of the other. The “reduced”, or less flexible (and thus more parsimonious) model plays the role of the null hypothesis, and the “full”, or more flexible (and thus less parsimonious) model plays the role of the alternative hypothesis. The LRT then formally evaluates whether the improvement in fit offered by the full model is statistically significant, that is, greater than what we would expect merely by chance. On the other hand, information criteria provide a penalized goodness-of-fit measure that can be used to compare many models at once. Information criteria produce a ranking of model fits, and thus a best-fitting model. The downside to information criteria is that there are no hard and fast guidelines to determine when one model provides a significantly better fit than another. The properties of information criteria are also less well understood than the properties of LRTs. To illustrate both, we will use the study of cone production by fir trees studied in \\(\\S\\) 6.6 of Bolker. These data are originally from work by Dodd and Silvertown. The data are much richer than we will examine here. Like Bolker, we will focus on whether the relationship between tree size (as measured by diameter at breast height, or dbh) and the number of cones produces differs between populations that “have experienced wave-like die-offs” and those that have not. First some preparatory work to import and assemble the data: require(emdbook) data(&quot;FirDBHFec&quot;) # give the data a simpler name fir &lt;- FirDBHFec rm(FirDBHFec) fir &lt;- fir[, c(&quot;WAVE_NON&quot;, &quot;DBH&quot;, &quot;TOTCONES&quot;)] # select just the variables we want summary(fir) ## WAVE_NON DBH TOTCONES ## n:166 Min. : 3.200 Min. : 0.0 ## w:205 1st Qu.: 6.400 1st Qu.: 14.0 ## Median : 7.600 Median : 36.0 ## Mean : 8.169 Mean : 49.9 ## 3rd Qu.: 9.700 3rd Qu.: 66.0 ## Max. :17.400 Max. :297.0 ## NA&#39;s :26 NA&#39;s :114 names(fir) &lt;- c(&quot;wave&quot;, &quot;dbh&quot;, &quot;cones&quot;) # rename the variables # get rid of the incomplete records fir &lt;- na.omit(fir) par(mfrow = c(1, 2)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;w&quot;)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;non-wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;n&quot;)) # any non-integral responses? with(fir, table(cones == round(cones))) # illustrate the use of &#39;with&#39; ## ## FALSE TRUE ## 6 236 # round the non-integral values fir$cones &lt;- round(fir$cones) # check with(fir, table(cones == round(cones))) ## ## TRUE ## 242 Like Bolker, we will assume that the average number of cones produced (\\(\\mu\\)) has a power-law relationship with tree dbh (\\(x\\)). We will also assume that the actual number of cones produced (\\(Y\\)) takes a negative binomial distribution with size-dependent mean and overdispersion parameter \\(k\\). That is, our model is \\[\\begin{align*} \\mu(x) &amp; = a x ^ b \\\\ Y &amp; \\sim \\mbox{NB}(\\mu(x), k) \\end{align*}\\] To head in a slightly different direction from Bolker, we will compare two models. In the first, or reduced, model the same parameters will prevail for both wave and non-wave populations. Thus this model has three parameters: \\(a\\), \\(b\\), and \\(k\\). In the second, or full, model, we will allow the \\(a\\) and \\(b\\) parameters to differ between the wave and non-wave populations. (We will continue to assume a common \\(k\\) for both population types.) Using subscripts on \\(a\\) and \\(b\\) to distinguish population types, the full model then has 5 parameters: \\(a_w\\), \\(a_n\\), \\(b_w\\), \\(b_n\\), and \\(k\\). We’ll fit the reduced model first. To do so, we’ll use the dnbinom function in R, in which the \\(k\\) parameter is located in the formal argument “size”. fir.neg.ll &lt;- function(parms, x, y){ a &lt;- parms[1] b &lt;- parms[2] k &lt;- parms[3] my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k, mu = my.mu, log = TRUE) neg.ll &lt;- -1 * sum(ll.values) return(neg.ll) } Note a subtle difference here. In preparation for fitting this same model to different subsets of the data, the function fir.neg.ll has formal arguments that receive the values of the \\(x\\) and \\(y\\) variables. In the call to optim, we can supply those additional values as subsequent arguments in the optim function, as illustrated below. # fit reduced model (fir.reduced &lt;- optim(f = fir.neg.ll, par = c(a = 1, b = 1, k = 1), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## 0.3041425 2.3190142 1.5033525 ## ## $value ## [1] 1136.015 ## ## $counts ## function gradient ## 134 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle &lt;- fir.reduced$par[1] b.mle &lt;- fir.reduced$par[2] k.mle &lt;- fir.reduced$par[3] Make a plot of the reduced model fit, with both populations pooled together: dbh.vals &lt;- seq(from = min(fir$dbh), to = max(fir$dbh), length = 100) fit.vals &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals[i] &lt;- a.mle * dbh.vals[i] ^ b.mle } par(mfrow = c(1, 1)) # don&#39;t break the next figure into two panels with(fir, plot(cones ~ dbh)) # plot the data points lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) Now fit the full model with separate values of \\(a\\) and \\(b\\) for each population: fir.neg.ll.full &lt;- function(parms) { a.w &lt;- parms[1] b.w &lt;- parms[2] a.n &lt;- parms[3] b.n &lt;- parms[4] k &lt;- parms[5] wave &lt;- subset(fir, wave == &quot;w&quot;) nonwave &lt;- subset(fir, wave == &quot;n&quot;) # note how we call fir.neg.ll here, but each time only # passing a subset of the data neg.ll.wave &lt;- fir.neg.ll(parms = c(a = a.w, b = b.w, k = k), x = wave$dbh, y = wave$cones) neg.ll.nonwave &lt;- fir.neg.ll(parms = c(a = a.n, b = b.n, k = k), x = nonwave$dbh, y = nonwave$cones) total.ll &lt;- neg.ll.wave + neg.ll.nonwave return(total.ll) } (fir.full &lt;- optim(f = fir.neg.ll.full, par = c(a.w = 1, b.w = 1, a.n = 1, b.n = 1, k = 1))) ## $par ## a.w b.w a.n b.n k ## 0.4136414 2.1417941 0.2874122 2.3550753 1.5083974 ## ## $value ## [1] 1135.677 ## ## $counts ## function gradient ## 502 NA ## ## $convergence ## [1] 1 ## ## $message ## NULL Let’s make a plot to show the different fits. a.w.mle &lt;- fir.full$par[1] b.w.mle &lt;- fir.full$par[2] a.n.mle &lt;- fir.full$par[3] b.n.mle &lt;- fir.full$par[4] par(mfrow = c(1, 2)) # wave populations fit.vals.wave &lt;- fit.vals.non &lt;- double(length = length(dbh.vals)) plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;w&quot;)) for (i in seq(along = dbh.vals)) { fit.vals.wave[i] &lt;- a.w.mle * dbh.vals[i] ^ b.w.mle } lines(fit.vals.wave ~ dbh.vals, col = &quot;blue&quot;) # non-wave populations plot(cones ~ dbh, data = fir, type = &quot;n&quot;, main = &quot;non-wave&quot;) points(cones ~ dbh, data = subset(fir, wave == &quot;n&quot;)) for (i in seq(along = dbh.vals)) { fit.vals.non[i] &lt;- a.n.mle * dbh.vals[i] ^ b.n.mle } lines(fit.vals.non ~ dbh.vals, col = &quot;red&quot;) Note that to compute the negative log likelihood for the full model, we compute the negative log likelihood for each population separately, and then sum the two negative log likelihoods. We can see the justification for doing so by writing out the log likelihood function explicitly: \\[\\begin{eqnarray*} \\ln L(a_w, a_n, b_w, b_n, k; \\mathbf{y}) &amp; = &amp; \\ln \\prod_{i \\in \\left\\{w, n \\right\\}} \\prod_{j=1}^{n_i} f(y_{ij}; a_w, a_n, b_w, b_n, k) \\\\ &amp; = &amp; \\sum_{i \\in \\left\\{w, n \\right\\}} \\sum_{j=1}^{n_i} \\ln f(y_{ij}; a_w, a_n, b_w, b_n, k) \\\\ &amp; = &amp; \\sum_{j=1}^{n_w} \\ln f(y_{w,j}; a_w, b_w, k) + \\sum_{j=1}^{n_n} \\ln f(y_{2, n}; a_n, b_n, k) \\end{eqnarray*}\\] Now conduct the likelihood ratio test: (lrt.stat &lt;- 2 * (fir.reduced$value - fir.full$value)) # compute the likelihood ratio test statistic ## [1] 0.6762567 (lrt.pvalue &lt;- pchisq(q = lrt.stat, df = 2, lower.tail = FALSE)) # calculate the p-vlaue ## [1] 0.7131037 The LRT suggests that the full model does not provide a significantly better fit than the reduced model (\\(\\chi^2_2 = 0.676\\), \\(p=0.71\\)). In other words, there is no evidence that the two population types have different relationships between tree size and avearage fecundity. Now compare AIC values for the two models. Because we have already done the LRT, this AIC comparison is for illustration. (aic.reduced &lt;- 2 * fir.reduced$value + 2 * 3) ## [1] 2278.03 (aic.full &lt;- 2 * fir.full$value + 2 * 5) ## [1] 2281.354 (delta.aic &lt;- aic.full - aic.reduced) ## [1] 3.323743 The reduced model is AIC-best, although the \\(\\Delta AIC\\) is only moderately large. We can also fit a Poisson model to these data. Because we have ruled out the need for different models for the two population type, we fit a Poisson model to the data with the two populations pooled together. fir.neg.ll.pois &lt;- function(parms, x, y){ a &lt;- parms[1] b &lt;- parms[2] my.mu &lt;- a * x^b ll.values &lt;- dpois(y, lambda = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.pois &lt;- optim(f = fir.neg.ll.pois, par = c(a = 1, b = 1), x = fir$dbh, y = fir$cones)) ## $par ## a b ## 0.2613297 2.3883860 ## ## $value ## [1] 3161.832 ## ## $counts ## function gradient ## 115 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL a.mle.pois &lt;- fir.pois$par[1] b.mle.pois &lt;- fir.pois$par[2] Calculate the AIC for this model: # calculate AIC (aic.pois &lt;- 2 * fir.pois$value + 2 * 2) ## [1] 6327.664 Whoa! The AIC suggests the negative binomial model is an overwhelmingly better fit. Finally, make a plot to compare the two fits: with(fir, plot(cones ~ dbh)) lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) # plot the fit from the NegBin model ## calculate and plot the fit for the Poisson model fit.vals.pois &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals.pois[i] &lt;- a.mle.pois * dbh.vals[i] ^ b.mle.pois } lines(fit.vals.pois ~ dbh.vals, col = &quot;red&quot;) legend(x = 4, y = 280, leg = c(&quot;Neg Bin&quot;, &quot;Poisson&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, bty = &quot;n&quot;) 2.5 The negative binomial distriution, revisited The negative binomial distribution is a funny distribution that is frequently misunderstood by ecologists. In ecology, the negative binomial distribution is typically parameterized by the distribution’s mean (which we typically write as \\(\\mu\\)) and the “overdispersion parameter”, almost always written as \\(k\\). In this parameterization, if \\(X\\) has a negative binomial distribution with mean \\(\\mu\\) and overdispersion parameter \\(k\\), then the variance of \\(X\\) is \\[ Var(X) = \\mu + \\frac{\\mu^2}{k} \\] Thus, for fixed \\(\\mu\\), the variance increases as \\(k\\) decreases. As \\(k\\) gets large, the variance approaches \\(\\mu\\), and the negative binomial distribution approaches a Poisson distribution. There are a few occasions in ecology where the overdispersion parameter \\(k\\) has a mechanistic interpretation. In all other cases, though, \\(k\\) is merely a phenomenological descriptor that captures the relationship between the mean and variance for one particular value of \\(\\mu\\). The error that most ecologists make is to assume that a single value of \\(k\\) should prevail across several values of \\(\\mu\\). If \\(k\\) is phenomenological, there is no reason that \\(k\\) should remain fixed as \\(\\mu\\) changes. The fit to the fir data exemplifies this error, as so far we have assumed that one value of \\(k\\) must prevail across all sizes of trees. By assuming that \\(k\\) is fixed, we impose a relationship on the data where the variance must increase quadratically as the mean increases. This may be a reasonable model for the relationship between the variance and the mean, or it may not be. Instead of assuming \\(k\\) constant, another equally viable approach might be to assume that \\(k\\) is a linear function of \\(\\mu\\). In other words, We might set \\(k = \\kappa \\mu\\) for some value of \\(\\kappa\\). In this case, for a given mean \\(\\mu\\), the variance would be \\(\\mu + \\frac{\\mu^2}{\\kappa \\mu} = \\mu \\left(1 + \\frac{1}{\\kappa}\\right)\\), so that the variance would increase linearly as the mean increases. We can try fitting this alternative model to the fir tree data, again pooling wave and non-wave populations together. fir.alt.neg.ll &lt;- function(parms, x, y){ a &lt;- exp(parms[1]) b &lt;- parms[2] k &lt;- exp(parms[3]) my.mu &lt;- a * x^b ll.values &lt;- dnbinom(y, size = k * my.mu, mu = my.mu, log = TRUE) -1 * sum(ll.values) } (fir.alt &lt;- optim(f = fir.alt.neg.ll, par = c(a = 0, b = 1, k = 0), x = fir$dbh, y = fir$cones)) ## $par ## a b k ## -1.008373 2.243545 -3.278311 ## ## $value ## [1] 1128.403 ## ## $counts ## function gradient ## 182 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL (a.mle.alt &lt;- exp(fir.alt$par[1])) ## a ## 0.3648121 (b.mle.alt &lt;- fir.alt$par[2]) ## b ## 2.243545 (k.mle.alt &lt;- exp(fir.alt$par[3])) ## k ## 0.03769186 If we compare the fits graphically, the alternative model doesn’t generate a dramatically different fit for the relationship between the average cone production and tree size: fit.vals.alt &lt;- double(length = length(dbh.vals)) for (i in seq(along = dbh.vals)) { fit.vals.alt[i] &lt;- a.mle.alt * dbh.vals[i] ^ b.mle.alt } with(fir, plot(cones ~ dbh)) lines(fit.vals ~ dbh.vals, col = &quot;blue&quot;) lines(fit.vals.alt ~ dbh.vals, col = &quot;red&quot;) legend(&quot;topleft&quot;, col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, leg = c(&quot;original&quot;, &quot;alternate&quot;)) However, the two models imply very different relationships between the variance in cone production and tree size. Let’s look at the implied relationship between the standard deviation of cone production and tree size: mu.vals &lt;- seq(from = 0, to = max(fit.vals), length = 100) sd.vals.nb1 &lt;- sqrt(mu.vals + mu.vals ^ 2 / k.mle) sd.vals.nb2 &lt;- sqrt(mu.vals * (1 + 1 / k.mle.alt)) plot(mu.vals, sd.vals.nb1, xlab = &quot;mean&quot;, ylab = &quot;SD&quot;, type = &quot;l&quot;, col = &quot;blue&quot;) lines(mu.vals, sd.vals.nb2, col = &quot;red&quot;) legend(&quot;topleft&quot;, col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, leg = c(&quot;original&quot;, &quot;alternate&quot;)) We can calculate the AIC for this alternate parameterization as well: (aic.alt &lt;- 2 * fir.alt$value + 2 * 3) ## [1] 2262.805 Recall that the AIC value for the original fit was 2278.0. Thus the model with the alternative parameterization is considerably better by AIC. Bibliography Bolker, Benjamin M. 2008. Ecological Models and Data in R. Princeton University Press. Kendall, Maurice George, and Alan Stuart. 1979. The Advanced Theory of Statistics. Vol. 2: Inference and Relationship. 4th ed. London: Griffin. See section 6.4.1.1 of Bolker to see how this follows from a result about the likelihood ratio.↩︎ "],["bayesian-computation.html", "Chapter 3 Bayesian computation 3.1 Computations with conjugate priors 3.2 MCMC and stochastic approximations of the posterior 3.3 rstanarm", " Chapter 3 Bayesian computation This chapter of the computing companion will focus solely on the computing aspects of Bayesian computation in R. See the relevant sections of Bolker for the underlying theory. The landscape of computing tools available to fit Bayesian models is fluid. Here, we will look at three tools currently available: R2jags, which is based on the JAGS (Just Another Gibbs Sampler) platform, rstan, which is based on the computer program Stan (itself based on Hamiltonian Monte Carlo, or HMC), and the recent rstanarm, which seeks to put much of the computational details in the background. (The “arm” portion of the name rstanarm is an acronym for applied regression modeling.) Throughout, we will be working with two data sets: the horse-kick data (again), and a data set that details how the rate at which a cricket chirps depends on the air temperature. The horse-kick data are useful in this context because a Gamma distribution is a conjugate prior for Poisson data. Thus, if we use a Gamma prior, then we know the posterior exactly. Therefore, we can compare the approximations provided by stochastic sampling schemes to the known posterior. The cricket data set will be used as an example of a simple linear regression, even though the data hint that the actual relationship between temperature and the rate of chirping is nonlinear. 3.1 Computations with conjugate priors Suppose that we observe an iid random sample \\(X_1, \\ldots, X_n\\) from a Poisson distribution with unknown parameter \\(\\lambda\\). (This is the setting for the horse-kick data.) If we place a Gamma prior with shape parameter \\(a\\) and rate parameter \\(r\\) on \\(\\lambda\\), then the posterior distribution is also Gamma with shape parameter \\(a + \\sum_n X_n\\) and rate parameter \\(r + n\\). In other words, \\[\\begin{align*} \\lambda &amp; \\sim \\mbox{Gamma}(a, r) \\\\ X_1, \\ldots, X_n &amp; \\sim \\mbox{Pois}(\\lambda) \\\\ \\lambda | X_1, \\ldots, X_n &amp; \\sim \\mbox{Gamma}(a + \\sum_n X_n, r + n) \\\\ \\end{align*}\\] In the horse-kick data, \\(\\sum_n x_n = 196\\) and \\(n = 280\\). Suppose we start with the vague Gamma prior \\(a=1\\), \\(r = 1\\) on \\(\\lambda\\). This prior has mean \\(a/r = 1\\) and variance \\(a/r^2 = 1\\).5 The posterior distribution for \\(\\lambda\\) is then a Gamma with shape parameter \\(a = 197\\) and rate parameter \\(281\\). We can plot it: horse &lt;- read.table(&quot;data/horse.txt&quot;, header = TRUE, stringsAsFactors = TRUE) l.vals &lt;- seq(from = 0, to = 2, length = 200) plot(l.vals, dgamma(l.vals, shape = 197, rate = 281), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;&quot;) lines(l.vals, dgamma(l.vals, shape = 1, rate = 1), lty = &quot;dashed&quot;) abline(v = 0.7, col = &quot;red&quot;) legend(&quot;topleft&quot;, leg = c(&quot;prior&quot;, &quot;posterior&quot;), lty = c(&quot;dashed&quot;, &quot;solid&quot;)) The red line shows the MLE, which is displaced slightly from the posterior mode. As a point estimate, we might consider any of the following. The posterior mean can be found exactly as \\(a/r\\) = 0.70001. Alternatively, we might consider the posterior median qgamma(0.5, shape = 197, rate = 281) ## [1] 0.6998817 Finally, we might conisder the posterior mode: optimize(f = function(x) dgamma(x, shape = 197, rate = 281), interval = c(0.5, 1), maximum = TRUE) ## $maximum ## [1] 0.6975082 ## ## $objective ## [1] 8.003938 To find a 95% confidence interval, we might consider the central 95% interval: qgamma(c(0.025, 0.975), shape = 197, rate = 281) ## [1] 0.6065824 0.8022917 A 95% highest posterior density (HPD) interval takes a bit more work. We’ll write a function to compute the width of a 95% credible interval based on the quantile of the upper endpoint, and then find the quantile that minimizes this width. interval.width &lt;- function(x){ upper &lt;- qgamma(p = x, shape = 197, rate = 281) lower &lt;- qgamma(p = x - .95, shape = 197, rate = 281) upper - lower } (upper.qtile &lt;- optimize(interval.width, interval = c(0.95, 1))$minimum) ## [1] 0.9722365 (hpd.ci &lt;- qgamma(p = c(upper.qtile - .95, upper.qtile), shape = 197, rate = 281)) ## [1] 0.6043347 0.7998231 We might also ask questions like: What is the posterior probability that \\(\\lambda &gt; 2/3\\)? These caluclations are straightforward in a Bayesian context, and they make full sense. pgamma(2/3, shape = 197, rate = 281, lower.tail = FALSE) ## [1] 0.7506288 Thus we would say that there is a 0.751 posterior probability that \\(\\lambda &gt; 2/3\\). As an illustration, note that if we had begun with a more informative prior — say, a gamma distribution with shape parameter \\(a = 50\\) and rate parameter = \\(100\\) — then the posterior would have been more of a compromise between the prior and the information in the data: plot(l.vals, dgamma(l.vals, shape = 196 + 50, rate = 100 + 280), type = &quot;l&quot;, xlab = expression(lambda), ylab = &quot;&quot;) lines(l.vals, dgamma(l.vals, shape = 50, rate = 100), lty = &quot;dashed&quot;) abline(v = 0.7, col = &quot;red&quot;) legend(&quot;topleft&quot;, leg = c(&quot;prior&quot;, &quot;posterior&quot;), lty = c(&quot;dashed&quot;, &quot;solid&quot;)) 3.2 MCMC and stochastic approximations of the posterior In most settings, a full conjugate prior is not available, and determining the posterior analytically is hard. In these situations, the contemporary approach is to approximate the posterior with a pseudo-random sample. While there are several methods for generating a pseudo-random sample from a posterior distribution, the most common approaches are based on Markov chain Monte Carlo (MCMC) sampling. There are multiple tools available for generating an MCMC sample from a posterior distribution, and software in this area changes rapidly. In this computing companion, we will use JAGS to approximate posterior distributions. JAGS is stand-alone software, but we can access JAGS from R using the r2jags package. This computing companion will largely use the default settings for the routines in r2jags, though in real practice the analyst will often have to do considerable work adjusting the settings to obtain a satisfactory approximation. 3.2.1 The horse-kick data, once more Here is JAGS code to approximate the posterior to \\(\\lambda\\) for the horse-kick data, using the vague prior. require(R2jags) ## Loading required package: R2jags ## Loading required package: rjags ## Loading required package: coda ## Linked to JAGS 4.3.1 ## Loaded modules: basemod,bugs ## ## Attaching package: &#39;R2jags&#39; ## The following object is masked from &#39;package:coda&#39;: ## ## traceplot horse.model &lt;- function() { for (j in 1:J) { # J = 280, number of data points y[j] ~ dpois (lambda) # data model: the likelihood } lambda ~ dgamma (1, 1) # prior # note that BUGS / JAGS parameterizes # gamma by shape, rate } jags.data &lt;- list(y = horse$deaths, J = length(horse$deaths)) jags.params &lt;- c(&quot;lambda&quot;) jags.inits &lt;- function(){ list(&quot;lambda&quot; = rgamma(1, 1, 1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = horse.model, n.chains = 3, n.iter = 5000, jags.seed = 1) ## module glm loaded ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 280 ## Unobserved stochastic nodes: 1 ## Total graph size: 283 ## ## Initializing model Let’s take a look at some summary statistics of the fit print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpILMLsx/model69485954274c&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved. Running time = 0.22 secs ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## lambda 0.700 0.051 0.604 0.665 0.698 0.733 0.805 1.001 3800 ## deviance 629.337 1.501 628.310 628.418 628.774 629.674 633.357 1.001 3800 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule: pV = var(deviance)/2) ## pV = 1.1 and DIC = 630.5 ## DIC is an estimate of expected predictive error (lower deviance is better). The Rhat values suggest that our chains have converged, as we might hope for such a simple model. We can generate a trace plot using traceplot to inspect convergence visually, but beware that visual assessment of convergence is prone to error. For an rjags object, the raw MCMC samples are stored in BUGSoutput$sims.list. Sometimes it is helpful to analyze these samples directly. For example, with these samples we can estimate other posterior quantities, such as the posterior median of \\(\\lambda\\), or generate a 95% central posterior confidence interval directly: mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) summary(mcmc.output) ## deviance lambda ## Min. :628.3 Min. :0.4987 ## 1st Qu.:628.4 1st Qu.:0.6653 ## Median :628.8 Median :0.6978 ## Mean :629.3 Mean :0.6996 ## 3rd Qu.:629.7 3rd Qu.:0.7326 ## Max. :648.5 Max. :0.9292 median(mcmc.output$lambda) ## [1] 0.6978272 quantile(mcmc.output$lambda, c(.025, .975)) ## 2.5% 97.5% ## 0.6044362 0.8050616 We can also use the lattice package to construct smoothed estimates of the posterior density: require(lattice) ## Loading required package: lattice jagsfit.mcmc &lt;- as.mcmc(jagsfit) densityplot(jagsfit.mcmc) 3.2.2 A simple regression example For a more involved example, let’s take a look at the simple regression fit to the cricket data. First, we’ll make a plot of the data and fit a SLR model by least squares. cricket &lt;- read.table(&quot;data/cricket.txt&quot;, header = TRUE) cricket.slr &lt;- lm(chirps ~ temperature, data = cricket) summary(cricket.slr) ## ## Call: ## lm(formula = chirps ~ temperature, data = cricket) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.56009 -0.57930 0.03129 0.59020 1.53259 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.30914 3.10858 -0.099 0.922299 ## temperature 0.21193 0.03871 5.475 0.000107 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9715 on 13 degrees of freedom ## Multiple R-squared: 0.6975, Adjusted R-squared: 0.6742 ## F-statistic: 29.97 on 1 and 13 DF, p-value: 0.0001067 plot(chirps ~ temperature, data = cricket) abline(cricket.slr) Now we’ll fit the same model in JAGS, using vague priors for all model parameters cricket.model &lt;- function() { for (j in 1:J) { # J = number of data points y[j] ~ dnorm (mu[j], tau) # data model: the likelihood # note that BUGS / JAGS uses precision # instead of variance mu[j] &lt;- b0 + b1 * x[j] # compute the mean for each observation } b0 ~ dnorm (0.0, 1E-6) # prior for intercept b1 ~ dnorm (0.0, 1E-6) # prior for slope tau ~ dgamma (1, 1) # prior for tau # note that BUGS / JAGS parameterizes # gamma by shape, rate sigma &lt;- pow(tau, -1/2) # the SD of the residaul errors } jags.data &lt;- list(y = cricket$chirps, x = cricket$temperature, J = nrow(cricket)) jags.params &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;sigma&quot;) jags.inits &lt;- function(){ list(&quot;b0&quot; = rnorm(1), &quot;b1&quot; = rnorm(1), &quot;tau&quot; = rgamma(1, 1, 1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = cricket.model, n.chains = 3, n.iter = 5000, jags.seed = 2) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 3 ## Total graph size: 69 ## ## Initializing model print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpILMLsx/model694855c6176a&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved. Running time = 0.06 secs ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## b0 -0.364 3.355 -6.898 -2.606 -0.266 1.816 6.215 1.002 2200 ## b1 0.213 0.042 0.131 0.185 0.211 0.240 0.294 1.001 2300 ## sigma 1.024 0.208 0.715 0.879 0.989 1.137 1.532 1.001 3800 ## tau 1.064 0.396 0.426 0.774 1.022 1.294 1.958 1.001 3800 ## deviance 42.759 2.560 39.783 40.830 42.104 43.966 49.262 1.002 1700 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule: pV = var(deviance)/2) ## pV = 3.3 and DIC = 46.0 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) The output of the print function gives the quantiles that one would use to calculate a 95% central credible interval. To find a HPD credible interval, we can use the HPDinterval function in the coda library. The coda library contains a variety of routines for post-processing of MCMC ouput. If we simply pass the jagsfit object to the HPDinterval function, it will return an HPD interval for each of the three chains. This isn’t what we want, so we’ll extract the raw MCMC samples first, and then coerce them to a data frame. mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) summary(mcmc.output) ## b0 b1 deviance sigma ## Min. :-14.4132 Min. :0.04201 Min. :39.59 Min. :0.5615 ## 1st Qu.: -2.6055 1st Qu.:0.18535 1st Qu.:40.83 1st Qu.:0.8790 ## Median : -0.2659 Median :0.21149 Median :42.10 Median :0.9893 ## Mean : -0.3639 Mean :0.21258 Mean :42.76 Mean :1.0237 ## 3rd Qu.: 1.8161 3rd Qu.:0.24039 3rd Qu.:43.97 3rd Qu.:1.1365 ## Max. : 13.9313 Max. :0.38485 Max. :61.71 Max. :2.2897 ## tau ## Min. :0.1907 ## 1st Qu.:0.7742 ## Median :1.0218 ## Mean :1.0645 ## 3rd Qu.:1.2944 ## Max. :3.1717 Now we’ll coerce the data frame mcmc.output to an MCMC object, and pass it to HPDinterval: HPDinterval(as.mcmc(mcmc.output)) ## lower upper ## b0 -7.0067834 6.0090943 ## b1 0.1344315 0.2962395 ## deviance 39.6127877 47.8373679 ## sigma 0.6792637 1.4375339 ## tau 0.3480057 1.8413064 ## attr(,&quot;Probability&quot;) ## [1] 0.9498667 One of the merits of the Bayesian approach is that the posterior samples provide an immediate tool for propagating uncertainty to (possibly derived) quantities of interest. We can summarize the uncertainty in the regression fit graphically by randomly sampling a subset of these samples (say, 100 of them) and using them to plot a collection of regression lines: plot(chirps ~ temperature, data = cricket, type = &quot;n&quot;) # we&#39;ll add the points later so that they lie on top of the lines, # instead of the other way around subset.samples &lt;- sample(nrow(mcmc.output), size = 100) for(i in subset.samples) { with(mcmc.output, abline(a = b0[i], b = b1[i], col = &quot;deepskyblue&quot;, lwd = 0.25)) } with(cricket, points(chirps ~ temperature)) with(mcmc.output, abline(a = mean(b0), b = mean(b1), col = &quot;purple3&quot;, lwd = 3)) We can also propagate the uncertainty to estimate, say, the posterior distribution for the value of the regression line when the temperature is 85 F. This quantifies the uncertainty in the average number of chirps at this temperature. (We can think of it as a vertical slice through the above plot.) avg.chirps.85 &lt;- with(mcmc.output, b0 + b1 * 85) summary(avg.chirps.85) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.51 17.48 17.70 17.71 17.92 19.20 quantile(avg.chirps.85, probs = c(.025, 0.975)) ## 2.5% 97.5% ## 17.04676 18.40220 We could use the density function to get a quick idea of the shape of the distribution: plot(density(avg.chirps.85)) Thus, we might say that the posterior mean for the average number of chirps at 85 F is 17.71, and a central 95% credible interval is (17.05, 18.4). Finally, we can use the posterior samples to estimate the uncertainty in a future observation. When we use a posterior distribution to estimate the distribution of a future observation, we refer to it as a posterior predictive distribution. The posterior predictive distribution must also include the error around the regression line. We can estimate the posterior predictive distribution as follows. Suppose we denote sample \\(i\\) from the posterior as \\(\\beta_{0, i}\\), \\(\\beta_{1, i}\\), and \\(\\sigma_i\\). Then for each posterior sample we will generate a new hypothetical observation \\(y_i^\\star\\) by sampling from a Gaussian distribution with mean equal to ${0,i} + {1,i} x $ and standard deviation \\(\\sigma_i\\), where \\(x = 85\\). The distribution of the \\(y_i^*\\)’s then gives the posterior predictive distribution that we seek. n.sims &lt;- nrow(mcmc.output) new.errors &lt;- with(mcmc.output, rnorm(n.sims, mean = 0, sd = sigma)) new.chirps.85 &lt;- with(mcmc.output, b0 + b1 * 85) + new.errors plot(density(new.chirps.85)) summary(new.chirps.85) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 13.32 17.04 17.74 17.73 18.44 23.13 quantile(new.chirps.85, probs = c(.025, 0.975)) ## 2.5% 97.5% ## 15.57821 19.86768 Thus, the posterior predictive distribution has a central 95% credible interval of (15.58, 19.87). Although it hasn’t caused any difficulty here, the slope and intercept are strongly negatively correlated in the posterior. We can visualize this posterior correlation: library(hexbin) library(RColorBrewer) rf &lt;- colorRampPalette(rev(brewer.pal(11, &#39;Spectral&#39;))) with(jagsfit$BUGSoutput$sims.list, hexbinplot(b1 ~ b0, colramp = rf)) We can estimate the posterior correlation between the intercept and the slope by accessing the raw MCMC samples cor(mcmc.output[, -c(3:4)]) ## b0 b1 tau ## b0 1.00000000 -0.99677020 0.01625532 ## b1 -0.99677020 1.00000000 -0.01710368 ## tau 0.01625532 -0.01710368 1.00000000 Thus we estimate that the intercept and slope have a posterior correlation of -0.997. We could make life easier on ourselves by centering the predictor and trying again: cricket$temp.ctr &lt;- cricket$temperature - mean(cricket$temperature) jags.data &lt;- list(y = cricket$chirps, x = cricket$temp.ctr, J = nrow(cricket)) jags.params &lt;- c(&quot;b0&quot;, &quot;b1&quot;, &quot;tau&quot;, &quot;sigma&quot;) jags.inits &lt;- function(){ list(&quot;b0&quot; = rnorm(1), &quot;b1&quot; = rnorm(1), &quot;tau&quot; = runif(1)) } jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = cricket.model, n.chains = 3, n.iter = 5000, jags.seed = 3) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 15 ## Unobserved stochastic nodes: 3 ## Total graph size: 69 ## ## Initializing model print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/RtmpILMLsx/model6948202cb1c&quot;, fit using jags, ## 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2 ## n.sims = 3750 iterations saved. Running time = 0.05 secs ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat n.eff ## b0 16.657 0.273 16.122 16.483 16.663 16.829 17.178 1.001 3800 ## b1 0.212 0.042 0.127 0.185 0.212 0.238 0.295 1.001 3800 ## sigma 1.027 0.207 0.721 0.887 0.994 1.132 1.525 1.001 3800 ## tau 1.054 0.385 0.430 0.780 1.012 1.272 1.925 1.001 3800 ## deviance 42.799 2.606 39.804 40.894 42.107 44.035 49.481 1.001 3800 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule: pV = var(deviance)/2) ## pV = 3.4 and DIC = 46.2 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) The posteriors for the intercept and slope are now uncorrelated: library(hexbin) library(RColorBrewer) rf &lt;- colorRampPalette(rev(brewer.pal(11, &#39;Spectral&#39;))) with(jagsfit$BUGSoutput$sims.list, hexbinplot(b1 ~ b0, colramp = rf)) mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) cor(mcmc.output[, -c(3:4)]) ## b0 b1 tau ## b0 1.000000000 -0.007996094 -0.012841996 ## b1 -0.007996094 1.000000000 -0.002818565 ## tau -0.012841996 -0.002818565 1.000000000 3.3 rstanarm The rstanarm package is a recent set of routines that seeks to provide a user-friendly front end to Bayesian analysis with Stan. Specifically, rstanarm provides functions for fitting standard statistical models that are meant to mimic the analogous fitting functions in R. For example, the basic routine for fitting linear models in R is lm; rstanarm provides a function stan_lm that strives to have the same functionality and interface as lm, albeit using Stan “under the hood” to generate Bayesian inference. (That said, the main workhorse function in rstanarm for model fitting is stan_glm, which attempts to mimic the native R function glm for fitting generalized linear models. Separately, the developers of rstanarm have taken the not unreasonable stance that generalized linear models should supplant general linear models as the analyst’s default approach to model fitting.) To provide functionality that is similar to R’s native model-fitting routines, the functions in rstanarm make a number of operational decisions behind the scenes. Most notably, the model fitting routines in rstanarm will select default priors and default HMC parameters. While these defaults can always be modified by the analyst, the implementation of software that chooses priors by default is radical. First, the developers of rstanarm have their own particular view about what the role of the prior should be in data analysis. While their view is a considered one, by no means does it reflect a consensus that extends beyond the developers of the software. If you use rstanarm’s routines out of the box, you are accepting this view as your own if you do not specify the priors yourself. Second, as best I understand, the methods by which rstanarm chooses default priors still appear to be in some flux. That means that future versions of rstanarm may supply different default priors than those that are supplied today. As a result, the behavior of rstanarm today may differ from its behavior tomorrow, if you use the default priors. All that said, here is how you might use rstanarm to fit the simple regression to the cricket data: require(rstanarm) ## Loading required package: rstanarm ## Loading required package: Rcpp ## This is rstanarm version 2.32.1 ## - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors! ## - Default priors may change, so it&#39;s safest to specify priors, even if equivalent to the defaults. ## - For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()) stanarm.cricket.fit &lt;- stan_glm(chirps ~ temp.ctr, data = cricket, family = gaussian, seed = 1) ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 6.7e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.67 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.039 seconds (Warm-up) ## Chain 1: 0.077 seconds (Sampling) ## Chain 1: 0.116 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 2.8e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.065 seconds (Warm-up) ## Chain 2: 0.049 seconds (Sampling) ## Chain 2: 0.114 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.4e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.046 seconds (Warm-up) ## Chain 3: 0.045 seconds (Sampling) ## Chain 3: 0.091 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.1e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.044 seconds (Warm-up) ## Chain 4: 0.045 seconds (Sampling) ## Chain 4: 0.089 seconds (Total) ## Chain 4: We can discover the priors that stan_glm has selected by using the function prior_summary. prior_summary(stanarm.cricket.fit) ## Priors for model &#39;stanarm.cricket.fit&#39; ## ------ ## Intercept (after predictors centered) ## Specified prior: ## ~ normal(location = 17, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 17, scale = 4.3) ## ## Coefficients ## Specified prior: ## ~ normal(location = 0, scale = 2.5) ## Adjusted prior: ## ~ normal(location = 0, scale = 0.63) ## ## Auxiliary (sigma) ## Specified prior: ## ~ exponential(rate = 1) ## Adjusted prior: ## ~ exponential(rate = 0.59) ## ------ ## See help(&#39;prior_summary.stanreg&#39;) for more details The rstanarm package has made a variety of decisions about how many chains to run, how long to run them, etc. We can obtain a summary of the model fit by the print command: print(stanarm.cricket.fit, digits = 3) ## stan_glm ## family: gaussian [identity] ## formula: chirps ~ temp.ctr ## observations: 15 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) 16.648 0.257 ## temp.ctr 0.210 0.041 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 1.008 0.198 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg There are a few parts of this output that deserve comment. First, this summary reports the posterior median of the parameters instead of the posterior mean. Second, the authors of rstanarm have made the curious decision to replace the posterior standard deviation (itself the Bayesian counterpart to the frequentist’s standard error) with someting they call “MAD SD”. This takes a bit of explanation. The “MAD” part stands for median absolute deviation. It is the median of the absolute deviations of the posterior samples from the posterior median. In other words, if we have a generic parameter \\(\\theta\\) and label its posterior samples as \\(\\theta_1, \\theta_2, \\ldots, \\theta_n\\), then the MAD of \\(\\theta\\) is \\[ \\mathrm{median}_i(| \\theta_i - \\mathrm{median}_i(\\theta_i) |) \\] According to the authors of rstanarm, “Because we are so used to working with standard deviations, when we compute the median absolute deviation, we then rescale it by multiplying by 1.483, which reproduces the standard deviation in the special case of the normal distribution. We call this the mad sd.” In other words, the MAD SD is a measure of posterior uncertainty that is meant to be comparable to the posterior standard deviation. (The authors of rstanarm clearly must think this is a more desirable estimate of the posterior uncertainty than the posterior standard deviation; though their reasoning here is not immediately clear to me.) If we want to compute our own summary statistics, we can extract the MCMC samples from the stam_glm fit using the as.matrix command: mcmc.sims &lt;- as.matrix(stanarm.cricket.fit) summary(mcmc.sims) ## (Intercept) temp.ctr sigma ## Min. :15.65 Min. :0.02782 Min. :0.5582 ## 1st Qu.:16.48 1st Qu.:0.18191 1st Qu.:0.8869 ## Median :16.65 Median :0.21023 Median :1.0079 ## Mean :16.65 Mean :0.20955 Mean :1.0457 ## 3rd Qu.:16.83 3rd Qu.:0.23740 3rd Qu.:1.1627 ## Max. :17.68 Max. :0.36177 Max. :2.3400 We might, for example, then use this output to find the posterior standard deviation of each of the parameters, or to find central 95% credible intervals: apply(mcmc.sims, 2, sd) ## (Intercept) temp.ctr sigma ## 0.27003507 0.04262539 0.22262375 apply(mcmc.sims, 2, function(x) quantile(x, c(0.025, 0.975))) ## parameters ## (Intercept) temp.ctr sigma ## 2.5% 16.10111 0.1240071 0.7171401 ## 97.5% 17.17942 0.2906848 1.5816217 Compare these values to the posterior standard deviations and 95% central credible intervals reported in the JAGS fit. Actually, a Gamma distribution with shape and rate parameters equal to 1 is the same as an exponential distribution with rate 1. Indeed, any Gamma distribution with shape parameter equal to 1 and rate parameter equal to \\(r\\) is the same as an exponential distribution with rate parameter equal to \\(r\\).↩︎ "],["smoothing-and-gams.html", "Chapter 4 Smoothing and GAMs 4.1 Loess smoothers 4.2 Splines 4.3 Generalized additive models (GAMs)", " Chapter 4 Smoothing and GAMs In this chapter, we examine methods for fitting a flexible trend between a response and a predictor that don’t require first committing ourselves to a particular mathematical assumption about the shape of that trend. In the first sections, we will explore fitting a flexible trend line for a single predictor and a single response. Later, we will see how these flexible trends can be included in regression models with several predictors. As a running example, we will consider a data set originally published by Gillibrand et al. (2007), and analyzed extensively in the textbook by Zuur et al. (2009). Zuur et al. say that these data describe the number of sources of “pelagic bioluminescence along a depth gradient in the northeast Atlantic Ocean.” The name of the data set (“ISIT”) refers to the type of camera used in the study. We focus particularly on the data at station 19. The pattern that we wish to characterize is shown below. ## download the data from the book&#39;s website isit &lt;- read.table(&quot;data/ISIT.txt&quot;, head = T) ## extract the data from station 19 st19 &lt;- subset(isit, Station == 19) ## retain just the variables that we want, and rename st19 &lt;- st19[, c(&quot;SampleDepth&quot;, &quot;Sources&quot;)] names(st19) &lt;- c(&quot;depth&quot;, &quot;sources&quot;) with(st19, plot(sources ~ depth)) As a preliminary consideration, all of the methods we discuss are variations on regression. As such, they inherent all of the usual regression assumptions about the distribution of the errors, namely, that the errors are iid draws from a Gaussian distribution. The bioluminescence data discussed in this chapter violate these assumptions rather severely. We see right away that, as typically happens when measuring the abundance of a particular organism or taxa, larger responses are more variable. To cope with this non-constant variance, we will take the usual step of modeling the (natural) log of the response, shown below. with(st19, plot(log(sources) ~ depth)) The log transformation successfully stabilizes the variance. In addition, because these data are collected at locations along a transect, they are likely characterized by substantial autocorrelation. For the sake of illustration, we ignore the autocorrelation in the analyses that follow. See the discussion of GAMMs to learn about coping with autocorrelation in generalized additive models. 4.1 Loess smoothers We illustrate a LOESS smoother first, using the loess function. We will first fit the smoother with the factory settings. st19.lo &lt;- loess(log(sources) ~ depth, data = st19) summary(st19.lo) ## Call: ## loess(formula = log(sources) ~ depth, data = st19) ## ## Number of Observations: 49 ## Equivalent Number of Parameters: 4.67 ## Residual Standard Error: 0.2054 ## Trace of smoother matrix: 5.12 (exact) ## ## Control settings: ## span : 0.75 ## degree : 2 ## family : gaussian ## surface : interpolate cell = 0.2 ## normalize: TRUE ## parametric: FALSE ## drop.square: FALSE Plot the fit, this takes a little work depth.vals &lt;- with(st19, seq(from = min(depth), to = max(depth), length = 100)) st19.fit &lt;- predict(object = st19.lo, newdata = depth.vals, se = TRUE) with(st19, plot(log(sources) ~ depth)) lines(x = depth.vals, y = st19.fit$fit, col = &quot;blue&quot;) # add 95% error bars lines(x = depth.vals, y = st19.fit$fit + st19.fit$se.fit * qt(p = .975, df = st19.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st19.fit$fit - st19.fit$se.fit * qt(p = .975, df = st19.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) Examine the residuals: plot(st19.lo$residuals ~ st19$depth) abline(h = 0, lty = &quot;dotted&quot;) Note the autocorrelation in the residuals. Let’s look at how changing the span changes the fit. We’ll write a custom function to fit a LOESS curve, and then call the function with various values for the span. PlotLoessFit &lt;- function(x, y, return.fit = FALSE, ...){ # Caluclates a loess fit with the &#39;loess&#39; function, and makes a plot # # Args: # x: predictor # y: response # return.fit: logical # ...: Optional arguments to loess # # Returns: # the loess fit my.lo &lt;- loess(y ~ x, ...) x.vals &lt;- seq(from = min(x), to = max(x), length = 100) my.fit &lt;- predict(object = my.lo, newdata = x.vals, se = TRUE) plot(x, y) lines(x = x.vals, y = my.fit$fit, col = &quot;blue&quot;) lines(x = x.vals, y = my.fit$fit + my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = x.vals, y = my.fit$fit - my.fit$se.fit * qt(p = .975, df = my.fit$df), col = &quot;blue&quot;, lty = &quot;dashed&quot;) if (return.fit) { return(my.lo) } } Now we’ll call the function several times, each time chanigng the value of the span argument to the loess function: PlotLoessFit(x = st19$depth, y = log(st19$sources), span = 0.6) PlotLoessFit(x = st19$depth, y = log(st19$sources), span = 0.4) PlotLoessFit(x = st19$depth, y = log(st19$sources), span = 0.2) Notice how the fit becomes rougher as the span decreases. The loess function includes an argument degree that specifies the degree of the polynomial regression fit at each location. The default value for degree is 2, so the fits that we have seen so far are local quadratic regressions. Let’s try a loess fit with a locally linear regression: PlotLoessFit(x = st19$depth, y = log(st19$sources), span = 0.4, degree = 1) 4.2 Splines Next we consider splines. There are many types of splines, and, correspondingly, there are many R tools for fitting splines. Most ecologists use the mgcv package to fit splines (and GAMs that include splines). While mgcv is a remarkable package, it’s a complicated piece of machinery. I’ve found the splines library to be more accessible to the non-expert, at least for regression splines. The downside to the splines library is that it doesn’t fit smoothing splines. The discussion in this section draws from Chapter 7 of James et al. (2021). 4.2.1 Regression splines The code below fits a regression spline to the bioluminescence data. Actually, the code uses the command bs (no bull) to create the basis functions for a cubic regression spline, which are then treated as predictors in a standard regression fit with lm. For expository purposes, we will construct a spline with two knots, arbitrarily placed at 1000 m and 2000 m depth. library(splines) fm1 &lt;- lm(log(sources) ~ bs(depth, knots = c(1000, 2000)), data = st19) # plot using predict.lm; code adapted from James et al (2021) depth.vals &lt;- seq(from = min(st19$depth), to = max(st19$depth), length = 100) fm1.pred &lt;- predict(fm1, newdata = list(depth = depth.vals), se = T) with(st19, plot(log(sources) ~ depth)) lines(depth.vals, fm1.pred$fit, lwd = 2, col = &quot;red&quot;) lines(depth.vals, fm1.pred$fit + qt(0.975, df = 43) * fm1.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) lines(depth.vals, fm1.pred$fit + qt(0.025, df = 43) * fm1.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) Just for fun, let’s fit a linear regression spline with a single knot at 1500m. We’ll use the degree argument of bs to specify a linear (as opposed to cubic) spline. fm2 &lt;- lm(log(sources) ~ bs(depth, knots = c(1500), degree = 1), data = st19) fm2.pred &lt;- predict(fm2, newdata = list(depth = depth.vals), se = T) with(st19, plot(log(sources) ~ depth)) lines(depth.vals, fm2.pred$fit, lwd = 2, col = &quot;red&quot;) lines(depth.vals, fm2.pred$fit + qt(0.975, df = 43) * fm2.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) lines(depth.vals, fm2.pred$fit + qt(0.025, df = 43) * fm2.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) Alternatively, instead of using the knots argument to specify the knots, we can use the df argument to specify the number of basis functions. For a cubic regression spline, the number of knots will then be the value of df minus 4 (because there are four basis functions, or degrees of freedom, needed for a cubic regression without any knots). According to the documentation for bs, the knots are placed at “suitable quantiles of \\(x\\)”. Here is code for a (cubic) regression spline with 4 knots (and thus df = 8). fm3 &lt;- lm(log(sources) ~ bs(depth, df = 8), data = st19) fm3.pred &lt;- predict(fm3, newdata = list(depth = depth.vals), se = T) with(st19, plot(log(sources) ~ depth)) lines(depth.vals, fm3.pred$fit, lwd = 2, col = &quot;red&quot;) lines(depth.vals, fm3.pred$fit + qt(0.975, df = 43) * fm3.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) lines(depth.vals, fm3.pred$fit + qt(0.025, df = 43) * fm3.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) Note that if we want to test for the significance of the regression spline, we can conduct an \\(F\\)-test in the usual way. fm0 &lt;- lm(log(sources) ~ 1, data = st19) anova(fm0, fm3) ## Analysis of Variance Table ## ## Model 1: log(sources) ~ 1 ## Model 2: log(sources) ~ bs(depth, df = 8) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 48 38.652 ## 2 40 0.914 8 37.739 206.54 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It can be a bit enlightening to see how bs creates basis functions. Here are the basis functions for the linear spline with a single knot at 1500m. linear.basis &lt;- bs(st19$depth, knots = c(1500), degree = 1) par(mfrow = c(1, 2)) for (i in 1:2) plot(st19$depth, linear.basis[, i], xlab = &quot;depth&quot;, ylab = &quot;basis function&quot;) And the basis function for the cubic regression spline with 6 df. cubic.basis &lt;- bs(st19$depth, df = 6) par(mfrow = c(2, 3)) for (i in 1:6) plot(st19$depth, cubic.basis[, i], xlab = &quot;depth&quot;, ylab = &quot;basis function&quot;) 4.2.2 Natural splines A (cubic) regression spline typically suffers from having too much flexibility in the regions outside the outermost knots. A natural spline is a regression spline that (a) places boundary knots at the minimun and maximum of the predictor values (in addition to any internal knots), and then (b) uses cubic fits for the segments between the knots, and linear fits for the segments outside the boundary knots. The effect of requiring a linear fit outside the boundary knots is to temper the flexibility of the spline near the extreme predictor values. In the splines library, the function ns generates basis functions for a natural spline. Here is an example of a natural spline with two internal knots, compared to a cubic regression spline with the same internal knots. fm4 &lt;- lm(log(sources) ~ ns(depth, knots = c(1000, 2000)), data = st19) fm4.pred &lt;- predict(fm4, newdata = list(depth = depth.vals), se = T) par(mfrow = c(1, 2)) with(st19, plot(log(sources) ~ depth, main = &quot;regression spline&quot;)) lines(depth.vals, fm1.pred$fit, lwd = 2, col = &quot;red&quot;) lines(depth.vals, fm1.pred$fit + qt(0.975, df = 43) * fm1.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) lines(depth.vals, fm1.pred$fit + qt(0.025, df = 43) * fm1.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) with(st19, plot(log(sources) ~ depth, main = &quot;natural spline&quot;)) lines(depth.vals, fm4.pred$fit, lwd = 2, col = &quot;red&quot;) lines(depth.vals, fm4.pred$fit + qt(0.975, df = 43) * fm4.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) lines(depth.vals, fm4.pred$fit + qt(0.025, df = 43) * fm4.pred$se, lty = &quot;dashed&quot;, col = &quot;red&quot;) 4.2.3 Smoothing splines In regression splines, the analyst chooses the degree of the spline (typically 3, for a cubic spline) and the number and location of the knots. In a smoothing spline, also known as a penalized regression spline, one begins with a large number of knots and then optimizes with respect to an objective function that is the sum of the residual sum of squares plus a penalty term for the roughness of the fit. The strength of the penalty is set by a tuning parameter that is typically determined by cross-validation. Most ecologists these days would use the gam function in the mgcv library. The code below illustrates. The mgcv::gam program is a complicated piece of machinery; see Wood (2017) for the necessary background theory. The bs argument specifies the type of basis function; the smooth shown below is based on a cubic regression (cr) basis. library(mgcv) ## Loading required package: nlme ## This is mgcv 1.9-3. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. st19.sspline &lt;- mgcv::gam(log(sources) ~ s(depth, bs = &quot;cr&quot;), data = st19) plot(st19.sspline, se = TRUE) The plot shows only the spline component, which thus does not include the intercept. To visualize the fit, we’ll need to do a bit more work. with(st19, plot(log(sources) ~ depth)) st19.fit &lt;- predict(st19.sspline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st19.fit$fit) Let’s ask for a summary: summary(st19.sspline) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## log(sources) ~ s(depth, bs = &quot;cr&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.83855 0.02226 127.5 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(depth) 8.541 8.928 173.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.97 Deviance explained = 97.5% ## GCV = 0.030161 Scale est. = 0.024288 n = 49 Note the edf component in the “Approximate significance of smooth terms” section. The label edf stands for effective degrees of freedom. We can think of the edf as the effective number of new predictors (basis functions) that have been added to the model to accommodate the spline. We can use the edf to determine the appropriate confidence bounds on our smooth. There are 49 data points in this data set, so the estimate of the residual error is based on 49 - (8.5 + 1) = 39.5 df. with(st19, plot(log(sources) ~ depth)) st19.fit &lt;- predict(st19.sspline, newdata = data.frame(depth = depth.vals), se = TRUE) lines(x = depth.vals, y = st19.fit$fit) lines(x = depth.vals, y = st19.fit$fit + qt(0.975, df = 39.5) * st19.fit$se.fit, lty = &quot;dashed&quot;) lines(x = depth.vals, y = st19.fit$fit + qt(0.025, df = 39.5) * st19.fit$se.fit, lty = &quot;dashed&quot;) The s() component of the model formula designates a spline, and specifies details about the particular type of spline to be fit. To fit a regression spline, fix the number of basis functions with the fx = TRUE argument. The default value for the fx argument is fx = FALSE, in which case the amount of smoothing is determined by (generalized) cross-validation. When fx = TRUE, the parameter k determines the dimensionality (degree of flexibility) of the spline. Larger values of k correspond to greater flexibility, and a less smooth fit. The AIC function will generate an AIC value for the penalized regression spline fit: AIC(st19.sspline) ## [1] -32.64326 4.3 Generalized additive models (GAMs) Generalized additive models replace the usual linear terms that appear in multiple regression models with splines. That is, suppose we seek to model the relationship between a response \\(y\\) and two predictors, \\(x_1\\) and \\(x_2\\). A standard regression model without polynomial effects or interactions would be written as \\[ y = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + \\varepsilon \\] where \\(\\varepsilon\\) is assumed to be an iid Gaussian random variate with variance \\(\\sigma^2_\\varepsilon\\). This is an additive model, in the sense that the combined effects of the two predictors equal the sum of their individual effects. A generalized additive model (GAM) replaces the individual regression terms with splines. Continuing with the generic example, a GAM would instead model the effects of the two predictors as \\[ y = \\beta_0 + s(x_1) +s(x_2) + \\varepsilon \\] where \\(s(\\cdot)\\) represents a spline. We continue to assume that, conditional on the covariate effects, the responses are normally distributed with constant variance \\(\\sigma^2_\\varepsilon\\). We will illustrate additive modeling using the bird data found in Appendix A of Zuur et al. (2009). Zuur et al. report that these data originally appeared in Loyn (1987) and were featured in Quinn &amp; Keough (2002)’s text. Zuur et al. describe these data in the following way: Forest bird densities were measured in 56 forest patches in south-eastern Victoria, Australia. The aim of the study was to relate bird densities to six habitat variables; size of the forest patch, distance to the nearest patch, distance to the nearest larger patch, mean altitude of the patch, year of isolation by clearing, and an index of stock grazing history (1 = light, 5 = intensive). We first read the data and perform some light exploratory analysis and housekeeping. rm(list = ls()) require(mgcv) bird &lt;- read.table(&quot;data/Loyn.txt&quot;, head = T) summary(bird) ## Site ABUND AREA DIST ## Min. : 1.00 Min. : 1.50 Min. : 0.10 Min. : 26.0 ## 1st Qu.:14.75 1st Qu.:12.40 1st Qu.: 2.00 1st Qu.: 93.0 ## Median :28.50 Median :21.05 Median : 7.50 Median : 234.0 ## Mean :28.50 Mean :19.51 Mean : 69.27 Mean : 240.4 ## 3rd Qu.:42.25 3rd Qu.:28.30 3rd Qu.: 29.75 3rd Qu.: 333.2 ## Max. :56.00 Max. :39.60 Max. :1771.00 Max. :1427.0 ## LDIST YR.ISOL GRAZE ALT ## Min. : 26.0 Min. :1890 Min. :1.000 Min. : 60.0 ## 1st Qu.: 158.2 1st Qu.:1928 1st Qu.:2.000 1st Qu.:120.0 ## Median : 338.5 Median :1962 Median :3.000 Median :140.0 ## Mean : 733.3 Mean :1950 Mean :2.982 Mean :146.2 ## 3rd Qu.: 913.8 3rd Qu.:1966 3rd Qu.:4.000 3rd Qu.:182.5 ## Max. :4426.0 Max. :1976 Max. :5.000 Max. :260.0 # get rid of the &#39;Site&#39; variable; it is redundant with the row label bird &lt;- bird[, -1] # log-transform area, distance, ldistance, to remove right-skew bird$L.AREA &lt;- log(bird$AREA) bird$L.DIST &lt;- log(bird$DIST) bird$L.LDIST &lt;- log(bird$LDIST) # change YR.ISOL to years since isolation (study was published in 1987) bird$YR.ISOL &lt;- 1987 - bird$YR.ISOL # keep the only the variables we want bird &lt;- bird[, c(&quot;ABUND&quot;, &quot;L.AREA&quot;, &quot;L.DIST&quot;, &quot;L.LDIST&quot;, &quot;YR.ISOL&quot;, &quot;ALT&quot;, &quot;GRAZE&quot;)] summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 ## Median :24.50 Median :140.0 Median :3.000 ## Mean :37.25 Mean :146.2 Mean :2.982 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 ## Max. :97.00 Max. :260.0 Max. :5.000 Our first attempt at a GAM will use penalized regression splines for all of the continuous predictors in the model. We will use a linear term for GRAZE because there are too few unique values to support a smooth term: bird.gam1 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + s(ALT), data = bird) summary(bird.gam1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + s(L.DIST) + s(L.LDIST) + s(YR.ISOL) + GRAZE + ## s(ALT) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.4443 2.7798 9.153 9.42e-12 *** ## GRAZE -1.9885 0.8968 -2.217 0.0318 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.446 3.089 12.635 3.98e-06 *** ## s(L.DIST) 3.693 4.559 0.855 0.461 ## s(L.LDIST) 1.000 1.000 0.386 0.538 ## s(YR.ISOL) 1.814 2.238 1.231 0.262 ## s(ALT) 1.000 1.000 0.629 0.432 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.72 Deviance explained = 77.6% ## GCV = 40.987 Scale est. = 32.238 n = 56 The output reports the partial regression coefficient for the lone quantitative predictor GRAZE, and approximate significance tests for the smooth terms for each of the other predictors. We can visualize these smooth terms with a call to plot: plot(bird.gam1) The usual theory of \\(F\\)-testing does not apply to smoothing splines, which is why the \\(F\\)-test shown for the penalized regression spline is labeled as “approximate”. The variable selection that follows is thus rather casual. We’ll drop smooth terms with large \\(p\\)-values to obtain: bird.gam2 &lt;- mgcv::gam(ABUND ~ s(L.AREA) + GRAZE, data = bird) summary(bird.gam2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + GRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.400 2.201 12.903 &lt; 2e-16 *** ## GRAZE -2.980 0.686 -4.344 6.56e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.284 2.903 13.18 3.4e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.68 Deviance explained = 69.9% ## GCV = 39.992 Scale est. = 36.932 n = 56 plot(bird.gam2) Note that the GRAZE variable is currently treated as a numerical predictor. We’ll try fitting a model with GRAZE as a factor. First we’ll create a new variable that treats GRAZE as a factor. We’ll use the summary command to confirm that the new variable fGRAZE is indeed a factor. bird$fGRAZE &lt;- as.factor(bird$GRAZE) summary(bird) ## ABUND L.AREA L.DIST L.LDIST ## Min. : 1.50 Min. :-2.3026 Min. :3.258 Min. :3.258 ## 1st Qu.:12.40 1st Qu.: 0.6931 1st Qu.:4.533 1st Qu.:5.064 ## Median :21.05 Median : 2.0127 Median :5.455 Median :5.824 ## Mean :19.51 Mean : 2.1459 Mean :5.102 Mean :5.859 ## 3rd Qu.:28.30 3rd Qu.: 3.3919 3rd Qu.:5.809 3rd Qu.:6.816 ## Max. :39.60 Max. : 7.4793 Max. :7.263 Max. :8.395 ## YR.ISOL ALT GRAZE fGRAZE ## Min. :11.00 Min. : 60.0 Min. :1.000 1:13 ## 1st Qu.:21.00 1st Qu.:120.0 1st Qu.:2.000 2: 8 ## Median :24.50 Median :140.0 Median :3.000 3:15 ## Mean :37.25 Mean :146.2 Mean :2.982 4: 7 ## 3rd Qu.:59.50 3rd Qu.:182.5 3rd Qu.:4.000 5:13 ## Max. :97.00 Max. :260.0 Max. :5.000 Now we’ll proceed to fit the model bird.gam3 &lt;- gam(ABUND ~ s(L.AREA) + fGRAZE, data = bird) plot(bird.gam3) summary(bird.gam3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## ABUND ~ s(L.AREA) + fGRAZE ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.727275 1.944080 11.691 1.11e-15 *** ## fGRAZE2 0.006623 2.845343 0.002 0.998152 ## fGRAZE3 -0.660124 2.585878 -0.255 0.799592 ## fGRAZE4 -2.170994 3.050736 -0.712 0.480122 ## fGRAZE5 -11.913966 2.872911 -4.147 0.000136 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(L.AREA) 2.761 3.478 11.67 4.71e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.723 Deviance explained = 75.7% ## GCV = 37.013 Scale est. = 31.883 n = 56 To formally compare the models with GRAZE as a numerical vs. categorical predictor, we’ll have to use AIC. We can’t use an \\(F\\)-test here because we have used penalized regression splines to capture the effect of L.AREA. Thus, the models are not nested. (If we had used regression splines for L.AREA, then the models would have been nested.) We can extract the AICs for these models by a simple call to the AIC function. AIC(bird.gam2) ## [1] 367.1413 AIC(bird.gam3) ## [1] 361.9655 We can see the contrasts used to incorporate the factor fGRAZE in the model by a call to contrasts: with(bird, contrasts(fGRAZE)) ## 2 3 4 5 ## 1 0 0 0 0 ## 2 1 0 0 0 ## 3 0 1 0 0 ## 4 0 0 1 0 ## 5 0 0 0 1 The output here is somewhat opaque because the levels of fGRAZE are 1, 2, \\(\\ldots\\), 5. The output of the call to contrasts shows each of the newly created indicator variables as a column. For example, the first column shows that the predictor named fGRAZE2 takes the value of 1 when the variable fGRAZE equals 2, and is 0 otherwise. Fit an additive model with only a smooth effect of L.AREA, in order to show residuals vs. GRAZE: bird.gam4 &lt;- gam(ABUND ~ s(L.AREA), data = bird) plot(x = bird$GRAZE, y = bird.gam4$residuals) abline(h = 0, lty = &quot;dashed&quot;) Both the plot and the model output suggest that the effect of grazing is primarily due to lower bird abundance in the most heavily grazed category. To conclude, we’ll conduct a formal test of whether the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE. In this case, we have to use regression splines for the smooth effect of L.AREA. If we want to continue working in mgcv::gam, we need to fix the number of knots by setting fx = TRUE. If we fix the number of knots, then we need to specify the number of knots with the argument k. Here, the edf of the log area effect is about 3, so we will set k=4. bird.gam5 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE, data = bird) bird.gam6 &lt;- gam(ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE, data = bird) anova(bird.gam5, bird.gam6, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + GRAZE ## Model 2: ABUND ~ s(L.AREA, k = 4, fx = TRUE) + fGRAZE ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 51 1869.0 ## 2 48 1543.1 3 325.93 3.3796 0.02565 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Alternatively, we could use the spline basis from the splines library. bird.gam7 &lt;- lm(ABUND ~ bs(L.AREA, df = 3) + GRAZE, data = bird) bird.gam8 &lt;- lm(ABUND ~ bs(L.AREA, df = 3) + fGRAZE, data = bird) anova(bird.gam7, bird.gam8, test = &quot;F&quot;) ## Analysis of Variance Table ## ## Model 1: ABUND ~ bs(L.AREA, df = 3) + GRAZE ## Model 2: ABUND ~ bs(L.AREA, df = 3) + fGRAZE ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 51 1877.2 ## 2 48 1561.0 3 316.23 3.2414 0.03003 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 I don’t know why the results differ slightly, but they don’t differ much. Both AIC and the \\(F\\)-test suggest that the model with GRAZE as a factor provides a significantly better fit than the model with a linear effect of GRAZE (\\(F_{3,48} = 3.24, p = 0.030\\)). As a final note, Zuur et al. (p.550) observe that “the non-linear L.AREA effect is mainly due to two large patches. It would be useful to sample more of this type of patch in the future.” (Note the rug plots in any of the plots of the area effect above.) Bibliography Gillibrand, EJV, AJ Jamieson, PM Bagley, Alain F Zuur, and IG Priede. 2007. “Seasonal Development of a Deep Pelagic Bioluminescent Layer in the Temperate NE Atlantic Ocean.” Marine Ecology Progress Series 341: 37–44. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. 2nd ed. Springer. Loyn, RH. 1987. “Effects of Patch Area and Habitat on Bird Abundances, Species Numbers and Tree Health in Fragmented Victorian Forests.” In Nature Conservation: The Role of Remnants of Native Vegetation, edited by A. A. Burbidge DA Saunders GW Arnold and AJM Hopkins, 65–77. Chipping Norton, NSW: Surrey Beatty &amp; Sons. Wood, Simon N. 2017. Generalized Additive Models: An Introduction with r. CRC press. Zuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with R. Vol. 574. Springer. "],["generalized-least-squares.html", "Chapter 5 Generalized Least Squares 5.1 Heterogeneous variance 5.2 Temporal (serial) correlation 5.3 Spatial data", " Chapter 5 Generalized Least Squares 5.1 Heterogeneous variance We will illustrate generalized least squares (GLS) using a data set that gives the percentage of male births for four countries (Canada, Denmark, the Netherlands, and the US) for several decades in the late twentieth century. The data were originally reported in Davis et al., JAMA 279:1018–1023 (1998). The data set that we will work with was scraped from this publication by Ramsey and Schafer for their book “The Statistical Sleuth” (2e, 2002). The data can be found as the data set ‘ex0726’ in the r library ‘sleuth2’. We will begin by reading the data and performing some housekeeping. #-------------------- # Ex 07.26 from the Statistical Sleuth, 2e #-------------------- library(Sleuth2) str(ex0726) ## &#39;data.frame&#39;: 45 obs. of 5 variables: ## $ Year : num 1950 1951 1952 1953 1954 ... ## $ Denmark : num 0.512 0.517 0.515 0.517 0.515 ... ## $ Netherlands: num 0.516 0.516 0.516 0.516 0.516 ... ## $ Canada : num NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... ## $ Usa : num NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... births &lt;- ex0726 require(reshape2) ## Loading required package: reshape2 names(births) &lt;- c(&quot;year&quot;, &quot;DK&quot;, &quot;NL&quot;, &quot;CA&quot;, &quot;US&quot;) births.melt &lt;- melt(births, id.vars = c(&quot;year&quot;)) births &lt;- births.melt rm(births.melt) names(births) &lt;- c(&quot;year&quot;, &quot;country&quot;, &quot;pct.male&quot;) births$pct.male &lt;- 100 * births$pct.male We will focus only on the years 1970 – 1990, for which data are available for all countries: births &lt;- subset(births, year &gt;= 1970 &amp; year &lt;= 1990) summary(births) ## year country pct.male ## Min. :1970 DK:21 Min. :50.87 ## 1st Qu.:1975 NL:21 1st Qu.:51.22 ## Median :1980 CA:21 Median :51.28 ## Mean :1980 US:21 Mean :51.30 ## 3rd Qu.:1985 3rd Qu.:51.38 ## Max. :1990 Max. :51.73 head(births) ## year country pct.male ## 21 1970 DK 51.40 ## 22 1971 DK 51.70 ## 23 1972 DK 51.26 ## 24 1973 DK 51.33 ## 25 1974 DK 51.27 ## 26 1975 DK 51.08 Let’s have a look at the time trends in percentage male births in each of the four countries: par(mfrow = c(2, 2), las = 1) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Canada&quot;)) with(subset(births, country == &quot;CA&quot;), points(pct.male ~ year)) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;USA&quot;)) with(subset(births, country == &quot;US&quot;), points(pct.male ~ year)) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Denmark&quot;)) with(subset(births, country == &quot;DK&quot;), points(pct.male ~ year)) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Netherlands&quot;)) with(subset(births, country == &quot;NL&quot;), points(pct.male ~ year)) For these data, we might want to ask: Is there evidence that the percentage of male births is changing through time? If so, does the rate of change differ among countries? Among continents? These are the types of questions that we would usually address with a regression model. However, there’s a lot going on with these data that would cause us to question the appropriateness of the usual ordinary least squares (OLS) assumptions. The responses are proportions. We know that when the response is a proportion, the variance in the response depends on the mean, with the variance decreasing as the mean approaches 0% or 100%, and obtaining its maximal value when the mean response is at 50%. For these data, however, all of the responses are sufficiently close to 50% that we don’t need to worry about heterogeneous variances that arise from the proportional nature of the response. The variance of the response also depends inversely on the number of births. Evidently, this will be a major issue, because these countries differ substantially in the sizes of their populations. If we knew the number of births in each country in each year (in other words, if we knew the denominator of the each data point), then could account for these differences using grouped logistic regression. However, the data as we have them do not contain any information about the number of births that underlie each data point. So, we will need a different approach to deal with the heterogeneous variances among countries. The data are time series. We will devote our attention to time-series data more fully later in the course. For now, it suffices to realize that time series data are typically autocorrelated. In other words, the residual errors for consecutive data points are often correlated (and usually positively correlated), and this correlation typically decays as the time between data points increases. For these data, it is less clear why the errors might be autocorrelated, but we want to allow for the possibility all the same. We’ll regress the percentage of male births on year and country. Following Zuur et al. (2009)’s good advice, we’ll begin with a model with richly specified fixed effects. In this case, that means country specific intercepts and slopes. In an equation, this model is \\[\\begin{equation} y_{it} = a_i + b_i x_{it} + \\varepsilon_{it} \\end{equation}\\] where \\(i = 1, \\ldots, 4\\) is an index that distinguishes among the four countries, and \\(t = 1, \\ldots, 21\\) is an index that distinguishes among the 21 years. The response \\(y_{it}\\) is the percentage of male births in country \\(i\\) in year \\(t\\), \\(x_{it}\\) is the year to which measurement \\(y_{it}\\) corresponds, the \\(a_i\\) are the country-specific intercepts, the \\(b_i\\) are the country-specific slopes, and the \\(\\varepsilon_{it}\\)’s are the errors. We’ll shift the predictor \\(x_{it}\\) so that it gives the number of years before or after 1980, so that the intercepts will give the fitted percentage of male births in the year 1980. (If we set \\(x_{it}\\) to the actual year, then the intercepts would give the percentage of male births extrapolated back to the year 1 BCE, which isn’t very useful.) To begin, we make the usual OLS assumption that the errors are iid, that is, \\(\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma^2)\\). births$yr.ctr &lt;- births$year - 1980 fm1 &lt;- with(births, lm(pct.male ~ yr.ctr * country)) summary(fm1) ## ## Call: ## lm(formula = pct.male ~ yr.ctr * country) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.30707 -0.05931 0.00100 0.04787 0.35314 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51.3709524 0.0275387 1865.408 &lt; 2e-16 *** ## yr.ctr 0.0008442 0.0045479 0.186 0.85324 ## countryNL -0.1547619 0.0389456 -3.974 0.00016 *** ## countryCA -0.0038095 0.0389456 -0.098 0.92234 ## countryUS -0.1109524 0.0389456 -2.849 0.00564 ** ## yr.ctr:countryNL -0.0073636 0.0064317 -1.145 0.25584 ## yr.ctr:countryCA -0.0119610 0.0064317 -1.860 0.06680 . ## yr.ctr:countryUS -0.0062727 0.0064317 -0.975 0.33251 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1262 on 76 degrees of freedom ## Multiple R-squared: 0.3052, Adjusted R-squared: 0.2412 ## F-statistic: 4.768 on 7 and 76 DF, p-value: 0.0001753 Let’s plot the percentage of male births vs. year for each country, and overlay the fit of regression lines. To make it easy to extract the country-specific slopes and intercepts, we’ll first re-fit the model without the global intercept: fm1a &lt;- with(births, lm(pct.male ~ country + yr.ctr:country- 1)) summary(fm1a) ## ## Call: ## lm(formula = pct.male ~ country + yr.ctr:country - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.30707 -0.05931 0.00100 0.04787 0.35314 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## countryDK 51.3709524 0.0275387 1865.408 &lt;2e-16 *** ## countryNL 51.2161905 0.0275387 1859.788 &lt;2e-16 *** ## countryCA 51.3671429 0.0275387 1865.270 &lt;2e-16 *** ## countryUS 51.2600000 0.0275387 1861.379 &lt;2e-16 *** ## countryDK:yr.ctr 0.0008442 0.0045479 0.186 0.8532 ## countryNL:yr.ctr -0.0065195 0.0045479 -1.434 0.1558 ## countryCA:yr.ctr -0.0111169 0.0045479 -2.444 0.0168 * ## countryUS:yr.ctr -0.0054286 0.0045479 -1.194 0.2363 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1262 on 76 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 1.735e+06 on 8 and 76 DF, p-value: &lt; 2.2e-16 There’s probably a more elegant way to extract the slope and intercept, but we’ll use the crude approach for now. par(mfrow = c(2, 2), las = 1) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Canada&quot;)) with(subset(births, country == &quot;CA&quot;), points(pct.male ~ year)) abline(a = 51.3671 - (-0.01112 * 1980), b = -0.01112) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;USA&quot;)) with(subset(births, country == &quot;US&quot;), points(pct.male ~ year)) abline(a = 51.26 - (-0.0054286 * 1980), b = -0.0054286) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Denmark&quot;)) with(subset(births, country == &quot;DK&quot;), points(pct.male ~ year)) abline(a = 51.3709 - (0.0008442 * 1980), b = 0.0008442) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Netherlands&quot;)) with(subset(births, country == &quot;NL&quot;), points(pct.male ~ year)) abline(a = 51.2162 - (-0.00652 * 1980), b = -0.0065195) We would like to draw inferences about the time and country effects. However, the error variance clearly differs among the countries, because of the different sizes of the countries’ populations. Thus, we can’t trust the usual inference procedures that assume iid errors. We will cope by fitting a GLS model that allows the error variances to differ among the countries. The model equation is nearly the same as above: \\[\\begin{equation} y_{it} = a_i + b_i x_{it} + \\varepsilon_{it}. \\end{equation}\\] The only difference is that now we assume that the variance of the errors differs among the countries: \\(\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma^2_i)\\). This change looks trivial in the notation, but it’s an important change to the model! require(nlme) ## Loading required package: nlme gls1 &lt;- gls(pct.male ~ yr.ctr * country, data = births, weights = varIdent(form = ~ 1 | country)) summary(gls1) ## Generalized least squares fit by REML ## Model: pct.male ~ yr.ctr * country ## Data: births ## AIC BIC logLik ## -94.69995 -66.73115 59.34998 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7264997 0.3971728 0.1347962 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.04219635 1217.4264 0.0000 ## yr.ctr 0.00084 0.00696850 0.1211 0.9039 ## countryNL -0.15476 0.05215650 -2.9673 0.0040 ## countryCA -0.00381 0.04540269 -0.0839 0.9334 ## countryUS -0.11095 0.04257798 -2.6059 0.0110 ## yr.ctr:countryNL -0.00736 0.00861336 -0.8549 0.3953 ## yr.ctr:countryCA -0.01196 0.00749801 -1.5952 0.1148 ## yr.ctr:countryUS -0.00627 0.00703152 -0.8921 0.3752 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA cntrUS yr.:NL yr.:CA ## yr.ctr 0.000 ## countryNL -0.809 0.000 ## countryCA -0.929 0.000 0.752 ## countryUS -0.991 0.000 0.802 0.921 ## yr.ctr:countryNL 0.000 -0.809 0.000 0.000 0.000 ## yr.ctr:countryCA 0.000 -0.929 0.000 0.000 0.000 0.752 ## yr.ctr:countryUS 0.000 -0.991 0.000 0.000 0.000 0.802 0.921 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.18586069 -0.69331440 -0.01165266 0.66706364 1.82625135 ## ## Residual standard error: 0.193368 ## Degrees of freedom: 84 total; 76 residual The summary doesn’t provide the residual variances for each country directly. Instead, in the portion of the summary labeled “Variance function”, we find the estimated ratio of the residual standard deviation for each country to the residual standard deviation for the reference country, Denmark. Thus, for example, the residual standard deviation for the Netherlands is \\(0.726\\) times the residual standard deviation for Denmark, and so on. We would like to ask if model with country-specific variances provides a statistically significant improvement in fit relative to the model with homogeneous error variances. Here, it is crucial to remember that the default fitting scheme in nlme::gls is REML. However, because the models share the same fixed-effect structure, we can compare AIC values from the REML fits directly. Further, because the modes are nested, we can use the REML fits for a likelihood ratio test. The anova.gls command provides both. gls0 &lt;- gls(pct.male ~ yr.ctr * country, data = births) # OLS fit anova(gls0, gls1) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls0 1 9 -42.18264 -21.20604 30.09132 ## gls1 2 12 -94.69995 -66.73115 59.34998 1 vs 2 58.51731 &lt;.0001 Both the LRT and the AIC suggest that the GLS model with country-specific variances provides a statistically significant improvement over the OLS model with homogeneous error variances. If the fixed-effect structures had not been the same, it would not have been correct to compare the models using the REML fits. Instead, we would have to re-fit the models using ML. For the sake of illustration, let’s do this anyway, and see if and how the results differ. gls0ML &lt;- gls(pct.male ~ yr.ctr * country, data = births, method = &quot;ML&quot;) gls1ML &lt;- gls(pct.male ~ yr.ctr * country, data = births, weights = varIdent(form = ~ 1 | country), method = &quot;ML&quot;) anova(gls0ML, gls1ML) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls0ML 1 9 -99.76871 -77.89136 58.88435 ## gls1ML 2 12 -158.44573 -129.27593 91.22287 1 vs 2 64.67702 &lt;.0001 We obtain somewhat different numerical results based on the ML fit, even though the qualitative outcome of the test is unchanged (the model with country-specific variances is still strongly favored). In any event, we can also compare the variance estimates from the ML fit to those from the REML fit. summary(gls1ML) ## Generalized least squares fit by maximum likelihood ## Model: pct.male ~ yr.ctr * country ## Data: births ## AIC BIC logLik ## -158.4457 -129.2759 91.22287 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7264997 0.3971729 0.1347962 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.04219635 1217.4265 0.0000 ## yr.ctr 0.00084 0.00696850 0.1211 0.9039 ## countryNL -0.15476 0.05215650 -2.9673 0.0040 ## countryCA -0.00381 0.04540269 -0.0839 0.9334 ## countryUS -0.11095 0.04257798 -2.6059 0.0110 ## yr.ctr:countryNL -0.00736 0.00861336 -0.8549 0.3953 ## yr.ctr:countryCA -0.01196 0.00749801 -1.5952 0.1148 ## yr.ctr:countryUS -0.00627 0.00703152 -0.8921 0.3752 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA cntrUS yr.:NL yr.:CA ## yr.ctr 0.000 ## countryNL -0.809 0.000 ## countryCA -0.929 0.000 0.752 ## countryUS -0.991 0.000 0.802 0.921 ## yr.ctr:countryNL 0.000 -0.809 0.000 0.000 0.000 ## yr.ctr:countryCA 0.000 -0.929 0.000 0.000 0.000 0.752 ## yr.ctr:countryUS 0.000 -0.991 0.000 0.000 0.000 0.802 0.921 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.29802807 -0.72889177 -0.01225061 0.70129397 1.91996548 ## ## Residual standard error: 0.1839296 ## Degrees of freedom: 84 total; 76 residual Note that the estimate of the residual standard deviation (mislabeled as the “residual standard error” in the R output) is smaller for the ML fit than for the REML fit, as we expect. To continue, we can also fit a first-order autoregressive correlation structure to the residual errors within each country. Here, because the data are evenly spaced and are already sorted in the data set, it’s simple to add the within country autocorrelation. To write this model as an equation, the fixed-effect specification remains unchanged: \\[\\begin{equation} y_{it} = a_i + b_i x_{it} + \\varepsilon_{it}. \\end{equation}\\] The marginal distribution of the errors is also unchanged: \\(\\varepsilon_{it} \\sim \\mathcal{N}(0, \\sigma^2_i)\\). However, the within-country errors are now correlated: \\[\\begin{equation} \\mathrm{Corr}(\\varepsilon_{it_1}, \\varepsilon_{jt_2}) = \\begin{cases} \\rho^{|t_1 - t_2|} &amp; i = j \\\\ 0 &amp; i \\neq j \\end{cases} \\end{equation}\\] gls2 &lt;- gls(pct.male ~ yr.ctr * country, data = births, weights = varIdent(form = ~ 1 | country), correlation = corAR1(form = ~ 1 | country)) summary(gls2) ## Generalized least squares fit by REML ## Model: pct.male ~ yr.ctr * country ## Data: births ## AIC BIC logLik ## -93.01222 -62.71269 59.50611 ## ## Correlation Structure: AR(1) ## Formula: ~1 | country ## Parameter estimate(s): ## Phi ## 0.07081995 ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7416576 0.4009707 0.1338036 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37134 0.04520738 1136.3486 0.0000 ## yr.ctr 0.00088 0.00741198 0.1187 0.9058 ## countryNL -0.15484 0.05628375 -2.7510 0.0074 ## countryCA -0.00385 0.04870615 -0.0791 0.9371 ## countryUS -0.11127 0.04561027 -2.4396 0.0170 ## yr.ctr:countryNL -0.00713 0.00922801 -0.7727 0.4421 ## yr.ctr:countryCA -0.01188 0.00798562 -1.4872 0.1411 ## yr.ctr:countryUS -0.00634 0.00747803 -0.8481 0.3991 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA cntrUS yr.:NL yr.:CA ## yr.ctr 0.000 ## countryNL -0.803 0.000 ## countryCA -0.928 0.000 0.746 ## countryUS -0.991 0.000 0.796 0.920 ## yr.ctr:countryNL 0.000 -0.803 0.000 0.000 0.000 ## yr.ctr:countryCA 0.000 -0.928 0.000 0.000 0.000 0.746 ## yr.ctr:countryUS 0.000 -0.991 0.000 0.000 0.000 0.796 0.920 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.15117280 -0.70400503 -0.02488281 0.66159776 1.82002897 ## ## Residual standard error: 0.1936784 ## Degrees of freedom: 84 total; 76 residual The estimate of the within-country correlation is small: only 0.071. The model with autocorrelated errors is nested within the model with heterogeneous variances, and both have the same fixed-effect structure, so we can compare the two REML fits directly: anova(gls0, gls1, gls2) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls0 1 9 -42.18264 -21.20604 30.09132 ## gls1 2 12 -94.69995 -66.73115 59.34998 1 vs 2 58.51731 &lt;.0001 ## gls2 3 13 -93.01222 -62.71269 59.50611 2 vs 3 0.31227 0.5763 By either AIC or the LRT, the model with the autocorrelated errors does not provide a statistically significant improvement in fit. We can now use the model with heterogeneous variances and independent errors to conduct the usual inferences on the fixed effects. Because we now compare models with different fixed-effect structures, we must work on the ML fits. Let’s start with a model that removes the interaction between time and country. The model is: \\[\\begin{equation} y_{it} = a_i + b x_{it} + \\varepsilon_{it}. \\end{equation}\\] In other words, there is a common slope among the countries. gls3ML &lt;- gls(pct.male ~ yr.ctr + country, data = births, weights = varIdent(form = ~ 1 | country), method = &quot;ML&quot;) summary(gls3ML) ## Generalized least squares fit by maximum likelihood ## Model: pct.male ~ yr.ctr + country ## Data: births ## AIC BIC logLik ## -159.5456 -137.6682 88.7728 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7098035 0.4232237 0.1323319 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.04238044 1212.1383 0.0000 ## yr.ctr -0.00585 0.00086365 -6.7731 0.0000 ## countryNL -0.15476 0.05197129 -2.9778 0.0039 ## countryCA -0.00381 0.04601974 -0.0828 0.9342 ## countryUS -0.11095 0.04274991 -2.5954 0.0113 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA ## yr.ctr 0.000 ## countryNL -0.815 0.000 ## countryCA -0.921 0.000 0.751 ## countryUS -0.991 0.000 0.808 0.913 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.32703514 -0.72533260 0.03426803 0.82167128 2.12375963 ## ## Residual standard error: 0.1883428 ## Degrees of freedom: 84 total; 79 residual anova(gls3ML, gls1ML) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls3ML 1 9 -159.5456 -137.6683 88.77280 ## gls1ML 2 12 -158.4457 -129.2759 91.22287 1 vs 2 4.900132 0.1793 Both AIC and the LRT favor a model with a common slope. Let’s go further to see if the intercepts differ among the countries. In other words, we can entertain the model \\[\\begin{equation} y_{it} = a + b x_{it} + \\varepsilon_{it}. \\end{equation}\\] gls4ML &lt;- gls(pct.male ~ yr.ctr, data = births, weights = varIdent(form = ~ 1 | country), method = &quot;ML&quot;) summary(gls4ML) ## Generalized least squares fit by maximum likelihood ## Model: pct.male ~ yr.ctr ## Data: births ## AIC BIC logLik ## -136.0564 -121.4715 74.0282 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.6559362 0.6051856 0.1159405 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.26375 0.005328997 9619.775 0 ## yr.ctr -0.00558 0.000880055 -6.335 0 ## ## Correlation: ## (Intr) ## yr.ctr 0 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.5381852 -0.3580929 0.1883530 0.9250208 2.3348069 ## ## Residual standard error: 0.2164103 ## Degrees of freedom: 84 total; 82 residual anova(gls4ML, gls3ML, gls1ML) ## Model df AIC BIC logLik Test L.Ratio p-value ## gls4ML 1 6 -136.0564 -121.4715 74.02820 ## gls3ML 2 9 -159.5456 -137.6683 88.77280 1 vs 2 29.489202 &lt;.0001 ## gls1ML 3 12 -158.4457 -129.2759 91.22287 2 vs 3 4.900132 0.1793 There is strong evidence that the percentage of male births differs among countries, after accounting for the effect of the temporal trend. We can visualize the model by making scatterplots and overlaying fitted regression lines. Having finished with model selection, we’ll revert to the REML fits for final parameter estimation. gls3.reml &lt;- gls(pct.male ~ yr.ctr + country, data = births, weights = varIdent(form = ~ 1 | country)) summary(gls3.reml) ## Generalized least squares fit by REML ## Model: pct.male ~ yr.ctr + country ## Data: births ## AIC BIC logLik ## -122.7459 -101.4209 70.37297 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | country ## Parameter estimates: ## DK NL CA US ## 1.0000000 0.7099907 0.4237511 0.1352729 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.04213583 1219.1750 0.0000 ## yr.ctr -0.00586 0.00087529 -6.7004 0.0000 ## countryNL -0.15476 0.05167590 -2.9949 0.0037 ## countryCA -0.00381 0.04576279 -0.0832 0.9339 ## countryUS -0.11095 0.04251960 -2.6094 0.0108 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA ## yr.ctr 0.000 ## countryNL -0.815 0.000 ## countryCA -0.921 0.000 0.751 ## countryUS -0.991 0.000 0.808 0.912 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.26855289 -0.70087717 0.03371643 0.79968077 2.07208997 ## ## Residual standard error: 0.1930906 ## Degrees of freedom: 84 total; 79 residual par(mfrow = c(2, 2), las = 1) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Canada&quot;)) with(subset(births, country == &quot;CA&quot;), points(pct.male ~ year)) abline(a = 51.3671 + 0.00586 * 1980, b = -0.00586) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;USA&quot;)) with(subset(births, country == &quot;US&quot;), points(pct.male ~ year)) abline(a = 51.26 + 0.00586 * 1980, b = -0.00586) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Denmark&quot;)) with(subset(births, country == &quot;DK&quot;), points(pct.male ~ year)) abline(a = 51.371 + 0.00586 * 1980, b = -0.00586) with(births, plot(pct.male ~ year, type = &quot;n&quot;, ylab = &quot;percent male&quot;, main = &quot;Netherlands&quot;)) with(subset(births, country == &quot;NL&quot;), points(pct.male ~ year)) abline(a = 51.2162 + 0.00586 * 1980, b = -0.00586) It is interesting to compare the estimate of the slope between the GLS model and the naive OLS fit. In the GLS model, the slope is estimated to be \\(-0.00586\\%\\) per year, with a standard error of \\(8.8 \\times 10^{-4}\\). In the OLS fit, the estimate is \\(-0.00556\\%\\) per year, with a standard error of \\(2.3 \\times 10^{-3}\\). Thus the GLS fit has substantially improved the precision of the estimate of the temporal trend. summary(gls(pct.male ~ yr.ctr + country, data = births)) ## Generalized least squares fit by REML ## Model: pct.male ~ yr.ctr + country ## Data: births ## AIC BIC logLik ## -70.12178 -55.9051 41.06089 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 51.37095 0.02762942 1859.2846 0.0000 ## yr.ctr -0.00556 0.00228142 -2.4350 0.0171 ## countryNL -0.15476 0.03907390 -3.9607 0.0002 ## countryCA -0.00381 0.03907390 -0.0975 0.9226 ## countryUS -0.11095 0.03907390 -2.8396 0.0057 ## ## Correlation: ## (Intr) yr.ctr cntrNL cntrCA ## yr.ctr 0.000 ## countryNL -0.707 0.000 ## countryCA -0.707 0.000 0.500 ## countryUS -0.707 0.000 0.500 0.500 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.517325067 -0.553487986 0.009393865 0.508672668 3.142893232 ## ## Residual standard error: 0.1266139 ## Degrees of freedom: 84 total; 79 residual 5.2 Temporal (serial) correlation Temporal structure often induces a (positive) correlation between data points that occur close together in time. These are the same types of correlations that we would expect to find for any data that occur as part of a series, or serial correlation. (Other data types may display serial correlations that are not driven by time, such as positions along a one-dimensional spatial transect.) We will illustrate how to handle temporal correlations using a time series of annual moorhen abundance on the island of Kauai. These data are analyzed in Ch. 6 of Zuur et al. (2009), and are originally from Reed et al. (2007). The data are available for download from the website associated with Zuur et al. (2009)’s text. More details about the models available to handle serial correlations in nlme::gls can be found in \\(\\S\\) 5.3.1 of Pinheiro and Bates (2000). First we load the data and do some housekeeping. rm(list = ls()) require(nlme) birds &lt;- read.table(&quot;data/Hawaii.txt&quot;, head = T) ## extract moorhen data moorhen &lt;- birds[, c(&quot;Year&quot;, &quot;Rainfall&quot;, &quot;Moorhen.Kauai&quot;)] ## rename variables names(moorhen) &lt;- c(&quot;year&quot;, &quot;rainfall&quot;, &quot;abundance&quot;) ## remove NAs moorhen &lt;- na.omit(moorhen) with(moorhen, plot(abundance ~ year)) with(moorhen, plot(log(abundance) ~ year)) with(moorhen, plot(log(abundance) ~ rainfall)) Suppose we want to characterize any possible (linear) temporal trend in moorhen abundance, and/or any association between moorhen abundance and annual rainfall. We log transform the abundance data to convert any multiplicative time trends into linear trends. First we will fit an OLS model and use the function acf to plot the autocorrelation function (ACF) of the residuals. fm1 &lt;- nlme::gls(log(abundance) ~ rainfall + year, data = moorhen) plot(residuals(fm1) ~ moorhen$year) acf(residuals(fm1)) The significant first-order autocorrelation suggests a first-order autoregressive model might be appropriate for these errors. We will fit such a model using the corAR1 correlation structure. In doing so, we use the formula form = ~ year to indicate that the year variable in the data set provides the time index. This is a necessary step with these data because some years are missing. fm2 &lt;- nlme::gls(log(abundance) ~ rainfall + year, data = moorhen, correlation = corAR1(form = ~ year)) summary(fm2) ## Generalized least squares fit by REML ## Model: log(abundance) ~ rainfall + year ## Data: moorhen ## AIC BIC logLik ## 124.6062 133.2946 -57.3031 ## ## Correlation Structure: ARMA(1,0) ## Formula: ~year ## Parameter estimate(s): ## Phi1 ## 0.5599778 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) -161.17809 32.93180 -4.894299 0.0000 ## rainfall -0.00783 0.01433 -0.546369 0.5877 ## year 0.08326 0.01663 5.005461 0.0000 ## ## Correlation: ## (Intr) ranfll ## rainfall -0.006 ## year -1.000 -0.001 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -3.3338721 -0.5125953 0.2117251 0.6813604 1.5181543 ## ## Residual standard error: 0.9112434 ## Degrees of freedom: 45 total; 42 residual The fit suggests that the residuals from adjacent years have a reasonably strong positive correlation of \\(\\approx 0.56\\). To see if the AR1 model has successfully accounted for the correlation structure in the residuals, we will inspect the “normalized” residuals (see the R help for residuals.gls for details). If all the structure in the residuals has been successfully accounted for, then the normalized residuals should look like iid draws from a standard Gaussian distribution. acf(residuals(fm2, type = &quot;normalized&quot;)) None of the autocorrelations among the normalized residuals differ significantly from zero. Finally, because the AR1 model nests the OLS model, we can use a LRT to inspect whether the first-order autoregression provides a significant improvement in fit. anova(fm1, fm2) ## Model df AIC BIC logLik Test L.Ratio p-value ## fm1 1 4 134.5734 141.5240 -63.28668 ## fm2 2 5 124.6062 133.2946 -57.30310 1 vs 2 11.96716 5e-04 The LRT suggests that the model with a first-order autocorrelation signficantly improves on the OLS model. We would then proceed to use this model to characterize the temporal trend in moorhen abundance, and the (lack of) association between moorhen abundance and rainfall. 5.3 Spatial data Data that are organized in space are also often correlated, with data points that occur close together in space being strongly (positively) correlated with one another. To illustrate spatial correlations, we will use the Wheat2 data provided as part of the nlme package. Pinheiro and Bates (2000) (p. 260) introduce the data as follows: “Stroup and Baenziger (1994) describe an agronomic experiment to compare the yield of 56 different varieties of wheat planted in four blocks arranged according to a randomized complete complete block design. All 56 varieties of wheat were used in each block. The latitude and longitude of each experimental unit in the trial were also recorded.” data(&quot;Wheat2&quot;) summary(Wheat2) ## Block variety yield latitude longitude ## 4:56 ARAPAHOE : 4 Min. : 1.05 Min. : 4.30 Min. : 1.20 ## 2:56 BRULE : 4 1st Qu.:23.52 1st Qu.:17.20 1st Qu.: 7.20 ## 3:56 BUCKSKIN : 4 Median :26.85 Median :25.80 Median :14.40 ## 1:56 CENTURA : 4 Mean :25.53 Mean :27.22 Mean :14.08 ## CENTURK78: 4 3rd Qu.:30.39 3rd Qu.:38.70 3rd Qu.:20.40 ## CHEYENNE : 4 Max. :42.00 Max. :47.30 Max. :26.40 ## (Other) :200 A plot of the spatial locations of these data shows that the blocks hide a lot of information about the actual spatial position of the individual plots. While a traditional RCBD analysis might account for some of the spatial variation, we could perhaps do better by ignoring the block designations and modeling spatial correlations based on the actual location of each plot. with(Wheat2, plot(x = longitude, y = latitude, pch = as.numeric(Block))) Our goal is simply to characterizes the differences in mean yield among the 56 varieties while accounting for possible spatial correlations. We begin by fitting a simple one-factor ANOVA model and inspecting the residuals. First, we will use the plot_ly function to generate a three-dimensional view of the residuals. This 3D plot can be rotated in R, although the rotation is not possible in this Rbook. fm1 &lt;- nlme::gls(yield ~ variety, data = Wheat2) require(plotly) ## Loading required package: plotly ## Loading required package: ggplot2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout plot_ly(x = Wheat2$latitude, y = Wheat2$longitude, z = resid(fm1), type = &quot;scatter3d&quot;, mode = &quot;markers&quot;, color = resid(fm1)) The residuals suggest a clear spatial trend in fertility. Next, We plot the semivariogram using nlme::Variogram. This command will actually plot the semivariance normalized by the sill, such that the quantity plotted is 1 minus the correlation between two points. In the plot below, the smooth curve is a loess curve fit to the calculated points. plot(Variogram(fm1, form = ~ latitude + longitude)) The semivariogram suggests a non-zero nugget. Here, we will fit spherical, Gaussian, and linear correlation models based on the latitude and longitude coordinates of each data point. For each fit, we will then plot a semivariogram of the normalized residuals. Again, if the model has done a good job accounting for the correlation structure in the data, then the normalized residuals should be independent. See \\(\\S\\) 5.3.2 of Pinheiro and Bates (2000) for more details about the different spatial correlation structures available in nlme::gls. In particular, see their Fig. 5.9 for a display of how different spatial correlation models compare. For each model, we must supply starting values for the range and nugget. Rough starting values based on the semivariogram of the raw residuals will suffice. Calls to Variogram will plot the calculated semivariances and overlay the fitted semivariogram. ## spherical covariance fm2 &lt;- nlme::gls(yield ~ variety, data = Wheat2, correlation = corSpher(c(28, 0.2), form = ~ latitude + longitude, nugget = TRUE)) # need to supply starting values plot(Variogram(fm2, form = ~ latitude + longitude)) ## Gaussian covariance fm3 &lt;- nlme::gls(yield ~ variety, data = Wheat2, correlation = corGaus(c(28, 0.2), form = ~ latitude + longitude, nugget = TRUE)) # need to supply starting values plot(Variogram(fm3, form = ~ latitude + longitude)) ## linear covariance fm4 &lt;- nlme::gls(yield ~ variety, data = Wheat2, correlation = corLin(c(28, 0.2), form = ~ latitude + longitude, nugget = TRUE)) # need to supply starting values plot(Variogram(fm4, form = ~ latitude + longitude)) If we wish, we can extract the estimated nugget and range from each model by calling print. print(fm4) ## Generalized least squares fit by REML ## Model: yield ~ variety ## Data: Wheat2 ## Log-restricted-likelihood: -533.2815 ## ## Coefficients: ## (Intercept) varietyBRULE varietyBUCKSKIN varietyCENTURA ## 28.41005933 -1.71161334 6.99635483 -2.58344151 ## varietyCENTURK78 varietyCHEYENNE varietyCODY varietyCOLT ## -2.46550574 -3.07214768 -4.49895886 -2.22833768 ## varietyGAGE varietyHOMESTEAD varietyKS831374 varietyLANCER ## -4.26332881 -6.20641501 -1.18492850 -4.53944671 ## varietyLANCOTA varietyNE83404 varietyNE83406 varietyNE83407 ## -6.32251412 -2.92856070 -1.17290874 -2.62264363 ## varietyNE83432 varietyNE83498 varietyNE83T12 varietyNE84557 ## -5.65619040 2.20715030 -5.32860054 -5.50430895 ## varietyNE85556 varietyNE85623 varietyNE86482 varietyNE86501 ## -0.19445894 -3.51776928 -2.99576076 -2.22781284 ## varietyNE86503 varietyNE86507 varietyNE86509 varietyNE86527 ## -0.13437309 -0.49972260 -5.64308018 -1.70998353 ## varietyNE86582 varietyNE86606 varietyNE86607 varietyNE86T666 ## -4.52784048 -0.04400593 -1.87905786 -11.34834217 ## varietyNE87403 varietyNE87408 varietyNE87409 varietyNE87446 ## -7.07640881 -3.87938223 -1.07868064 -5.46517624 ## varietyNE87451 varietyNE87457 varietyNE87463 varietyNE87499 ## -2.90699758 -3.59541052 -4.49563865 -5.67322707 ## varietyNE87512 varietyNE87513 varietyNE87522 varietyNE87612 ## -5.60494961 -4.84498289 -7.64855589 0.48268951 ## varietyNE87613 varietyNE87615 varietyNE87619 varietyNE87627 ## 1.37319577 -3.69500935 1.18693565 -10.07566895 ## varietyNORKAN varietyREDLAND varietyROUGHRIDER varietySCOUT66 ## -5.27977336 0.36480075 -1.53131835 0.30979481 ## varietySIOUXLAND varietyTAM107 varietyTAM200 varietyVONA ## -2.75246638 -5.45138542 -9.11014805 -3.25735184 ## ## Correlation Structure: Linear spatial correlation ## Formula: ~latitude + longitude ## Parameter estimate(s): ## range nugget ## 10.7962043 0.2050487 ## Degrees of freedom: 224 total; 168 residual ## Residual standard error: 6.960609 We can use AIC to compare the fits of the two different spatial correlation structures. anova(fm1, fm2, fm3, fm4) ## Model df AIC BIC logLik Test L.Ratio p-value ## fm1 1 57 1354.742 1532.808 -620.3709 ## fm2 2 59 1185.863 1370.177 -533.9315 1 vs 2 172.8787 &lt;.0001 ## fm3 3 59 1185.102 1369.416 -533.5509 ## fm4 4 59 1184.563 1368.877 -533.2815 The linear correlation structure is AIC best. At this point, if we were really interested in these data, we would proceed to analyze for significant differences among the 56 wheat varieties. For our present purposes, we will merely note that the usual \\(F\\)-test rejects the null hypothesis of equality of means when we account for the spatial correlation in the residuals, but does not do so when we assumed the residuals were independent. anova(fm1) ## Denom. DF: 168 ## numDF F-value p-value ## (Intercept) 1 2454.621 &lt;.0001 ## variety 55 0.730 0.9119 anova(fm4) ## Denom. DF: 168 ## numDF F-value p-value ## (Intercept) 1 233.98320 &lt;.0001 ## variety 55 2.65823 &lt;.0001 Bibliography Pinheiro, José, and Douglas Bates. 2000. Mixed-Effects Models in S and S-PLUS. Springer. Zuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with R. Vol. 574. Springer. "],["hierarchical-mixed-models.html", "Chapter 6 Hierarchical (mixed) models 6.1 One-factor layout: Dyestuff data 6.2 Bayesian analysis 6.3 Negative within-group correlations 6.4 Random coefficient models: RIKZ data 6.5 Nested and crossed random effects", " Chapter 6 Hierarchical (mixed) models 6.1 One-factor layout: Dyestuff data We will illustrate the basic ideas of hierarchical models with the Dyestuff data contained in lme4. According to Bates (2012+), these data originally appeared in Davies (1947), and “are described in Davies and Goldsmith (1972, Table 6.3, p. 131) … as coming from ‘an investigation to find out how much the variation from batch to batch in the quality of an intermediate product contributes to the variation in the yield of the dyestuff made from it’”. The data consist of 6 batches, each of which gives rise to 5 observations. Preparatory work: require(lme4) ## Loading required package: lme4 ## Loading required package: Matrix data(Dyestuff) summary(Dyestuff) ## Batch Yield ## A:5 Min. :1440 ## B:5 1st Qu.:1469 ## C:5 Median :1530 ## D:5 Mean :1528 ## E:5 3rd Qu.:1575 ## F:5 Max. :1635 with(Dyestuff, stripchart(Yield ~ Batch, pch = 16)) To develop some notation, let \\(i = 1, \\ldots, 6\\) index the batches, let \\(j = 1, \\ldots, 5\\) index the observations within each batch, and let \\(y_{ij}\\) denote observation \\(j\\) from batch \\(i\\). We will use nlme::gls to fit a model that assumes that the data within each batch are correlated. In other words, we fit the model \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(\\mu_i, \\sigma^2) \\\\ \\mathrm{Corr}(y_{ij}, y_{ik}) &amp; = \\rho \\end{align}\\] require(nlme) ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmList fm2 &lt;- gls(Yield ~ 1, data = Dyestuff, correlation = corCompSymm(form = ~ 1 | Batch)) summary(fm2) ## Generalized least squares fit by REML ## Model: Yield ~ 1 ## Data: Dyestuff ## AIC BIC logLik ## 325.6543 329.7562 -159.8271 ## ## Correlation Structure: Compound symmetry ## Formula: ~1 | Batch ## Parameter estimate(s): ## Rho ## 0.4184874 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 1527.5 19.38341 78.80449 0 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.34770180 -0.90488550 0.03850577 0.73160955 1.65574793 ## ## Residual standard error: 64.92534 ## Degrees of freedom: 30 total; 29 residual The most salient components of this output are the estimate of the overall mean, and the estimate of the within-batch correlation (\\(\\hat{\\rho} = 0.42\\)). Now we will fit a hierarchical model that includes a random effect for the batch. We can write the model as \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(B_i, \\sigma_\\varepsilon^2) \\\\ B_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma_B^2). \\end{align}\\] fm3 &lt;- lmer(Yield ~ 1 + (1 | Batch), data = Dyestuff) summary(fm3) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Yield ~ 1 + (1 | Batch) ## Data: Dyestuff ## ## REML criterion at convergence: 319.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4117 -0.7634 0.1418 0.7792 1.8296 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Batch (Intercept) 1764 42.00 ## Residual 2451 49.51 ## Number of obs: 30, groups: Batch, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1527.50 19.38 78.8 The estimate of the overall mean is the same as it is in the GLS fit. Note also that we can recover the estimate of the within-batch correlation from the estimates of the variances of the random effects: var.B &lt;- 1764 var.eps &lt;- 2451 var.B / (var.B + var.eps) ## [1] 0.4185053 To obtain the conditional modes (BLUPs) of the batch-level random effect, we can use the command ranef: ranef(fm3) ## $Batch ## (Intercept) ## A -17.6068514 ## B 0.3912634 ## C 28.5622256 ## D -23.0845385 ## E 56.7331877 ## F -44.9952868 ## ## with conditional variances for &quot;Batch&quot; The conditional modes given here correspond to the differences between the mean for each batch and the overall mean (\\(\\mu\\)). To convert these to best guesses for the mean of each batch, we have to the overall mean back. This can be done by using the command fixef to extract the lone fixed-effect estimate from the model: (batch.conditional.modes &lt;- (fixef(fm3) + ranef(fm3)$Batch$`(Intercept)`)) ## [1] 1509.893 1527.891 1556.062 1504.415 1584.233 1482.505 It is informative to compare the conditional models for each batch to the sample means. We can calculate the sample means with the tapply function (batch.means &lt;- with(Dyestuff, tapply(Yield, Batch, mean))) ## A B C D E F ## 1505 1528 1564 1498 1600 1470 Now plot the sample means against the conditional modes: cbind(batch.means, batch.conditional.modes) ## batch.means batch.conditional.modes ## A 1505 1509.893 ## B 1528 1527.891 ## C 1564 1556.062 ## D 1498 1504.415 ## E 1600 1584.233 ## F 1470 1482.505 plot(x = batch.means, y = batch.conditional.modes, xlim = range(batch.means), ylim = range(batch.means), xlab = &quot;sample means&quot;, ylab = &quot;conditional modes&quot;, pch = LETTERS[1:6]) abline(a = 0, b = 1) The conditional modes are “shrunken” towards the global mean relative to the sample means. Why is this so? To conduct inferences about the parameters in the hierarchical model, lme4::lmer offers likelihood profiling. This is the same idea that we encountered when we were using the likelihood to calculate profile-based confidence intervals earlier in the course. lme4::lmer does all its profiling on the ML fit, so we begin by refitting our hierarchical model using ML. To do so, set the optional argument REML to FALSE. fm3ML &lt;- lmer(Yield ~ 1 + (1 | Batch), data = Dyestuff, REML = FALSE) summary(fm3ML) ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: Yield ~ 1 + (1 | Batch) ## Data: Dyestuff ## ## AIC BIC logLik -2*log(L) df.resid ## 333.3 337.5 -163.7 327.3 27 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4315 -0.7972 0.1480 0.7721 1.8037 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Batch (Intercept) 1388 37.26 ## Residual 2451 49.51 ## Number of obs: 30, groups: Batch, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1527.50 17.69 86.33 Switching to ML has decreased our estimate of the batch-level variance, and decreased it by quite a bit. To construct profile-based intervals, we use the lme4::profile function. We will use the xyplot function from the lattice package to display the profiles. pr &lt;- profile(fm3ML) lattice::xyplot(pr) There are three panels here, one for each of the parameters in the model. These parameters are labeled as \\(\\sigma_1\\) for the standard deviation of the block-level random effect (what we have written \\(\\sigma_B\\)), \\(\\sigma\\) for the standard deviation of the errors (what we have written as \\(\\sigma_\\varepsilon\\)), and “(Intercept)” for the global mean (what we have written as \\(\\mu\\)). Within each panel, the values plotted on the vertical axis are the signed square roots of the likelihood ratio test statistic. This value is denoted by the Greek letter \\(\\zeta\\) (“zeta”); hence, the plots we are viewing are called zeta-plots. I do not know how widely used zeta-plots are. They may be more common in other realms of science, although I have never encountered them outside lme4. Basically, they are a visualization of profile-likelihood based confidence intervals. The vertical lines in each panel give the limits of (working from the inside out) 50%, 80%, 90%, 95%, and 99% confidence intervals for each parameter. We can also extract these confidence limits using the confint function. Below, we show 95% confidence intervals; of course, one can change the confidence level as needed. confint(pr, level = .95) ## 2.5 % 97.5 % ## .sig01 12.19854 84.06305 ## .sigma 38.22998 67.65770 ## (Intercept) 1486.45150 1568.54849 So, a 95% confidence interval for \\(\\mu\\) ranges from 1486 to 1569. Here is a bit more about the logic behind zeta plots (feel free to skip this paragraph if you wish). Recall that negative log-likelihood profiles are convex and approximately quadratic. Plotting the LRT statistic instead of the likelihood itself has the effect of placing the nadir of these curves at 0, instead of at the value of the negative log-likelihood. Taking the square root of the LRT statistic converts a quadratic curve (a u-shape) into a linear one (a v-shape). We can see this v-shape by using the optional argument absVal = TRUE: lattice::xyplot(pr, absVal = TRUE) The purpose of using a signed square root is to turn these V-shaped plots into straight lines. Straight lines are useful in turn because if the profile log likelihood is actually quadratic, then the zeta plot will yield a perfectly straight line, and thus a local approximation to the confidence interval will be appropriate. If the zeta-plot is non-linear, then the confidence interval becomes more asymmetric, and a local approximation fares more poorly. (Start reading again if you skipped the above detail). There is an interesting detail to the zeta-plots. Consider the zeta-plot for \\(\\sigma_1\\) (the standard deviation of the batch-level random effects), and try to find the lower limit of a 99% confidence interval. You’ll notice that the zeta-plot hits 0 (the lowest possible value for a standard deviation) before the interval is completed. Thus, the lower limit of this interval is 0: confint(pr, level = 0.99) ## 0.5 % 99.5 % ## .sig01 0.00000 113.68769 ## .sigma 35.56317 75.66803 ## (Intercept) 1465.87401 1589.12602 Although we are not much in the habit of conducting hypothesis tests in this course, we would conclude that we would fail to reject the null hypothesis that the batch-level variance equals 0 with a 99% level test. We can also obtain bivariate confidence regions from the profile likelihood using the lattice::splom command. lattice::splom(pr) Panels above the diagonal show bivariate, profile-based confidence regions for each pair of parameters. Within each panel, we see 50%, 80%, 90%, 95%, and 99% confidence regions. (You should be able to figure out which is which.) The panels show, for example, that the estimate of the observation-level standard deviation is slightly negatively correlated with the estimate of the batch-level standard deviation. Panels below the diagonal show the same plots on the \\(\\zeta\\) scale. See Bates (2012+) 1.5.3 for a detailed description of how to interpret these plots. There is a second approach to calculating confidence intervals and/or conducting hypothesis tests for fixed-effect parameters. Famously, lme4::lmer does not provide degrees of freedom for the estimates of the fixed effects. The package lmerTest uses the Satterthwaite approximation to estimate these df. require(lmerTest) ## Loading required package: lmerTest ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step fm5 &lt;- lmerTest::lmer(Yield ~ 1 + (1 | Batch), data = Dyestuff) summary(fm5) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Yield ~ 1 + (1 | Batch) ## Data: Dyestuff ## ## REML criterion at convergence: 319.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.4117 -0.7634 0.1418 0.7792 1.8296 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Batch (Intercept) 1764 42.00 ## Residual 2451 49.51 ## Number of obs: 30, groups: Batch, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 1527.50 19.38 5.00 78.8 6.23e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 lmerTest::lmer gives the associated df for the estimate of the intercept as 5. If this were a class in experimental design, we would regard the individual measurements from each batch as subsamples, in which case the correct df for inferences about the intercept should be \\(6 - 1 = 5\\). In this case, the calculation from lmerTest::lmer matches our intuition. Equipped with this information, we could construct a 95% confidence interval for the overall mean as 1527.5 + 19.38 * qt(c(0.025, 0.975), df = 5) ## [1] 1477.682 1577.318 Compare this interval with the profile interval generated by lme4::profile. Finally, we can also obtain prediction intervals on the conditional modes of the random effects by using the condVar = TRUE option in a call to ranef. We illustrate below, and direct the output to lattice::dotplot for visualization. Each line below shows a 95% prediction interval for a conditional mode. When there are many levels of the random effect, Bates (2012+) recommends using lattice::qqmath instead of lattice::dotplot. lattice::dotplot(ranef(fm3, condVar = TRUE)) ## $Batch 6.2 Bayesian analysis The mixed-model formulation is our first encounter with a hierarchical model. One often hears the phrase “Bayesian hierarchical model” in ecology. It is important to realize that not all Bayesian models are hierarchical, and not all hierarchical models are Bayesian. Indeed, we have seen examples of both: none of the Bayesian examples that we have seen in earlier chapters were hierarchical, and the mixed-model analysis of the Dyestuff data is a hierarchical model analyzed from a frequentist perspective. However, we can certainly analyze hierarchical models from a Bayesian perspective as well, leading to a Bayesian hierarchical model. The above paragraph begs the question: What defines a hierarchical model? One can find definitions in the literature, though it isn’t clear to me that any of these definitions are fully precise, although I may just be ignorant. As best I can tell, a hierarchical model is one that includes so-called “latent” variables. Latent variables are unobservable quantities on which the data depend, that in turn are related to model parameters via a statistical model. This “layered” construction of the model (observables depend on latent variables, and latent variables depend on model parameters) gives rise to the “hierarchy” that gives hierarchical models their name. This hierarchical perspective can be emphasized by how the model is written. For the dyestuff data, when we write \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(B_i, \\sigma^2_\\varepsilon) \\\\ B_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2_B) \\\\ \\end{align}\\] we emphasize that the distribution of the data depends in part on the latent variables \\(B_i\\), and the distribution of the latent variables depends in turn on some of the model parameters. To complete the Bayesian specification, we need to place priors on our three parameters: \\(\\mu\\), \\(\\sigma^2_B\\), and \\(\\sigma^2_\\varepsilon\\). In the absence of any information, we might choose a vague normal prior for \\(\\mu\\), and vague gamma priors for \\(1/\\sigma^2_B\\) and \\(1/\\sigma^2_\\varepsilon\\). The following R2Jags code implements such a model. require(R2jags) ## Loading required package: R2jags ## Loading required package: rjags ## Loading required package: coda ## Linked to JAGS 4.3.1 ## Loaded modules: basemod,bugs ## ## Attaching package: &#39;R2jags&#39; ## The following object is masked from &#39;package:coda&#39;: ## ## traceplot dyestuff.model &lt;- function() { ## likelihood for (j in 1:J) { y[j] ~ dnorm(B[batch[j]], tau_eps) # data distribution } ## latent variables for (b in 1:6){ B[b] ~ dnorm(mu, tauB) } mu ~ dnorm (0.0, 1E-6) # prior for the overall mean tau_eps ~ dgamma(.1, .1) tauB ~ dgamma(.1, .1) sd_eps &lt;- pow(tau_eps, -1/2) sdB &lt;- pow(tauB, -1/2) } jags.data &lt;- list(y = Dyestuff$Yield, batch = as.numeric(Dyestuff$Batch), J = nrow(Dyestuff)) jags.params &lt;- c(&quot;mu&quot;, &quot;sd_eps&quot;, &quot;sdB&quot;, &quot;B[1]&quot;, &quot;B[2]&quot;) jags.inits &lt;- function(){ list(&quot;mu&quot; = rnorm(1, .01), &quot;tauB&quot; = dexp(1, 1), &quot;tau_eps&quot; = dexp(1, 1)) } set.seed(1) jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = dyestuff.model, n.chains = 3, n.iter = 1e5) ## module glm loaded The code above requires a way to associate each observation with the batch from which the observation was drawn. We accomplish this by creating the variable batch that associates each observation with the numerical index of the batch. (That is, batch “A” is associated with the index 1, etc.) In the code, this happens with the line batch = as.numeric(Dyestuff$Batch) The as.numeric command returns numerical values for each level of the factor Batch. Note also that we have only asked for the posterior draws for the latent means of the first two batches, \\(B_1\\) and \\(B_2\\). This is merely to keep the output small for this example. We could of course ask for the means of the other batches as well. Let’s have a look at the output: print(jagsfit) ## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmp0Y6f62/model3304c6539f8&quot;, fit using jags, ## 3 chains, each with 1e+05 iterations (first 50000 discarded), n.thin = 50 ## n.sims = 3000 iterations saved. Running time = 0.34 secs ## mu.vect sd.vect 2.5% 25% 50% 75% 97.5% Rhat ## B[1] 1513.686 19.332 1474.441 1501.079 1515.075 1527.207 1548.378 1.001 ## B[2] 1526.920 19.084 1490.881 1514.551 1526.882 1538.502 1566.481 1.001 ## mu 1526.644 20.320 1483.504 1515.818 1527.076 1538.074 1567.462 1.003 ## sdB 36.782 25.641 0.481 20.352 35.033 49.224 95.307 1.006 ## sd_eps 53.651 9.262 38.914 47.164 52.396 58.987 75.790 1.001 ## deviance 323.666 6.500 314.954 318.451 321.889 328.567 336.638 1.002 ## n.eff ## B[1] 3000 ## B[2] 3000 ## mu 3000 ## sdB 450 ## sd_eps 3000 ## deviance 1600 ## ## For each parameter, n.eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor (at convergence, Rhat=1). ## ## DIC info (using the rule: pV = var(deviance)/2) ## pV = 21.1 and DIC = 344.8 ## DIC is an estimate of expected predictive error (lower deviance is better). traceplot(jagsfit) require(lattice) ## Loading required package: lattice jagsfit.mcmc &lt;- as.mcmc(jagsfit) densityplot(jagsfit.mcmc) As of this writing, there are some aspects of the output that I don’t understand. The traceplots show that one chain starts far away from the region of high posterior density and then rapidly converges to it. This behavior seems to persist regardless of how long the burn-in period is, which doesn’t make sense to me. I also do not understand why densityplot produces a 6-by-1 stack of panels instead of a more useful layout. One especially appealing aspect of analyzing this model from a Bayesian perspective is that there is no awkwardness in analyzing the posterior distributions of the latent variables, namely, the batch-specific means. From the frequentist perspective, it is somewhat awkward (though not prohibitively so) to define the conditional modes (BLUPs). We also lack straightforward methods for quantifying the uncertainty in these conditional modes. From the Bayesian viewpoint, this awkwardness disappears, because the distinction between model parameters and latent variables vanishes. Both are simply unobserved quantities. Consequently, it is natural to summarize our posterior knowledge about the latent variables by their (marginal) posterior distributions, in the same way that we use marginal posteriors to summarize our inferences about the model parameters. 6.3 Negative within-group correlations The hierarchical model formulation, regardless of whether analyzed from a frequentist or Bayesian perspective, is just a model. All models are wrong, but some models are useful. One can encounter grouped data where the hierarchical formulation is not useful. To illustrate, we consider the Dyestuff2 data provided in lme4. These are synthetic (fake) data that have been created by Box &amp; Tiao (1973) for the sake of illustration. The Dyestuff2 data have the same structure as the original Dyestuff data, that is, 5 observations from each of 6 batches. data(Dyestuff2) summary(Dyestuff2) ## Batch Yield ## A:5 Min. :-0.892 ## B:5 1st Qu.: 2.765 ## C:5 Median : 5.365 ## D:5 Mean : 5.666 ## E:5 3rd Qu.: 8.151 ## F:5 Max. :13.434 with(Dyestuff2, stripchart(Yield ~ Batch, pch = 16)) We’ll begin by fitting a GLS model that includes a within-batch correlation: fm6 &lt;- gls(Yield ~ 1, data = Dyestuff2, correlation = corCompSymm(form = ~ 1 | Batch)) summary(fm6) ## Generalized least squares fit by REML ## Model: Yield ~ 1 ## Data: Dyestuff2 ## AIC BIC logLik ## 167.2092 171.3111 -80.60461 ## ## Correlation Structure: Compound symmetry ## Formula: ~1 | Batch ## Parameter estimate(s): ## Rho ## -0.0970284 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 5.6656 0.5271409 10.74779 0 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.77661357 -0.78584319 -0.08143986 0.67335540 2.10464878 ## ## Residual standard error: 3.691067 ## Degrees of freedom: 30 total; 29 residual Notice that the within-batch correlation is negative. What does this imply about the structure of the data? Let’s have a look at a hierarchical model, fit with lmer. fm6 &lt;- lme4::lmer(Yield ~ 1 + (1 | Batch), data = Dyestuff2) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(fm6) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Yield ~ 1 + (1 | Batch) ## Data: Dyestuff2 ## ## REML criterion at convergence: 161.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.7648 -0.7806 -0.0809 0.6689 2.0907 ## ## Random effects: ## Groups Name Variance Std.Dev. ## Batch (Intercept) 0.00 0.000 ## Residual 13.81 3.716 ## Number of obs: 30, groups: Batch, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 5.6656 0.6784 8.352 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) The estimate of the among-batch variance, \\(\\sigma^2_B\\), is 0. The warning generated by R tells us that our fit is singular: one of the estimated parameters lies on the boundary of its possible values. To quote Bates (2012+, using our notation): “An estimate of 0 for \\(\\sigma_B\\) does not mean that there is no variation between the groups. Indeed, … there is some small amount of variability between the groups. The estimate, \\(\\hat{\\sigma}_B=0\\), simply indicates that the level of ‘between-group’ variability is not sufficient to warrant incorporating random effects in the model. “The important point … is that we must allow for the estimates of variance components to be zero. We describe such a model as being degenerate, in the sense that it corresponds to a linear model in which we have removed the randdom effects associated with Batch. Degenerate models can and do occur in practice. Eveen when the final fitted model is not degenerate, we must allow for such models when determining the parameter estimates through numerical optimization.” Can you think of any ecological mechanisms that might give rise to negative within-group correlations? 6.4 Random coefficient models: RIKZ data So-called random coefficient models are popular modeling structures in ecology. Random-coefficient models are useful when data are grouped, like the Dyestuff data. However, unlike the Dyestuff data, random-coefficient models refer to scenarios where the statistical model that we want to entertain within each group is more complex than just a simple random sample with a group-specific mean. To illustrate random-coefficient models, we will consider the RIKZ data from Zuur et al. (2009). These data were first analyzed in an earlier textbook (Zuur, Ieno, and Smith (2007)). Zuur et al. (2009) (p. 101) describe the data as follows: “Zuur, Ieno, and Smith (2007) used marine benthic data from nine inter-tidal areas along the Dutch coast. The data were collected by the Dutch institute RIKZ in the summer of 2002. In each inter-tidal area (denoted by ‘beach’), five samples were taken, and the macro-fauna and abiotic variables were measured. … The underlying question for these data is whether there is a relationship between species richness, exposure, and NAP (the height of a sampling station compared to mean tidal level). Exposure is an index composed of the following elements: wave action, length of the surf zone, slope, grain size, and the depth of the anaerobic layer.” In other words, there are 9 beaches, and 5 samples from each beach. The response, measured at each sample, is the macrofaunal species richness. There are two covariates: NAP, which is a sample-level covariate, and exposure, which is a beach-level covariate. Because species richness is a count variable and includes the occasional zero (and because we have not yet discussed hierarchical models for non-Gaussian responses) we will use the square-root of richness as a variance-stabilizing transformation. Using the square root of species richness as the response has the added benefit of making our analysis different from the analysis in Zuur et al. (2009). We are going to analyze these data exhaustively, considering various approaches for their analysis and comparing the pros and cons. In our first pass, we will ignore the exposure covariate, and seek only to model the relationship between species richness and NAP. Once that analysis is complete, we will circle back and consider how the analysis changes when we consider the beach-level covariate as well. Like all data from Zuur et al. (2009), the data are available for download from the book’s associated webpage. We will read in the data and do some housekeeping first. require(lme4) require(lmerTest) rikz &lt;- read.table(&quot;data/RIKZ.txt&quot;, head = T) with(rikz, plot(Richness ~ NAP, pch = Beach)) # raw response; note the non-constant variance with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach)) # transformation stabilizes the variance legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) # change the Beach variable to a factor # would have been better to code the beaches as b1, b2, ... rikz$fBeach &lt;- as.factor(rikz$Beach) 6.4.1 Analysis without beach-level covariate 6.4.1.1 Using fixed effects for among-beach differences To develop some notation for modeling, let \\(i=1, \\ldots, 9\\) index the different beaches, let \\(j = 1, \\ldots, 5\\) index the different samples at each beach, let \\(y_{ij}\\) be the square root of the species richness at sample \\(j\\) at beach \\(i\\), and let \\(x_{ij}\\) be the NAP covariate at sample \\(j\\) at beach \\(i\\). In a standard linear models course, we would identify this as a two-factor design with a categorical factor (the beaches) and a numerical factor (NAP). Suppose we wish to characterize the relationship between NAP and (the square root of) species richness, while controlling for differences among the beaches. To do so, we might entertain the additive model \\[\\begin{align*} y_{ij} &amp; = a_i + b x_{ij} + \\varepsilon_{ij} \\\\ \\varepsilon_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma_\\varepsilon^2). \\end{align*}\\] As usual, there are several different ways in which we could write the same model. We might instead write \\[\\begin{align*} y_{ij} &amp; = \\mu_{ij} + \\varepsilon_{ij} \\\\ \\mu_{ij} &amp; = a_i + b x_{ij} \\\\ \\varepsilon_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma_\\varepsilon^2) \\end{align*}\\] to emphasize that the mean of each observation (\\(\\mu_{ij}\\)) is equal to the sum of a beach-level intercept (\\(a_i\\)) and a common slope times the NAP value (\\(b x_{ij}\\)). Alternatively, we might write \\[\\begin{equation} y_{ij} \\sim \\mathcal{N}(a_i + b x_{ij}, \\sigma_\\varepsilon^2). \\end{equation}\\] In any case, the model states that each observation is drawn independently from a Gaussian distribution. The fitted value (or mean) for each datum is determined by a regression line with beach-specific intercepts and a common slope. (The additive model has the same structure has a parallel-lines ANCOVA.) The error variance is the same for all observations. Let’s fit the model and see what it yields. fm0 &lt;- lm(sqrt(Richness) ~ fBeach + NAP, data = rikz) summary(fm0) ## ## Call: ## lm(formula = sqrt(Richness) ~ fBeach + NAP, data = rikz) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.58544 -0.28653 -0.06544 0.23657 1.69043 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.99457 0.26711 11.211 3.92e-13 *** ## fBeach2 0.61544 0.37909 1.623 0.11346 ## fBeach3 -1.21158 0.37491 -3.232 0.00268 ** ## fBeach4 -1.13596 0.38510 -2.950 0.00564 ** ## fBeach5 -0.32863 0.38648 -0.850 0.40093 ## fBeach6 -0.90219 0.37835 -2.385 0.02265 * ## fBeach7 -0.98741 0.39419 -2.505 0.01705 * ## fBeach8 -0.89080 0.38392 -2.320 0.02628 * ## fBeach9 -0.79350 0.38561 -2.058 0.04712 * ## NAP -0.66410 0.09655 -6.878 5.49e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5882 on 35 degrees of freedom ## Multiple R-squared: 0.7596, Adjusted R-squared: 0.6978 ## F-statistic: 12.29 on 9 and 35 DF, p-value: 1.744e-08 This analysis gives us an estimate for the common slope (-0.664) and a basis to draw inferences about this slope. For example, we can get a confidence interval in the usual way: confint(fm0, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 2.4523092 3.53682298 ## fBeach2 -0.1541506 1.38502150 ## fBeach3 -1.9726894 -0.45047493 ## fBeach4 -1.9177509 -0.35416521 ## fBeach5 -1.1132200 0.45596483 ## fBeach6 -1.6702860 -0.13408870 ## fBeach7 -1.7876580 -0.18716421 ## fBeach8 -1.6702073 -0.11139365 ## fBeach9 -1.5763374 -0.01066776 ## NAP -0.8601180 -0.46808725 We can also use the anova command to test for whether the differences among the beaches are statistically significant, after accounting for the effect of NAP. Note that such a test makes sense in this case, because we have used fixed-effects to capture the differences among the beaches, and thus can carry out inference about these 9 beaches specifically. anova(fm0) ## Analysis of Variance Table ## ## Response: sqrt(Richness) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fBeach 8 21.900 2.7375 7.9109 5.119e-06 *** ## NAP 1 16.370 16.3701 47.3073 5.492e-08 *** ## Residuals 35 12.111 0.3460 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Finally, we can visualize the model by plotting each of the beach-specific fits. We’ll use the R trick of re-fitting the model without the intercept to obtain the beach-specific intercepts directly as model parameters, instead of having to back out those intercepts from the contrasts. fm0.temp &lt;- lm(sqrt(Richness) ~ fBeach + NAP - 1, data = rikz) coef(fm0.temp) ## fBeach1 fBeach2 fBeach3 fBeach4 fBeach5 fBeach6 fBeach7 ## 2.9945661 3.6100015 1.7829839 1.8586080 2.6659385 2.0923787 2.0071550 ## fBeach8 fBeach9 NAP ## 2.1037656 2.2010635 -0.6641026 For later comparison, we’ll make a note of the intercept for beach 1, which in this case is 2.995. Now we’ll proceed to make the plot. with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach, main = &quot;Fixed-effects fit, additive model&quot;)) legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) # add a line for each beach b &lt;- coef(fm0)[&quot;NAP&quot;] for(i in 1:9){ abline(a = coef(fm0.temp)[i], b = b, col = &quot;red&quot;, lty = &quot;dotted&quot;) } Continuing with the fixed-effects analysis, we might also consider a model in which the relationship between NAP and species richness varies among beaches. In other words, we might fit a model with a beach-by-NAP interaction. This model is \\[\\begin{align*} y_{ij} &amp; \\sim \\mathcal{N}(a_i + b_i x_{ij}, \\sigma_\\varepsilon^2). \\end{align*}\\] We fit this model in the usual way: fm0b &lt;- lm(sqrt(Richness) ~ fBeach * NAP, data = rikz) summary(fm0b) ## ## Call: ## lm(formula = sqrt(Richness) ~ fBeach * NAP, data = rikz) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.84831 -0.16080 -0.03091 0.14909 0.98737 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.28835 0.24259 13.555 1.45e-13 *** ## fBeach2 0.30239 0.32158 0.940 0.355394 ## fBeach3 -1.50542 0.31542 -4.773 5.61e-05 *** ## fBeach4 -1.56073 0.33715 -4.629 8.25e-05 *** ## fBeach5 0.11078 0.35432 0.313 0.756947 ## fBeach6 -1.25466 0.31812 -3.944 0.000513 *** ## fBeach7 -1.39537 0.41116 -3.394 0.002144 ** ## fBeach8 -1.17907 0.32697 -3.606 0.001242 ** ## fBeach9 -0.85912 0.33879 -2.536 0.017314 * ## NAP -0.05077 0.28172 -0.180 0.858319 ## fBeach2:NAP -0.54313 0.36261 -1.498 0.145780 ## fBeach3:NAP -0.47943 0.37104 -1.292 0.207267 ## fBeach4:NAP -0.37552 0.35511 -1.057 0.299666 ## fBeach5:NAP -1.82561 0.38805 -4.705 6.74e-05 *** ## fBeach6:NAP -0.36229 0.33258 -1.089 0.285636 ## fBeach7:NAP -0.48212 0.41379 -1.165 0.254155 ## fBeach8:NAP -0.62429 0.32975 -1.893 0.069089 . ## fBeach9:NAP -1.01278 0.35527 -2.851 0.008256 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4508 on 27 degrees of freedom ## Multiple R-squared: 0.8911, Adjusted R-squared: 0.8225 ## F-statistic: 13 on 17 and 27 DF, p-value: 7.079e-09 We can test for whether the differences among the slopes for the beaches are statistically significant with the usual \\(F\\)-test: anova(fm0, fm0b) ## Analysis of Variance Table ## ## Model 1: sqrt(Richness) ~ fBeach + NAP ## Model 2: sqrt(Richness) ~ fBeach * NAP ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 35 12.1113 ## 2 27 5.4865 8 6.6248 4.0753 0.002742 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We’ll save the model of the beach-specific intercepts and slopes, and use them to visualize the fit. We’ll do the usual trick of refitting the model without the intercept to make it easy to extract the beach-level intercepts and slopes fm0b.temp &lt;- lm(sqrt(Richness) ~ fBeach + fBeach:NAP - 1, data = rikz) (fixed.params &lt;- data.frame(beach = 1:9, intercept = coef(fm0b.temp)[1:9], slope = coef(fm0b.temp)[10:18])) ## beach intercept slope ## fBeach1 1 3.288351 -0.05077384 ## fBeach2 2 3.590739 -0.59390495 ## fBeach3 3 1.782930 -0.53019915 ## fBeach4 4 1.727622 -0.42629196 ## fBeach5 5 3.399128 -1.87638770 ## fBeach6 6 2.033687 -0.41306694 ## fBeach7 7 1.892979 -0.53289652 ## fBeach8 8 2.109276 -0.67506643 ## fBeach9 9 2.429231 -1.06355553 row.names(fixed.params) &lt;- NULL with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach, main = &quot;Fixed-effects fit, with beach-NAP interaction&quot;)) legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) for(i in 1:9){ abline(a = fixed.params$intercept[i], b = fixed.params$slope[i], col = &quot;red&quot;, lty = &quot;dotted&quot;) } 6.4.1.2 Using random-effects for among-beach differences Now let’s use random effects to capture the differences among beaches. A random effect is appropriate if we want to treat these beaches as a representative sample from a larger collection of beaches, and draw inferences about this larger collection. We’ll start with the additive model again, so that the random beach effect only affects the intercept. Using the notational style of mixed modeling, we could write our model as \\[\\begin{align*} y_{ij} &amp; \\sim \\mathcal{N}(A_i + b x_{ij}, \\sigma_\\varepsilon^2) \\\\ A_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(a, \\sigma_a^2). \\end{align*}\\] Here, the \\(A_i\\)’s are the random beach-level intercept. We’ll fit the model using lmerTest::lmer. fm1 &lt;- lmerTest::lmer(sqrt(Richness) ~ 1 + NAP + (1 | fBeach), data = rikz) summary(fm1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) ## Data: rikz ## ## REML criterion at convergence: 97.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5693 -0.4286 -0.1869 0.3230 2.9399 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.2957 0.5438 ## Residual 0.3460 0.5882 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.37424 0.20405 8.36034 11.635 1.88e-06 *** ## NAP -0.68063 0.09501 37.15971 -7.163 1.68e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## NAP -0.162 Note that the random-intercepts model includes the parameter \\(a\\), which is the average intercept across the population of beaches. This is the parameter listed as “(Intercept)” in the fixed-effects portion of the model output above. The model with fixed-effect for the beach-level intercepts has no such parameter. This makes sense, because that model did not envision a larger population of beaches. We can go further by finding the conditional modes of the intercepts for each beach. (beach.conditional.modes &lt;- (fixef(fm1)[&quot;(Intercept)&quot;] + ranef(fm1)$fBeach$`(Intercept)`)) ## [1] 2.870511 3.379324 1.895117 1.963771 2.618721 2.148964 2.088426 2.161791 ## [9] 2.241556 In particular, note the conditional mode for the intercept for beach 1, which is 2.871. This conditional mode is shrunken back towards the average intercept, compared to the intercept for this beach in the fixed-effects model. To visualize the model, we will again make a plot that shows the conditional modes of the fit for each beach. We can also add a line for the average relationship across the population of beaches. with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach, main = &quot;Random intercepts fit&quot;)) legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) a &lt;- coef(summary(fm1))[1, 1] b &lt;- coef(summary(fm1))[2, 1] abline(a = a, b = b, col = &quot;red&quot;, lwd = 2) # make a plot with a line for each beach for(i in 1:9){ abline(a = beach.conditional.modes[i], b = b, col = &quot;red&quot;, lty = &quot;dotted&quot;) } Though it’s subtle, notice again that the implied fits for each beach have been shrunken back to the overall mean. Now let’s consider a model that includes both separate intercepts and slopes for each beach, while continuing to model the among-beach differences in both with random effects. In other words, we’ll fit a “random coefficients” model with random intercepts and slopes for each beach. We have two options here. Either we can allow for the random intercept and slope for each beach to be a draw from a bivariate normal distribution with possible correlations, or we can insist that the random intercepts and slopes are independent. To write the first model in mixed-model notation, we might write \\[\\begin{align*} y_{ij} &amp; \\sim \\mathcal{N}(A_i + B_i x_{ij}, \\sigma_\\varepsilon^2) \\\\ \\left(\\begin{array}{c} A \\\\ B \\end{array} \\right)_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}_2 \\left(\\left(\\begin{array}{c} a \\\\ b \\end{array} \\right), \\left(\\begin{array}{cc} \\sigma_A^2 &amp; \\sigma_{AB} \\\\ \\sigma_{AB} &amp; \\sigma_B^2 \\end{array} \\right) \\right). \\end{align*}\\] To fit the model using lmerTest::lmer, we use fm2 &lt;- lmerTest::lmer(sqrt(Richness) ~ 1 + NAP + (1 + NAP | fBeach), data = rikz) summary(fm2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ 1 + NAP + (1 + NAP | fBeach) ## Data: rikz ## ## REML criterion at convergence: 92.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6245 -0.4430 -0.1095 0.3023 2.1610 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## fBeach (Intercept) 0.4389 0.6625 ## NAP 0.1582 0.3978 -0.41 ## Residual 0.2159 0.4647 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.4369 0.2348 7.9127 10.379 6.97e-06 *** ## NAP -0.7026 0.1543 6.7971 -4.552 0.00283 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## NAP -0.390 Because this model nests the random-intercept model, we can compare the two directly with a LRT: anova(fm1, fm2) ## refitting model(s) with ML (instead of REML) ## Data: rikz ## Models: ## fm1: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) ## fm2: sqrt(Richness) ~ 1 + NAP + (1 + NAP | fBeach) ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## fm1 4 100.83 108.06 -46.416 92.833 ## fm2 6 101.26 112.10 -44.630 89.259 3.5736 2 0.1675 Interestingly, both the LRT and ANOVA suggest that the random-coefficients model does not provide a statistically significant improvement over the random-intercepts model. In other words, we cannot reject the null hypothesis that \\(\\sigma_B^2 = 0\\). Alternatively, we could try a model with independent random effects for intercepts and slopes. This model is \\[\\begin{align*} y_{ij} &amp; \\sim \\mathcal{N}(A_i + B_i x_{ij}, \\sigma_\\varepsilon^2) \\\\ A_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(a, \\sigma^2_A) \\\\ B_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(b, \\sigma^2_B). \\end{align*}\\] To fit it in R, we use fm3 &lt;- lmerTest::lmer(sqrt(Richness) ~ 1 + NAP + (1 | fBeach) + (0 + NAP | fBeach), data = rikz) summary(fm3) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) + (0 + NAP | fBeach) ## Data: rikz ## ## REML criterion at convergence: 93.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.5841 -0.4070 -0.1042 0.2804 2.2006 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.4247 0.6517 ## fBeach.1 NAP 0.1489 0.3858 ## Residual 0.2168 0.4657 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.4413 0.2314 7.8858 10.552 6.32e-06 *** ## NAP -0.6912 0.1510 6.6684 -4.576 0.00289 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## NAP -0.061 Because each of these random-effect models are nested within one another, we can compare them directly with LRTs: anova(fm1, fm3, fm2) ## refitting model(s) with ML (instead of REML) ## Data: rikz ## Models: ## fm1: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) ## fm3: sqrt(Richness) ~ 1 + NAP + (1 | fBeach) + (0 + NAP | fBeach) ## fm2: sqrt(Richness) ~ 1 + NAP + (1 + NAP | fBeach) ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## fm1 4 100.83 108.06 -46.416 92.833 ## fm3 5 100.16 109.19 -45.078 90.156 2.6767 1 0.1018 ## fm2 6 101.26 112.10 -44.630 89.259 0.8969 1 0.3436 Here we see a conflict between AIC and the LRT. AIC favors the model with random but independent intercepts and slopes, whereas the LRT continues to suggest that we cannot reject the null hypothesis that \\(\\sigma_B^2 = 0\\). Finally, let’s compare the conditional modes for the intercepts and slopes from model fm3 with the beach-specific intercept and slopes from the fixed-effects model. (conditional.modes &lt;- data.frame(beach = 1:9, intercept = fixef(fm3)[&quot;(Intercept)&quot;] + ranef(fm3)$fBeach$`(Intercept)`, slope = fixef(fm3)[&quot;NAP&quot;] + ranef(fm3)$fBeach$`NAP`)) ## beach intercept slope ## 1 1 3.091736 -0.3225870 ## 2 2 3.484038 -0.5930830 ## 3 3 1.843949 -0.5776709 ## 4 4 1.841492 -0.5218237 ## 5 5 3.066017 -1.4310799 ## 6 6 2.083390 -0.4693246 ## 7 7 2.032495 -0.6452486 ## 8 8 2.145453 -0.6869329 ## 9 9 2.383348 -0.9728640 Let’s make a scatterplot that compares the fixed-effect estimates to the conditional modes. par(mfrow = c(1, 2)) with(fixed.params, plot(slope ~ intercept, main = &quot;fixed-effects fit&quot;, pch = 1:9)) with(fixed.params, plot(slope ~ intercept, main = &quot;conditional modes&quot;, type = &quot;n&quot;)) with(conditional.modes, points(slope ~ intercept, pch = 1:9)) points(fixef(fm3)[1], fixef(fm3)[2], pch = 16, col = &quot;red&quot;, cex = 2) Note, again, that the conditional modes of the intercepts and slopes have shrunk (sometimes substantially) back towards the population means of each. The population means of the intercept and slope are shown by the red dot on the right-hand panel. Finally, we visualize the model by plotting beach-specific “fits”: par(mfrow = c(1, 1)) with(rikz, plot(sqrt(Richness) ~ NAP, pch = Beach, main = &quot;Random-coefficients fit&quot;)) legend(&quot;topright&quot;, leg = 1:9, pch = 1:9) for(i in 1:9){ abline(a = conditional.modes$intercept[i], b = conditional.modes$slope[i], col = &quot;red&quot;, lty = &quot;dotted&quot;) } abline(a = fixef(fm3)[1], b = fixef(fm3)[2], col = &quot;red&quot;, lwd = 2) 6.4.2 Adding a beach-level covariate Finally, we consider the effect of exposure, a beach-level covariate. Exposure is coded in the data set as a numerical predictor. However, there are only three unique values: 8, 10, and 11 (and only one beach has exposure level 8). We will follow Zuur et al. (2009) in treating exposure as a binary predictor, grouping the beaches with exposure levels 8 and 10 together as low exposure beaches. # create an &#39;exposure&#39; factor, with two levels: low and high with(rikz, table(fBeach, Exposure)) ## Exposure ## fBeach 8 10 11 ## 1 0 5 0 ## 2 5 0 0 ## 3 0 0 5 ## 4 0 0 5 ## 5 0 5 0 ## 6 0 0 5 ## 7 0 0 5 ## 8 0 5 0 ## 9 0 5 0 rikz$fExp &lt;- rikz$Exposure # make a new variable so that we can leave the original alone rikz$fExp[rikz$Exposure == 8] &lt;- 10 # assign a value of 10 to the lone beach with exposure = 8 rikz$fExp &lt;- as.factor(rikz$fExp) # make the new variable into a factor summary(rikz) ## Sample Richness Exposure NAP Beach ## Min. : 1 Min. : 0.000 Min. : 8.00 Min. :-1.3360 Min. :1 ## 1st Qu.:12 1st Qu.: 3.000 1st Qu.:10.00 1st Qu.:-0.3750 1st Qu.:3 ## Median :23 Median : 4.000 Median :10.00 Median : 0.1670 Median :5 ## Mean :23 Mean : 5.689 Mean :10.22 Mean : 0.3477 Mean :5 ## 3rd Qu.:34 3rd Qu.: 8.000 3rd Qu.:11.00 3rd Qu.: 1.1170 3rd Qu.:7 ## Max. :45 Max. :22.000 Max. :11.00 Max. : 2.2550 Max. :9 ## ## fBeach fExp ## 1 : 5 10:25 ## 2 : 5 11:20 ## 3 : 5 ## 4 : 5 ## 5 : 5 ## 6 : 5 ## (Other):15 We will consider a model in which we use separate distributions of random intercepts for the the low- and high-exposure beaches. To fit this model, we will need to embellish our notation. Now, let \\(i=1, 2\\) index the exposure level of the beaches. Let \\(j=1,\\ldots, n_i\\) index the replicate beaches within each exposure level. Let \\(k=1, \\ldots, 5\\) index the samples at each beach. Here we see an interesting consequence of our decision to model the differences among the beaches with either a fixed or random effect. In order to draw inferences about the effect of low vs. high exposure, we must use a random effect for the beach-to-beach differences. This makes sense when we reflect upon it. In order to draw inferences about low vs. high exposure, we have to envision separate populations of low and high exposure beaches, and construe the beaches in this experiment as two separate random samples from those populations. If instead we treat the beach-to-beach differences with fixed effects, then we are effectively saying that these are the only low- and high-exposure beaches that we care about. Therefore, in the fixed-effects formulation, these nine beaches are our two populations, and there is no inference to be done. As above, we will consider a series of three models with various specifications of the random effect. We will consider: A model with random intercepts for the beaches but common slopes within each exposure group A model with independent random intercepts and slopes for beaches within each exposure group. A model with random intercepts and slopes for beaches within each exposure group, and potential correlations between them Initially, we fit a model with richly specified fixed effects. In this case, this will mean that the average slope and intercept will vary between the exposure groups. These models can be written and fit as follows. The random-intercepts model is \\[\\begin{align*} y_{ijk} &amp; \\sim \\mathcal{N}(A_{ij} + b_i x_{ijk}, \\sigma_\\varepsilon^2)\\\\ A_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(a_i, \\sigma^2_A). \\end{align*}\\] We fit this model in R with the code fm4 &lt;- lmerTest::lmer(sqrt(Richness) ~ fExp * NAP + (1 | fBeach), data = rikz) summary(fm4) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) ## Data: rikz ## ## REML criterion at convergence: 89.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4532 -0.5066 -0.0280 0.3478 2.6221 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.1395 0.3735 ## Residual 0.3182 0.5641 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.7722 0.2047 7.3111 13.544 1.92e-06 *** ## fExp11 -0.9213 0.3096 7.5513 -2.976 0.0189 * ## NAP -0.8580 0.1207 37.7420 -7.110 1.82e-08 *** ## fExp11:NAP 0.3979 0.1818 37.1348 2.189 0.0350 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fExp11 NAP ## fExp11 -0.661 ## NAP -0.174 0.115 ## fExp11:NAP 0.115 -0.212 -0.664 The model with independent intercepts and slopes can be written as \\[\\begin{align*} y_{ijk} &amp; \\sim \\mathcal{N}(A_{ij} + B_{ij} x_{ijk}, \\sigma_\\varepsilon^2)\\\\ A_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(a_i, \\sigma^2_A) \\\\ B_{ij} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(b_i, \\sigma^2_B) . \\end{align*}\\] We fit this model with the code fm5 &lt;- lmerTest::lmer(sqrt(Richness) ~ fExp * NAP + (1 | fBeach) + (0 + NAP | fBeach), data = rikz) summary(fm5) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) + (0 + NAP | fBeach) ## Data: rikz ## ## REML criterion at convergence: 85.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.70750 -0.40036 -0.01429 0.32923 2.02319 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.1783 0.4222 ## fBeach.1 NAP 0.1476 0.3842 ## Residual 0.2122 0.4606 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.8962 0.2153 7.1463 13.450 2.46e-06 *** ## fExp11 -1.0394 0.3248 7.2622 -3.200 0.01434 * ## NAP -0.8618 0.2007 5.7464 -4.295 0.00565 ** ## fExp11:NAP 0.3904 0.3012 5.7584 1.296 0.24444 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fExp11 NAP ## fExp11 -0.663 ## NAP -0.062 0.041 ## fExp11:NAP 0.041 -0.090 -0.666 The model with possibly correlated random intercepts and slopes can be written as \\[\\begin{align*} y_{ijk} &amp; \\sim \\mathcal{N}(A_{ij} + B_{ij} x_{ijk}, \\sigma_\\varepsilon^2)\\\\ \\left(\\begin{array}{c} A \\\\ B \\end{array} \\right)_i &amp; \\sim \\mathcal{N}_2 \\left(\\left(\\begin{array}{c} a_i \\\\ b_i \\end{array} \\right), \\left(\\begin{array}{cc} \\sigma_A^2 &amp; \\sigma_{AB} \\\\ \\sigma_{AB} &amp; \\sigma_B^2 \\end{array} \\right) \\right). \\end{align*}\\] We fit the model in R as follows fm6 &lt;- lmerTest::lmer(sqrt(Richness) ~ fExp * NAP + (1 + NAP| fBeach), data = rikz) summary(fm6) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ fExp * NAP + (1 + NAP | fBeach) ## Data: rikz ## ## REML criterion at convergence: 85.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.70561 -0.39766 -0.01305 0.32821 2.02192 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## fBeach (Intercept) 0.1767 0.4203 ## NAP 0.1464 0.3826 0.05 ## Residual 0.2125 0.4610 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.8956 0.2146 6.9871 13.494 2.93e-06 *** ## fExp11 -1.0389 0.3237 7.0837 -3.210 0.01462 * ## NAP -0.8588 0.2001 5.6602 -4.292 0.00587 ** ## fExp11:NAP 0.3874 0.3003 5.6586 1.290 0.24724 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fExp11 NAP ## fExp11 -0.663 ## NAP -0.026 0.017 ## fExp11:NAP 0.017 -0.054 -0.666 Because these models form a series of nested models, we can use LRTs or AIC to find the most parsimonious fit. anova(fm4, fm5, fm6) ## refitting model(s) with ML (instead of REML) ## Data: rikz ## Models: ## fm4: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) ## fm5: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) + (0 + NAP | fBeach) ## fm6: sqrt(Richness) ~ fExp * NAP + (1 + NAP | fBeach) ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## fm4 6 94.275 105.11 -41.137 82.275 ## fm5 7 94.418 107.06 -40.209 80.418 1.8565 1 0.1730 ## fm6 8 96.373 110.83 -40.186 80.373 0.0454 1 0.8312 Both AIC and the LRT suggest that the model with only random intercepts provides the most parsimonious fit. Let’s take a closer look at that fit summary(fm4) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: sqrt(Richness) ~ fExp * NAP + (1 | fBeach) ## Data: rikz ## ## REML criterion at convergence: 89.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4532 -0.5066 -0.0280 0.3478 2.6221 ## ## Random effects: ## Groups Name Variance Std.Dev. ## fBeach (Intercept) 0.1395 0.3735 ## Residual 0.3182 0.5641 ## Number of obs: 45, groups: fBeach, 9 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.7722 0.2047 7.3111 13.544 1.92e-06 *** ## fExp11 -0.9213 0.3096 7.5513 -2.976 0.0189 * ## NAP -0.8580 0.1207 37.7420 -7.110 1.82e-08 *** ## fExp11:NAP 0.3979 0.1818 37.1348 2.189 0.0350 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) fExp11 NAP ## fExp11 -0.661 ## NAP -0.174 0.115 ## fExp11:NAP 0.115 -0.212 -0.664 We can analyze the fixed-effects as follows. Based on the contrasts R has used, the low exposure beaches are the baseline; thus the values for the “Intercept” and “NAP” coefficients give the intercept and slope for low-exposure beaches. The values of the “fExp11” and “fExp11:NAP” coefficients (resp.) give the differences of the intercepts and slopes (resp.) between the high vs. low exposure beaches. Thus the negative value of the “NAP” coefficient suggests that species richness declines as NAP increases at low-exposure beaches. The positive coefficient for the “fExp11:NAP” interaction suggests that species richness declines more gradually with increasing NAP at high-exposure beaches as compared to low-exposure beaches. Because we have analyzed the square-root transform of the response, it is hard to assign any biological meaning to the magnitudes of the coefficients. This is one downside of using a transformation to stabilize the variance in the response. We’ll visualize the model with a plot. a0 &lt;- fixef(fm4)[1] # mean intercept for low-exposure beaches b0 &lt;- fixef(fm4)[2] # mean slope for low-exposure beaches a1 &lt;- fixef(fm4)[3] # difference in mean intercepts for high vs low b1 &lt;- fixef(fm4)[4] # difference in mean slopes for high vs low c.mode &lt;- ranef(fm4)$fBeach low.beaches &lt;- c(1, 2, 5, 8, 9) # indices of the low-exposure beaches high.beaches &lt;- c(3, 4, 6, 7) # indices of the high-exposure beacues par(mfrow = c(1, 2)) # split the plot region with(rikz, plot(sqrt(Richness) ~ NAP, type = &quot;n&quot;, main = &quot;Exposure = 8 or 10&quot;)) # set up the axes with(subset(rikz, fExp == &quot;10&quot;), points(sqrt(Richness) ~ NAP, pch = Beach)) # plot points for low exposure beaches abline(a = a0, b = b0, col = &quot;red&quot;, lwd = 2) # add the average line for low-exposure beaches for (i in 1:length(low.beaches)) { abline(a = a0 + c.mode[low.beaches[i], 1], b = b0, col = &quot;red&quot;, lty = &quot;dotted&quot;) } # Repeat for high exposure beaches with(rikz, plot(sqrt(Richness) ~ NAP, type = &quot;n&quot;, main = &quot;Exposure = 11&quot;)) # set up the axes with(subset(rikz, fExp == &quot;11&quot;), points(sqrt(Richness) ~ NAP, pch = Beach)) # plot points for low exposure beaches abline(a = a0 + a1, b = b0 + b1, col = &quot;blue&quot;, lwd = 2) # add the average line for low-exposure beaches for (i in 1:length(high.beaches)) { abline(a = a0 + a1 + c.mode[high.beaches[i], 1], b = b0 + b1, col = &quot;blue&quot;, lty = &quot;dotted&quot;) } 6.5 Nested and crossed random effects 6.5.1 Nested random effects A major advantage of the lme4::lmer software is that it allows nested and crossed random effects. We will look first at an example of nested random effects. These data come from a study by Yates (1935), as reported in Venables &amp; Ripley (Modern Applied Statistics with S, 4e, 2002). They are found in the MASS library as the oats data. The description in the help documentation states: The yield of oats from a split-plot field trial using three varieties and four levels of manurial treatment. The experiment was laid out in 6 blocks of 3 main plots, each split into 4 sub-plots. The varieties were applied to the main plots and the manurial treatments to the sub-plots. Let’s first take a look at the data. rm(list = ls()) library(MASS) require(lme4) require(lmerTest) data(oats) summary(oats) ## B V N Y ## I :12 Golden.rain:24 0.0cwt:18 Min. : 53.0 ## II :12 Marvellous :24 0.2cwt:18 1st Qu.: 86.0 ## III:12 Victory :24 0.4cwt:18 Median :102.5 ## IV :12 0.6cwt:18 Mean :104.0 ## V :12 3rd Qu.:121.2 ## VI :12 Max. :174.0 The variables are (respectively) [B]lock, [V]ariety, [N]itrogen (the manure), and the [Y]ield, in units of 1/4lbs, according to the help documentation. To develop notation, let \\(i = 1, \\ldots, 3\\) index the three varieties, let \\(j = 1, \\ldots, 4\\) index the four manure treatments, and let \\(k = 1, \\ldots, 6\\) index the blocks. We wish to entertain the usual model for a split-plot design with a blocking factor at the whole-plot level: \\[\\begin{align} y_{ijk} &amp; \\sim \\mathcal{N}(\\mu_{ij} + B_k + W_{ik}, \\sigma^2_\\varepsilon) \\\\ B_k &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2_B) \\\\ W_{ik} &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2_W). \\end{align}\\] Here, \\(\\mu_{ij}\\) is the average response for the combination of variety \\(i\\) and manure treatment \\(j\\); the \\(B_k\\) are the random effects for the blocks, the \\(W_{ik}\\) are the whole-plot errors, and the \\(\\varepsilon_{ijk}\\) are the split-plot (observation-level) errors. We proceed to fit the model using lmerTest::lmer. A key to the coding here is to notice that each combination of block and variety uniquely specifies one of the 18 whole plots. (In other words, variety is not replicated within the blocks.) Therefore, we can code the whole-plot random effect as (1 | B : V), which creats a random effect for each unique combination of block and variety. The rest of the model coding is straightforward. fm1 &lt;- lmerTest::lmer(Y ~ V * N + (1 | B) + (1 | B : V), data = oats) # for nested random effects, lmer provides the coding shortcut fm1a &lt;- lmerTest::lmer(Y ~ V * N + (1 | B / V), data = oats) summary(fm1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Y ~ V * N + (1 | B) + (1 | B:V) ## Data: oats ## ## REML criterion at convergence: 529 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.81301 -0.56145 0.01758 0.63865 1.57034 ## ## Random effects: ## Groups Name Variance Std.Dev. ## B:V (Intercept) 106.1 10.30 ## B (Intercept) 214.5 14.65 ## Residual 177.1 13.31 ## Number of obs: 72, groups: B:V, 18; B, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 80.0000 9.1070 16.0816 8.784 1.55e-07 *** ## VMarvellous 6.6667 9.7150 30.2308 0.686 0.4978 ## VVictory -8.5000 9.7150 30.2308 -0.875 0.3885 ## N0.2cwt 18.5000 7.6829 45.0000 2.408 0.0202 * ## N0.4cwt 34.6667 7.6829 45.0000 4.512 4.58e-05 *** ## N0.6cwt 44.8333 7.6829 45.0000 5.835 5.48e-07 *** ## VMarvellous:N0.2cwt 3.3333 10.8653 45.0000 0.307 0.7604 ## VVictory:N0.2cwt -0.3333 10.8653 45.0000 -0.031 0.9757 ## VMarvellous:N0.4cwt -4.1667 10.8653 45.0000 -0.383 0.7032 ## VVictory:N0.4cwt 4.6667 10.8653 45.0000 0.430 0.6696 ## VMarvellous:N0.6cwt -4.6667 10.8653 45.0000 -0.430 0.6696 ## VVictory:N0.6cwt 2.1667 10.8653 45.0000 0.199 0.8428 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) VMrvll VVctry N0.2cw N0.4cw N0.6cw VM:N0.2 VV:N0.2 VM:N0.4 ## VMarvellous -0.533 ## VVictory -0.533 0.500 ## N0.2cwt -0.422 0.395 0.395 ## N0.4cwt -0.422 0.395 0.395 0.500 ## N0.6cwt -0.422 0.395 0.395 0.500 0.500 ## VMrvll:N0.2 0.298 -0.559 -0.280 -0.707 -0.354 -0.354 ## VVctry:N0.2 0.298 -0.280 -0.559 -0.707 -0.354 -0.354 0.500 ## VMrvll:N0.4 0.298 -0.559 -0.280 -0.354 -0.707 -0.354 0.500 0.250 ## VVctry:N0.4 0.298 -0.280 -0.559 -0.354 -0.707 -0.354 0.250 0.500 0.500 ## VMrvll:N0.6 0.298 -0.559 -0.280 -0.354 -0.354 -0.707 0.500 0.250 0.500 ## VVctry:N0.6 0.298 -0.280 -0.559 -0.354 -0.354 -0.707 0.250 0.500 0.250 ## VV:N0.4 VM:N0.6 ## VMarvellous ## VVictory ## N0.2cwt ## N0.4cwt ## N0.6cwt ## VMrvll:N0.2 ## VVctry:N0.2 ## VMrvll:N0.4 ## VVctry:N0.4 ## VMrvll:N0.6 0.250 ## VVctry:N0.6 0.500 0.500 anova(fm1) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## V 526.1 263.0 2 10 1.4853 0.2724 ## N 20020.5 6673.5 3 45 37.6857 2.458e-12 *** ## V:N 321.7 53.6 6 45 0.3028 0.9322 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We proceed to analyze the fixed effects in the usual fashion, noting first that the variety-by-nitrogen interaction is not significant. We then proceed to inspect the \\(F\\)-tests of the fixed effects, and see that the marginal means for the manure treatment are significantly different, but there are no significant differences among the marginal means for the three varieties. Note that it would be incorrect to have coded the model as fm1.wrong &lt;- lmerTest::lmer(Y ~ V * N + (1 | B) + (1 | V), data = oats) as this would have treated the random effects for block and variety as crossed, not nested. To complete the analysis, we should notice that the levels of the nitrogen treatment correspond to equally spaced values of a numerical covariate. We can thus extract the polynomial trends by assigning polynomial contrasts to the nitrogen treatment. contrasts(oats$N) &lt;- contr.poly(n = 4) fm1 &lt;- lmerTest::lmer(Y ~ V * N + (1 | B) + (1 | B : V), data = oats) summary(fm1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: Y ~ V * N + (1 | B) + (1 | B:V) ## Data: oats ## ## REML criterion at convergence: 533.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.81301 -0.56145 0.01758 0.63865 1.57034 ## ## Random effects: ## Groups Name Variance Std.Dev. ## B:V (Intercept) 106.1 10.30 ## B (Intercept) 214.5 14.65 ## Residual 177.1 13.31 ## Number of obs: 72, groups: B:V, 18; B, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 104.5000 7.7976 8.8688 13.402 3.45e-07 *** ## VMarvellous 5.2917 7.0789 10.0000 0.748 0.472 ## VVictory -6.8750 7.0789 10.0000 -0.971 0.354 ## N.L 33.6901 5.4327 45.0000 6.201 1.57e-07 *** ## N.Q -4.1667 5.4327 45.0000 -0.767 0.447 ## N.C -0.8199 5.4327 45.0000 -0.151 0.881 ## VMarvellous:N.L -4.8075 7.6829 45.0000 -0.626 0.535 ## VVictory:N.L 2.5715 7.6829 45.0000 0.335 0.739 ## VMarvellous:N.Q -1.9167 7.6829 45.0000 -0.249 0.804 ## VVictory:N.Q -1.0833 7.6829 45.0000 -0.141 0.888 ## VMarvellous:N.C 3.9877 7.6829 45.0000 0.519 0.606 ## VVictory:N.C -2.8696 7.6829 45.0000 -0.374 0.711 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) VMrvll VVctry N.L N.Q N.C VM:N.L VV:N.L VM:N.Q ## VMarvellous -0.454 ## VVictory -0.454 0.500 ## N.L 0.000 0.000 0.000 ## N.Q 0.000 0.000 0.000 0.000 ## N.C 0.000 0.000 0.000 0.000 0.000 ## VMrvlls:N.L 0.000 0.000 0.000 -0.707 0.000 0.000 ## VVictry:N.L 0.000 0.000 0.000 -0.707 0.000 0.000 0.500 ## VMrvlls:N.Q 0.000 0.000 0.000 0.000 -0.707 0.000 0.000 0.000 ## VVictry:N.Q 0.000 0.000 0.000 0.000 -0.707 0.000 0.000 0.000 0.500 ## VMrvlls:N.C 0.000 0.000 0.000 0.000 0.000 -0.707 0.000 0.000 0.000 ## VVictry:N.C 0.000 0.000 0.000 0.000 0.000 -0.707 0.000 0.000 0.000 ## VV:N.Q VM:N.C ## VMarvellous ## VVictory ## N.L ## N.Q ## N.C ## VMrvlls:N.L ## VVictry:N.L ## VMrvlls:N.Q ## VVictry:N.Q ## VMrvlls:N.C 0.000 ## VVictry:N.C 0.000 0.500 We see that only the linear trend of the nitrogen treatment is significant. Let’s make a quick plot to visualize this effect. with(oats, plot(Y ~ N)) 6.5.2 Crossed random effects To illustrate crossed random effects, we will model players’ scores from the 2018 US Open golf tournament. This data set includes scores for all 136 players who competed in the tournament. All players participated in the first two days of the tournament. Players who had a sufficiently low (good) total score from those first two days qualified to compete in the next two days. Players who with a high (poor) total score from the first two days were disqualified or “cut” from the tournament. To develop notation, let \\(i=1, \\ldots, 136\\) index the players, and let \\(j = 1, \\ldots, 4\\) index the days. We seek to fit the model \\[\\begin{align} y_{ij} &amp; \\sim \\mathcal{N}(\\mu + A_i + B_j, \\sigma^2_\\varepsilon) \\\\ A_i &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2_A) \\\\ B_j &amp; \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2_B). \\end{align}\\] In this model, \\(\\mu\\) is the average score, \\(A_i\\) are the player-level random effects, \\(B_j\\) are the day-level random effects, and \\(\\varepsilon_{ij}\\) are the observation-level errors. Note that because each player played at most once in each day, there is no possibility to separate a possible player-by-day interaction (also a random effect) from the observation-level error. If the players had played multiple rounds in a given day, we could have tried to separate the player-by-day random effect from the observation-level error. We fit the model in lmerTest::lmer. rm(list = ls()) golf &lt;- read.table(&quot;data/golf.txt&quot;, head = T) fm1 &lt;- lmerTest::lmer(score ~ 1 + (1 | player) + (1 | round), data = golf) summary(fm1) # comparison of the std devs of the random effects is interesting ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: score ~ 1 + (1 | player) + (1 | round) ## Data: golf ## ## REML criterion at convergence: 2130.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6856 -0.6669 -0.0515 0.5825 4.5577 ## ## Random effects: ## Groups Name Variance Std.Dev. ## player (Intercept) 0.5059 0.7113 ## round (Intercept) 3.1513 1.7752 ## Residual 11.2507 3.3542 ## Number of obs: 400, groups: player, 136; round, 4 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 74.2631 0.9081 3.0093 81.78 3.9e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It is interesting to compare the standard deviations of the random effects. It is also interesting to use the profile function to see the asymmetry in the confidence intervals for these standard deviations. pp.golf &lt;- profile(fm1) confint(pp.golf) ## 2.5 % 97.5 % ## .sig01 0.0000000 1.551145 ## .sig02 0.8129721 3.837297 ## .sigma 3.0513011 3.666234 ## (Intercept) 72.2434040 76.265786 lattice::xyplot(pp.golf, absVal = TRUE) We can also extract the conditional modes for the players and rounds. player.modes &lt;- ranef(fm1)$player head(player.modes) ## (Intercept) ## Akiyoshi 0.3983034 ## Aphibarnrat -0.3069054 ## Axley -0.0142805 ## Babcock 0.1920115 ## Baddeley -0.1925652 ## Berger -0.4212456 (round.modes &lt;- ranef(fm1)$round) ## (Intercept) ## rd1 1.7211234 ## rd2 -0.9012137 ## rd3 1.1661848 ## rd4 -1.9860945 According to Wikipedia, on day 1 “conditions were extremely difficult as gusty winds hung around all day with sunny skies, making the course firm and fast.” This corresponds with the large conditional mode for round 1. Finally, it is interesting to compare the conditional modes for the players who qualified to play in rounds 3 and 4, vs. the players who were “cut”. player.stats &lt;- data.frame(name = row.names(player.modes), mode = player.modes[, 1], rds = with(golf, as.numeric(table(player)))) with(player.stats, stripchart(mode ~ as.factor(rds), method = &quot;jitter&quot;, ylab = &quot;rounds played&quot;, xlab = &quot;conditional mode&quot;, pch = 1)) Interestingly, some players who qualified to play in rounds 3 and 4 ended up with higher (worse) conditional modes than some of the players who were “cut”. We might infer that these players played above their abilities on days 1 and 2. Thanks to xkcd for the perspective. Bibliography Bates, Douglas M. 2012+. lme4: Mixed-Effects Modeling with R. Unpublished. Zuur, Alain F, Elena N Ieno, and Graham M Smith. 2007. Analysing Ecological Data. Springer. Zuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with R. Vol. 574. Springer. "],["generalized-linear-models.html", "Chapter 7 Generalized linear models 7.1 GLMs: The big picture 7.2 Poisson regression 7.3 Binary responses 7.4 Zero-adjusted models for count data 7.5 Generalized additive models (GAMs)", " Chapter 7 Generalized linear models 7.1 GLMs: The big picture Generalized linear models (GLMs)6 occupy a middle ground between general linear models (regression and ANOVA) and more all-encompassing maximum likelihood approaches. On the one hand, general linear models require strong assumptions about the distribution of the response (independence and normality). Those assumptions translate into lots of mathematical structure, and that structure in turn supports a powerful fitting algorithm that is easily automated by routines like lm. On the other hand, maximum likelihood is a more versatile approach that can in theory be applied to any parametric model one can propose. However, the versatility of likelihood comes with a cost, in that it requires the analyst to write their own model-fitting code from scratch for each model. GLMs relax the assumption of normality (of the response) while retaining enough shared mathematical structure to support a common scheme for fitting the models by maximum likelihood. This common model-fitting scheme can be automated and packaged into user-friendly statistical software that allows analysts to fit these models without having to re-code the likelihood maximization from scratch each time.7 The common model-fitting scheme is also reasonably robust, and it typically doesn’t require the user to specify starting values (although sometimes starting values are necessary; see the discussion in Wood (2017).) GLMs do not encompass all possible distributions for the response variable. Instead, GLMs can accommodate any response distribution in a group of probability distributions known as the “exponential family”.8 The exponential family of distributions includes many of the distributions that we encounter in practical data analysis, including Poisson, binomial, gamma, and beta distributions. The Gaussian distribution is included in the exponential family as well; thus we can think of GLMs as an extension of the general linear model. One notable distribution that is not part of the exponential family is the negative binomial distribution. That said, a few caveats are worth bearing in mind. First, GLMs have their own specialized jargon. Personally, I find it hard to remember the jargon and have to re-learn it each time I encounter a GLM. Surely, analysts who use GLMs routinely will find the specialized jargon helpful. However, if you only use GLMs occasionally, you may need to relearn the jargon each time as well. If this is cumbersome, remember that fitting a GLM is ultimately an exercise in maximizing a likelihood, and you can always write customized likelihood-maximization code instead. Others will find the Bayesian route appealing here, because in a Bayesian context it is trivially simple to choose a non-Gaussian distribution for the response. Moreover, whether that distribution belongs to the exponential family is of no consequence in a Bayesian setting. Secondly, the vast majority of GLMs encountered in practice are either logistic regression for binary responses and Poisson regression for count responses. (Every once in a blue moon, one might encounter a GLM with a Gamma- or beta-distributed response.) Most users are probably just as well served by learning logistic regression and Poisson regression as methods in their own right as they are by learning these methods as two particular instances of a broader class of GLMs. Finally, whatever approach you choose — generalized linear modeling, direct likelihood maximization, or the Bayesian analysis — non-Gaussian distributions do not lend themselves to handling correlations among responses as readily as Gaussian distributions do. Thus, with data that are both correlated and non-Gaussian, one faces a choice: cope with the correlations and assume a Gaussian response, assume independence and cope with the non-Gaussian response, or try to cope with both the correlations and the non-Gaussian response at once. The former two options lend themselves to straightforward methods: mixed models in the first instance, and GLMs in the second. The last option — trying to do both — is not impossible, but it is substantially harder, and will take us into the domain of generalized linear mixed models (GLMMs), in which these notes will culminate. 7.2 Poisson regression 7.2.1 Horse-kick data revisited To emphasize the connection between GLMs and likelihood approaches, we will take another look at fitting the horse-kick data in Chapters 1 and 2. To fit these data as an iid sample from a Poission distribution, we use a model that includes only an intercept and that uses an “indentity” link. We will discuss link functions later. horse &lt;- read.table(&quot;data/horse.txt&quot;, head = T) fm1 &lt;- glm(deaths ~ 1, family = poisson(link = &quot;identity&quot;), data = horse) summary(fm1) ## ## Call: ## glm(formula = deaths ~ 1, family = poisson(link = &quot;identity&quot;), ## data = horse) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.70 0.05 14 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 323.23 on 279 degrees of freedom ## Residual deviance: 323.23 on 279 degrees of freedom ## AIC: 630.31 ## ## Number of Fisher Scoring iterations: 3 Notice that the estimate of the intercept is exactly the same as the MLE of the intercept that we obtained from maximizing the likelihood directly in section 1.2. Notice also that the standard error is exactly the same as the value that we obtained by the quadratic approximation in section 2.3. 7.2.2 Elephant matings We will begin with an example of Poisson regression. These data are originally from Poole (1989), and were analyzed in Ramsey and Schafer (2002). They describe an observational study of 41 male elephants over 8 years at Amboseli National Park in Kenya. Each record in this data set gives the age of a male elephant at the beginning of a study and the number of successful matings for the elephant over the study’s duration. The number of matings is a count variable. Our goal is to characterize how the number of matings is related to the elephant’s age. We’ll start by fitting a model with the canonical log link. elephant &lt;- read.table(&quot;data/elephant.txt&quot;, head = T) head(elephant) ## age matings ## 1 27 0 ## 2 28 1 ## 3 28 1 ## 4 28 1 ## 5 28 3 ## 6 29 0 with(elephant, plot(matings ~ age)) fm1 &lt;- glm(matings ~ age, family = poisson(link = &quot;log&quot;), data = elephant) # log link is the default summary(fm1) ## ## Call: ## glm(formula = matings ~ age, family = poisson(link = &quot;log&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.58201 0.54462 -2.905 0.00368 ** ## age 0.06869 0.01375 4.997 5.81e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 51.012 on 39 degrees of freedom ## AIC: 156.46 ## ## Number of Fisher Scoring iterations: 5 Thus the so-called pseudo-\\(R^2\\) for the model with the log link is \\[ \\mathrm{pseudo}-R^2 = 1 - \\frac{51.012}{75.372} = 32.3\\% \\] We can visualize the fit by plotting a best-fitting line with a 95% confidence interval. Because the scale parameter is not estimated here, we will use a critical value from a standard normal distribution. Later, when we estimate the scale parameter based on data, we will use a critical value from a \\(t\\)-distribution instead. new.data &lt;- data.frame(age = seq(from = min(elephant$age), to = max(elephant$age), length = 100)) predict.fm1 &lt;- predict(fm1, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm1$fit, col = &quot;red&quot;) # add lines for standard errors lines(x = new.data$age, y = predict.fm1$fit - 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm1$fit + 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) While the canonical link is a natural starting point, we are free to try other link functions as well. Below, we try the identity link and plot the fit. fm2 &lt;- glm(matings ~ age, family = poisson(link = &quot;identity&quot;), data = elephant) summary(fm2) ## ## Call: ## glm(formula = matings ~ age, family = poisson(link = &quot;identity&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.55205 1.33916 -3.399 0.000676 *** ## age 0.20179 0.04023 5.016 5.29e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 50.058 on 39 degrees of freedom ## AIC: 155.5 ## ## Number of Fisher Scoring iterations: 5 predict.fm2 &lt;- predict(fm2, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm2$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm2$fit - 1.96 * predict.fm2$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm2$fit + 1.96 * predict.fm2$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) Note that the choice of the link function has a substantial impact on the shape of the fit. The canonical (log) link suggests that the average number of matings increases with age at an accelerating rate, while the identity link suggests that the average number of matings increases steadily with age. The AIC favors the identity link here. We can also have a look at the residuals to see if they suggest any model deficiencies. In general, we prefer the deviance residuals, so we will look at them. plot(x = elephant$age, y = residuals(fm2, type = &quot;deviance&quot;), xlab = &quot;age&quot;, ylab = &quot;Deviance residuals&quot;) abline(h = 0, lty = &quot;dashed&quot;) The residuals do not suggest any deficiency in the fit. For this fit, the residual deviance suggests a small amount of overdispersion. To be on the safe side, we can fit a quasi-Poisson model in which the scale (overdispersion) parameter is estimated from the data. Note that when we estimate the overdispersion parameter, the estimates of the model parameters do not change, but their standard errors increase. Consequently, the uncertainty in the fit increases as well. In this case, however, the increase is so slight that it is barely noticeable. fm3 &lt;- glm(matings ~ age, family = quasipoisson(link = &quot;identity&quot;), data = elephant) summary(fm3) ## ## Call: ## glm(formula = matings ~ age, family = quasipoisson(link = &quot;identity&quot;), ## data = elephant) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.55205 1.42164 -3.202 0.00272 ** ## age 0.20179 0.04271 4.725 2.97e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 1.126975) ## ## Null deviance: 75.372 on 40 degrees of freedom ## Residual deviance: 50.058 on 39 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 predict.fm3 &lt;- predict(fm3, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm3$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm3$fit + qt(0.025, df = 39) * predict.fm3$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm3$fit + qt(0.975, df = 39) * predict.fm3$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) As an alternative, we could fit a model that uses a negative binomial distribution for the response. Negative binomial distributions belong to the exponential family, so we can fit them using the GLM framework. However, the authors of glm did not include a negative binomial family in their initial code. Venables &amp; Ripley’s MASS package includes a program called glm.nb which is specifically designed for negative binomial responses. MASS::glm.nb uses the parameterization familiar to ecologists, although they use the parameter \\(\\theta\\) instead of \\(k\\). So, in their notation, if \\(y \\sim \\mathrm{NB}(\\mu, \\theta)\\), then \\(\\mathrm{Var}(y) = \\mu + \\mu^2/\\theta\\). require(MASS) ## Loading required package: MASS fm4 &lt;- glm.nb(matings ~ age, link = identity, data = elephant) summary(fm4) ## ## Call: ## glm.nb(formula = matings ~ age, data = elephant, link = identity, ## init.theta = 15.80269167) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.56939 1.45770 -3.135 0.00172 ** ## age 0.20232 0.04428 4.569 4.9e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(15.8027) family taken to be 1) ## ## Null deviance: 64.836 on 40 degrees of freedom ## Residual deviance: 43.214 on 39 degrees of freedom ## AIC: 156.87 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 15.8 ## Std. Err.: 23.0 ## ## 2 x log-likelihood: -150.872 predict.fm4 &lt;- predict(fm4, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(elephant, plot(matings ~ age)) lines(x = new.data$age, y = predict.fm4$fit, col = &quot;blue&quot;) lines(x = new.data$age, y = predict.fm4$fit + 1.96 * predict.fm4$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$age, y = predict.fm4$fit - 1.96 * predict.fm4$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) Notice that \\(\\hat{\\theta} = 15.8\\), again indicating that the extra-Poisson variation is mild. Notice also that the error bounds on the fitted curve are ever so slightly larger than the error bounds from the Poisson fit, and nearly identical to the error bounds from the quasi-Poisson fit. –&gt; 7.3 Binary responses We generally distinguish between two types of data with binary responses: Data in which each individual record is a separate a binary response, and data in which each record consists of a group of binary observations. The same methods can be used for either type of data. We will begin by studying a data set with individual binary responses, and then move to grouped binary responses. 7.3.1 Individual binary responses: TB in boar To illustrate individual binary data, we will use a data set analyzed by Zuur et al. (2009) in their Ch. 10. As explained by Zuur et al., these data describe the incidence of “tuberculosis-like lesions in wild boar Sus scrofa” in southern Spain, and were originally collected by Vicente et al. (2006). The potential explanatory variables in the data set include a measure of the animal’s size, it’s sex, and a grouping into one of four age classes. Preparatory work: boar &lt;- read.table(&quot;data/boar.txt&quot;, head = T) # remove incomplete records boar &lt;- na.omit(boar) # convert sex to a factor boar$SEX &lt;- as.factor(boar$SEX) names(boar) &lt;- c(&quot;tb&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;length&quot;) summary(boar) ## tb sex age length ## Min. :0.0000 1:206 Min. :1.000 Min. : 46.5 ## 1st Qu.:0.0000 2:288 1st Qu.:3.000 1st Qu.:107.0 ## Median :0.0000 Median :3.000 Median :122.0 ## Mean :0.4575 Mean :3.142 Mean :117.3 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:130.4 ## Max. :1.0000 Max. :4.000 Max. :165.0 We’ll fit the usual logistic regression model first, considering only the animal’s size as a predictor. Size in this case is a measure of the length of the animal, in cm. fm1 &lt;- glm(tb ~ length, family = binomial(link = &quot;logit&quot;), data = boar) summary(fm1) ## ## Call: ## glm(formula = tb ~ length, family = binomial(link = &quot;logit&quot;), ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.137107 0.695381 -5.949 2.69e-09 *** ## length 0.033531 0.005767 5.814 6.09e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 641.23 on 492 degrees of freedom ## AIC: 645.23 ## ## Number of Fisher Scoring iterations: 4 with(boar, plot(tb ~ length)) # add a line for the fitted probabilities of tb new.data &lt;- data.frame(length = seq(from = min(boar$length), to = max(boar$length), length = 100)) predict.fm1 &lt;- predict(fm1, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) lines(x = new.data$length, y = predict.fm1$fit, col = &quot;red&quot;) # add lines for standard errors # use critical value from z distribution here because # the scale parameter is not estimated lines(x = new.data$length, y = predict.fm1$fit - 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x = new.data$length, y = predict.fm1$fit + 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) Regression coefficients in logistic regression can be a bit hard to interpret. One interpretation flows from exponentiating the regression coefficient to obtain an odds ratio. For the boar data, the regression coefficient of 0.0335 corresponds to an odds ratio of \\(e^{0.0335}\\) = 1.034. This means that for two boars that differ by one cm in length, the larger boar’s odds of having a TB-like lesion will be 1.034 times the smaller boar’s odds of having such a lesion. Overdispersion is typically not an issue with individual binary response data. Nonetheless, the pseudo-\\(R^2\\) here is fairly low. We can try the probit and complementary log-log links to see if we obtain a better fit: # probit link fm1a &lt;- glm(tb ~ length, family = binomial(link = &quot;probit&quot;), data = boar) # complementary log-log link fm1b &lt;- glm(tb ~ length, family = binomial(link = &quot;cloglog&quot;), data = boar) AIC(fm1, fm1a, fm1b) ## df AIC ## fm1 2 645.2265 ## fm1a 2 645.2665 ## fm1b 2 645.6100 # make a plot to compare the fits with the different links predict.fm1a &lt;- predict(fm1a, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) predict.fm1b &lt;- predict(fm1b, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(boar, plot(tb ~ length)) lines(x = new.data$length, y = predict.fm1$fit, col = &quot;red&quot;, lwd = 2) lines(x = new.data$length, y = predict.fm1a$fit, col = &quot;blue&quot;, lwd = 2) lines(x = new.data$length, y = predict.fm1b$fit, col = &quot;forestgreen&quot;, lwd = 2) legend(&quot;left&quot;, leg = c(&quot;logit&quot;, &quot;probit&quot;, &quot;cloglog&quot;), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;forestgreen&quot;), pch = 16) The logit and probit links are nearly identical. The complementary log-log link differs slightly, but the logit link is AIC-best. Now we’ll try adding sex and age class as predictors. # fit a model with sex, age (as a categorical predictor) and their interaction fm2 &lt;- glm(tb ~ length + sex * as.factor(age), family = binomial, data = boar) summary(fm2) ## ## Call: ## glm(formula = tb ~ length + sex * as.factor(age), family = binomial, ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -16.55356 724.50177 -0.023 0.982 ## length 0.01840 0.01253 1.469 0.142 ## sex2 14.19739 724.50190 0.020 0.984 ## as.factor(age)2 13.83446 724.50169 0.019 0.985 ## as.factor(age)3 14.31136 724.50191 0.020 0.984 ## as.factor(age)4 14.68141 724.50219 0.020 0.984 ## sex2:as.factor(age)2 -14.53254 724.50204 -0.020 0.984 ## sex2:as.factor(age)3 -14.36861 724.50196 -0.020 0.984 ## sex2:as.factor(age)4 -14.53354 724.50196 -0.020 0.984 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 635.43 on 485 degrees of freedom ## AIC: 653.43 ## ## Number of Fisher Scoring iterations: 14 Notice the gigantic standard errors on some of the parameter estimates. These large standard errors indicate that the likelihood is flat, and that the MLEs cannot be found. In this case, the likelihood surface is flat because because none of the individuals with and are infected. We can see this by tallying the frequency of infection by sex and age class: with(boar, table(tb, age, sex)) ## , , sex = 1 ## ## age ## tb 1 2 3 4 ## 0 4 37 37 28 ## 1 0 14 34 52 ## ## , , sex = 2 ## ## age ## tb 1 2 3 4 ## 0 7 40 62 53 ## 1 2 11 48 65 Because none of the individuals with and are infected, the likelihood surface flattens out as the parameter corresponding to this demographic group becomes increasingly negative. (Formally speaking, the MLE of the log odds of infection for this group is \\(-\\infty\\), but no numerical optimization scheme will find this value.) This phenomenon is known as “complete separation”. Although it’s only the and group that is problematic here, in this case that is the group that is coded as the baseline, so the parameters for all the other groups are defined relative to it, and thus are problematic as well. There are several possible remedies here. The first is to try to reduce the number of parameters in the model, perhaps by eliminating the interaction between sex and age class. # fit a model with sex, age (as a categorical predictor) and their interaction fm3 &lt;- glm(tb ~ length + sex + as.factor(age), family = binomial, data = boar) summary(fm3) ## ## Call: ## glm(formula = tb ~ length + sex + as.factor(age), family = binomial, ## data = boar) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.67730 1.07306 -2.495 0.0126 * ## length 0.01959 0.01237 1.584 0.1133 ## sex2 -0.24297 0.19354 -1.255 0.2093 ## as.factor(age)2 -0.19847 0.92641 -0.214 0.8304 ## as.factor(age)3 0.33908 1.06938 0.317 0.7512 ## as.factor(age)4 0.59041 1.20582 0.490 0.6244 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 681.25 on 493 degrees of freedom ## Residual deviance: 637.41 on 488 degrees of freedom ## AIC: 649.41 ## ## Number of Fisher Scoring iterations: 4 A second option is to use so-called ``exact’’ methods for inference. There doesn’t appear to be a good package available for implementing these methods in R. Other software packages might be necessary. 7.3.2 Grouped binary data: TB in red deer For an example with grouped binary data, we will consider a second data set from Vicente et al. (2006), which is also discussed in Zuur et al. (2009). This data set gives the prevalence of tuberculosis infection in red deer in southern Spain at several different estates. Several deer were sampled at each estate, and the sampled deer were tested for tuberculosis. We’ll import the data first and do a bit of housekeeping. deer &lt;- read.table(&quot;data/tbdeer.txt&quot;, head = T) deer &lt;- deer[, c(4, 5, 8:14)] deer &lt;- na.omit(deer) The data contain several possible covariates. Here, we will just inspect the relationship between the prevalence of TB and a covariate called ReedDeerIndex. (Presumably the name of the covariate is a typo, and this is instead an index of the abundance of red deer.) In these data, the variable DeerPosTB gives the number of deer that tested positive for TB at each estate, and the variable DeerSampledTB gives the total number of deer sampled at each estate. deer$DeerPropTB &lt;- with(deer, DeerPosTB / DeerSampledTB) with(deer, plot(DeerPropTB ~ ReedDeerIndex, pch = 16)) fm1 &lt;- glm(cbind(DeerPosTB, DeerSampledTB - DeerPosTB) ~ ReedDeerIndex, family = binomial(link = &quot;logit&quot;), data = deer) summary(fm1) ## ## Call: ## glm(formula = cbind(DeerPosTB, DeerSampledTB - DeerPosTB) ~ ReedDeerIndex, ## family = binomial(link = &quot;logit&quot;), data = deer) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.92645 0.20380 -14.359 &lt; 2e-16 *** ## ReedDeerIndex 0.05952 0.01119 5.321 1.03e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 113.274 on 25 degrees of freedom ## Residual deviance: 87.662 on 24 degrees of freedom ## AIC: 131.7 ## ## Number of Fisher Scoring iterations: 5 Note that the model formula gives the response variable as a 2-column matrix, where the columns gives the counts of each outcome type. With grouped binomial data, overdispersion is again a possibility. Notice that the residual deviance is much greater than its corresponding df, suggesting that these data show substantial extra-binomial variation. Thus we will fit a quasibinomial model to account for the extra-binomial variation. fm2 &lt;- glm(cbind(DeerPosTB, DeerSampledTB - DeerPosTB) ~ ReedDeerIndex, family = quasibinomial(link = &quot;logit&quot;), data = deer) summary(fm2) ## ## Call: ## glm(formula = cbind(DeerPosTB, DeerSampledTB - DeerPosTB) ~ ReedDeerIndex, ## family = quasibinomial(link = &quot;logit&quot;), data = deer) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.92645 0.40220 -7.276 1.62e-07 *** ## ReedDeerIndex 0.05952 0.02208 2.696 0.0126 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasibinomial family taken to be 3.894694) ## ## Null deviance: 113.274 on 25 degrees of freedom ## Residual deviance: 87.662 on 24 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 Accounting for the extra-binomial variation has nearly doubled the standard error of the slope on the logit scale. Thus the naive characterization of the uncertainty in the estimate was far too small. We can see the effect of accounting for the overdispersion by comparing the confidence envelopes for the respective fits. new.data &lt;- data.frame(ReedDeerIndex = seq(from = min(deer$ReedDeerIndex), to = max(deer$ReedDeerIndex), length = 100)) predict.fm1 &lt;- predict(fm1, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) predict.fm2 &lt;- predict(fm2, newdata = new.data, type = &quot;response&quot;, se.fit = TRUE) with(deer, plot(DeerPropTB ~ ReedDeerIndex, pch = 16)) lines(x = new.data$ReedDeerIndex, y = predict.fm1$fit) # use critical value from z distribution when the scale parameter is assumed known # use t-distribution when the scale parameter is estimated lines(x = new.data$ReedDeerIndex, y = predict.fm1$fit - 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x = new.data$ReedDeerIndex, y = predict.fm1$fit + 1.96 * predict.fm1$se.fit, col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x = new.data$ReedDeerIndex, y = predict.fm2$fit + qt(0.025, df = 24) * predict.fm2$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) lines(x = new.data$ReedDeerIndex, y = predict.fm2$fit + qt(0.975, df = 24) * predict.fm2$se.fit, col = &quot;blue&quot;, lty = &quot;dashed&quot;) legend(&quot;top&quot;, pch = 16, col = c(&quot;blue&quot;, &quot;red&quot;), leg = c(&quot;quasibinomial&quot;, &quot;naive&quot;)) 7.4 Zero-adjusted models for count data In count data, the frequency of zeros often differs from what the standard Poisson or negative binomial distributions would predict. This occurs because the processes that lead to zero counts are often separate from the processes that govern the intensity of non-zero counts. For example, when counting parasite loads of animals, one process may determine if an individual animal is infected at all, and a second process may determine the intensity of the parasite load for those animals that are infected. This section discusses models for count data in which the proportion of zeros is modeled separately from the rest of the distribution. Any of these models may be paired with Poisson or negative binomial models for the non-zero counts. 7.4.1 Zero-truncated models Zero-truncated distributions are simply those that condition on the count variable being greater than 0. They are appropriate when the sampling procedure makes zeros unobservable. Zero-truncated Poisson and negative binomial distributions are available as something of a side benefit the R library VGAM (an acronym for vector generalized additive models). This package provides the function vglm (a version of glm that allows for vector-valued responses), which in turn accommodates the distribution families pospoisson and posnegbinomial. The “pos” portion of each refers to the fact that the counts must be strictly positive. We illustrate the use of a zero-truncated distribution with a species-abundance data set for butterflies provided in the VGAM package. The description of the data set in VGAM reads: “About 3300 individual butterflies were caught in Malaya by naturalist Corbet trapping butterflies. They were classified to about 500 species.” The data give the frequencies in a species abundance distribution. require(VGAM) data(corbet) head(corbet) ## ofreq species ## 1 1 118 ## 2 2 74 ## 3 3 44 ## 4 4 24 ## 5 5 29 ## 6 6 22 with(corbet, barplot(species, names = ofreq, xlab = &quot;no. of individuals&quot;, ylab = &quot;frequency&quot;)) We will fit a zero-truncated negative binomial distribution to these data, and compare the fitted zero-truncated distribution to the data. 7.4.2 Zero-inflated models As the name suggests, zero-inflated (henceforth ZI) models are appropriate when zeros are more frequent than the distribution of the non-zero counts would suggest. ZI models have two components. In the “zero-inflation” component, a logistic regression (or some other generalized linear model for a binary response) captures whether the response is a “false” zero or a realization from a proper count distribution. In the “count” component, a generalized linear model for count data (such as Poisson regression) is used to model the distribution of the non-false-zero counts. The ZI model can be motivated by envisioning that the data arise from a combination of two separate mechanisms: a first mechanism that determines whether or not the count is a “false” zero, and a second that determines the count if it is not a false zero. In ecology, the zero-inflation component might correspond to (true) presence vs. absence, and the count component might correspond to the detected intensity conditional on presence. To illustrate ZI models, we will use the cod parasite data described in \\(\\S\\) 11.3.2 of Zuur et al. (2009). These data are counts of the number of trypanosome blood parasites in individual cod, and were initially reported in Hemmingsen et al. (2005). First we load the data and do some housekeeping. cod &lt;- read.table(&quot;data/ParasiteCod.txt&quot;, head = T) # remove observations with missing data cod &lt;- na.omit(cod) summary(cod) ## Sample Intensity Prevalence Year ## Min. : 1.0 Min. : 0.000 Min. :0.0000 Min. :1999 ## 1st Qu.: 299.5 1st Qu.: 0.000 1st Qu.:0.0000 1st Qu.:1999 ## Median : 598.0 Median : 0.000 Median :0.0000 Median :2000 ## Mean : 613.7 Mean : 6.209 Mean :0.4534 Mean :2000 ## 3rd Qu.: 955.5 3rd Qu.: 4.000 3rd Qu.:1.0000 3rd Qu.:2001 ## Max. :1254.0 Max. :257.000 Max. :1.0000 Max. :2001 ## Depth Weight Length Sex ## Min. : 50.0 Min. : 34 Min. : 17.00 Min. :0.000 ## 1st Qu.:110.0 1st Qu.: 769 1st Qu.: 44.00 1st Qu.:1.000 ## Median :180.0 Median :1446 Median : 54.00 Median :1.000 ## Mean :176.3 Mean :1718 Mean : 53.53 Mean :1.427 ## 3rd Qu.:236.0 3rd Qu.:2232 3rd Qu.: 62.00 3rd Qu.:2.000 ## Max. :293.0 Max. :9990 Max. :101.00 Max. :2.000 ## Stage Age Area ## Min. :0.000 Min. : 0.000 Min. :1.000 ## 1st Qu.:1.000 1st Qu.: 3.000 1st Qu.:2.000 ## Median :1.000 Median : 4.000 Median :3.000 ## Mean :1.426 Mean : 4.118 Mean :2.568 ## 3rd Qu.:2.000 3rd Qu.: 5.000 3rd Qu.:3.000 ## Max. :4.000 Max. :10.000 Max. :4.000 There are a large number of potential predictors. Following the analysis in Zuur et al., we will focus on the effects of length (a measure of the fish’s size), year (which we will treat as a categorical predictor for flexibility), and “area”, a categorical predictor for one of four areas in which the fish was sampled. Before beginning, we will plot the data. In the plot below, each row of the plot corresponds to a year, and each column (unlabeled) corresponds to one of the four areas, going from area 1 (the leftmost column) to area 4 (the rightmost column). Because most of the parasite loads are zero or close to zero, we plot parasite load on a log(y + 1) scale. par(mfrow = c(3, 4), mar = c(2, 2, 1, 1), oma = c(3, 7, 0, 0), las = 1) for (i in unique(cod$Year)) { for (j in sort(unique(cod$Area))) { with(subset(cod, Year == i &amp; Area == j), plot(log(Intensity + 1) ~ Length, xlim = range(cod$Length), ylim = log(range(cod$Intensity) + 1), xlab = &quot;&quot;, ylab = &quot;&quot;, yaxt = &quot;n&quot;)) axis(2, at = log(c(0, 10, 100) + 1), lab = c(0, 10, 100)) if (j == 1) mtext(i, side = 2, line = 3) } } mtext(&quot;length&quot;, side = 1, outer = T, line = 2) mtext(&quot;parasite intensity&quot;, side = 2, outer = T, line = 5, las = 0) With ZI models, one can use separate model combinations of predictors for the two components. Indeed, if we think of the two components as capturing two different natural processes, then there is no reason that the predictors should have the same affect on both components. Following Zuur et al., however, we will start with a ZINB model that includes length, year, area, and an interaction between year and area for both components. Because both components are generalized linear models, both include a link function. Here, we will use the default logit link for the zero-inflation component and log link for the count component. To fit ZI models, we will use the pscl library from R. The name pscl is an acronym for political science computing laboratory. The pscl library includes functions for ZI poisson and negative binomial models (henceforth abbreviated ZIP and ZINB models). require(pscl) ## Loading required package: pscl ## Classes and Methods for R originally developed in the ## Political Science Computational Laboratory ## Department of Political Science ## Stanford University (2002-2015), ## by and under the direction of Simon Jackman. ## hurdle and zeroinfl functions by Achim Zeileis. formula.1 &lt;- formula(Intensity ~ Length + as.factor(Year) * as.factor(Area)) cod.nb.fm1 &lt;- zeroinfl(formula.1, data = cod, dist = &quot;negbin&quot;) summary(cod.nb.fm1) ## ## Call: ## zeroinfl(formula = formula.1, data = cod, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -0.6142 -0.4429 -0.3581 -0.1252 11.4530 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.733903 0.344397 10.842 &lt; 2e-16 *** ## Length -0.036410 0.005108 -7.128 1.02e-12 *** ## as.factor(Year)2000 0.063889 0.295638 0.216 0.82890 ## as.factor(Year)2001 -0.939151 0.606059 -1.550 0.12124 ## as.factor(Area)2 0.197774 0.329133 0.601 0.54791 ## as.factor(Area)3 -0.646729 0.277792 -2.328 0.01991 * ## as.factor(Area)4 0.707498 0.252265 2.805 0.00504 ** ## as.factor(Year)2000:as.factor(Area)2 -0.653991 0.535416 -1.221 0.22191 ## as.factor(Year)2001:as.factor(Area)2 0.967267 0.718168 1.347 0.17803 ## as.factor(Year)2000:as.factor(Area)3 1.024962 0.429598 2.386 0.01704 * ## as.factor(Year)2001:as.factor(Area)3 1.002701 0.677504 1.480 0.13888 ## as.factor(Year)2000:as.factor(Area)4 0.534532 0.414974 1.288 0.19771 ## as.factor(Year)2001:as.factor(Area)4 0.855163 0.654396 1.307 0.19128 ## Log(theta) -0.966623 0.096344 -10.033 &lt; 2e-16 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.19091 0.78132 0.244 0.806971 ## Length -0.03884 0.01203 -3.227 0.001249 ** ## as.factor(Year)2000 -1.07170 2.00018 -0.536 0.592094 ## as.factor(Year)2001 3.29315 0.71042 4.636 3.56e-06 *** ## as.factor(Area)2 2.01366 0.57288 3.515 0.000440 *** ## as.factor(Area)3 1.90526 0.54987 3.465 0.000530 *** ## as.factor(Area)4 -0.73617 0.86221 -0.854 0.393206 ## as.factor(Year)2000:as.factor(Area)2 0.46538 2.07885 0.224 0.822864 ## as.factor(Year)2001:as.factor(Area)2 -3.20741 0.83595 -3.837 0.000125 *** ## as.factor(Year)2000:as.factor(Area)3 -0.79455 2.15765 -0.368 0.712690 ## as.factor(Year)2001:as.factor(Area)3 -3.50408 0.82999 -4.222 2.42e-05 *** ## as.factor(Year)2000:as.factor(Area)4 -13.57000 1509.57406 -0.009 0.992828 ## as.factor(Year)2001:as.factor(Area)4 -2.91050 1.10456 -2.635 0.008414 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 0.3804 ## Number of iterations in BFGS optimization: 53 ## Log-likelihood: -2450 on 27 Df The neatly organized output gives coefficients for each component of the model, with the count component presented first and the zero-inflation component presented second. The output for the zero-inflation component shows a very large standard error associated with the coefficient for area 4 in the year 2000. Let’s take a look at the frequency of zeros, as broken down by year and area. with(cod, table(Year, Area, Intensity &gt; 0)) ## , , = FALSE ## ## Area ## Year 1 2 3 4 ## 1999 57 65 121 34 ## 2000 16 31 32 6 ## 2001 63 72 112 42 ## ## , , = TRUE ## ## Area ## Year 1 2 3 4 ## 1999 89 27 45 69 ## 2000 39 18 43 44 ## 2001 7 34 45 80 We see that there are only a few zeros in area 4 for 2000. Thus, the zero-inflation component is trying to fit a zero probability of zero inflation for area 4 in year 2000, leading to complete separation. We need to reduce the number of parameters somehow. Lacking any better ideas, we’ll follow Zuur et al. and remove the interaction between year and area for the zero-inflation portion of the model. To do so, we need a model formula that differs between the two model components. In pscl, we implement this model by providing a model formula where the differing predictor combinations for the two components are separated by a vertical bar. formula.2 &lt;- formula(Intensity ~ Length + as.factor(Year) * as.factor(Area) | Length + as.factor(Year) + as.factor(Area)) cod.nb.fm2 &lt;- zeroinfl(formula.2, data = cod, dist = &quot;negbin&quot;) summary(cod.nb.fm2) ## ## Call: ## zeroinfl(formula = formula.2, data = cod, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -0.5911 -0.4480 -0.3794 -0.1258 12.0794 ## ## Count model coefficients (negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.78614 0.34093 11.105 &lt; 2e-16 *** ## Length -0.03669 0.00502 -7.309 2.70e-13 *** ## as.factor(Year)2000 0.03438 0.29188 0.118 0.90625 ## as.factor(Year)2001 -2.32201 0.38125 -6.090 1.13e-09 *** ## as.factor(Area)2 0.12404 0.32906 0.377 0.70620 ## as.factor(Area)3 -0.82582 0.27838 -2.966 0.00301 ** ## as.factor(Area)4 0.65502 0.25266 2.592 0.00953 ** ## as.factor(Year)2000:as.factor(Area)2 -0.75546 0.50817 -1.487 0.13711 ## as.factor(Year)2001:as.factor(Area)2 2.39045 0.52066 4.591 4.41e-06 *** ## as.factor(Year)2000:as.factor(Area)3 1.21458 0.42003 2.892 0.00383 ** ## as.factor(Year)2001:as.factor(Area)3 2.52719 0.45772 5.521 3.37e-08 *** ## as.factor(Year)2000:as.factor(Area)4 0.59338 0.41572 1.427 0.15348 ## as.factor(Year)2001:as.factor(Area)4 2.23075 0.44603 5.001 5.69e-07 *** ## Log(theta) -1.01443 0.09655 -10.507 &lt; 2e-16 *** ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.92837 0.70656 1.314 0.188870 ## Length -0.04686 0.01252 -3.744 0.000181 *** ## as.factor(Year)2000 -1.25036 0.50280 -2.487 0.012889 * ## as.factor(Year)2001 0.25487 0.32785 0.777 0.436923 ## as.factor(Area)2 1.62677 0.48056 3.385 0.000711 *** ## as.factor(Area)3 1.19152 0.49142 2.425 0.015324 * ## as.factor(Area)4 -1.26324 0.64392 -1.962 0.049788 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta = 0.3626 ## Number of iterations in BFGS optimization: 32 ## Log-likelihood: -2460 on 21 Df There’s a lot of output here to process. One observation we might make is that there seems to be a strongly significant negative association between length and both the probability of a false zero in the first model component, and the parasite abundance in the second model component. To visualize the model, we might plot the fitted probability of a false zero vs. length for each combination of area and year, and then plot the fitted intensity (mean) of the count process vs. length, also for each combination of area and year. See Ch. 11 of Zuur et al. for such plots. Alternatively, we might merge the two model components to generate predicted values as a function of length for each year and area. The library pscl provides methods to extract predicted values directly; see the help documentation for pscl::predict.zeroinfl. We use that method here to generate predicted mean parasite counts (merging the zero-inflation and count components) for each combination of year and area. In the plot below, bear in mind that because the data are shown on a log scale, the fitted line will not necessarily pass through the center of the plotted data cloud. length.vals &lt;- seq(from = min(cod$Length), to = max(cod$Length), length = 100) par(mfrow = c(3, 4), mar = c(2, 2, 1, 1), oma = c(3, 7, 0, 0), las = 1) for (i in unique(cod$Year)) { for (j in sort(unique(cod$Area))) { new.data &lt;- data.frame(Length = length.vals, Year = i, Area = j) predicted.vals &lt;- predict(cod.nb.fm2, newdata = new.data, type = &quot;response&quot;) plot(x = range(cod$Length), y = log(range(cod$Intensity) + 1), xlab = &quot;&quot;, ylab = &quot;&quot;, type = &quot;n&quot;, yaxt = &quot;n&quot;) lines(log(predicted.vals + 1) ~ length.vals) with(subset(cod, Year == i &amp; Area == j), points(log(Intensity + 1) ~ Length)) axis(2, at = log(c(0, 10, 100) + 1), lab = c(0, 10, 100)) if (j == 1) mtext(i, side = 2, line = 3) } } mtext(&quot;length&quot;, side = 1, outer = T, line = 2) mtext(&quot;parasite intensity&quot;, side = 2, outer = T, line = 6, las = 0) 7.4.3 Zero-altered, or “hurdle”, models Zero-altered (ZA) models, also known as hurdle models, are similar to ZI models in the sense that there are two components. However, in ZA models, the binary component models whether the response is 0 or greater than 0. This binary component of the model is called the hurdle component. The count component is then modeled using a zero-truncated distribution. Consequently, in ZA models, the proportion of zeros may be either greater or less than the distribution of the non-zero counts suggests. The analysis path for ZA models is the same as it is for ZI models. The two components of the model can be contemplated independently of one another, and either a zero-truncated Poisson or negative binomial distribution can be used for the count component. The pscl library contains the hurdle function for fitting ZA modes. However, in ZA models, the two components are entirely separate from one another, so one doesn’t really need additional specialized software. ZA models could be fit just as readily by bolting together a logistic regression for the hurdle component with a model for zero-truncated responses in the count component. This is not true for ZI models, though. For ZI models, the estimated proportion of false zeros affects the estimate of the intensity for the non-false-zero counts, so the two components cannot be fit separately from one another. We will use the cod parasite data to illustrate hurdle models also. This is something of an artificial example, however, because the fact that some infected fish will probably yield samples with zero counts suggests that a ZI model has a more natural ecological interpretation. We will proceed directly to the final model chosen by Zuur et al., which includes additive effects of length, year, and area in the intensity component, and an interaction between area and year (but no effect of length) in the hurdle component. formula.3 &lt;- formula(Intensity ~ Length + as.factor(Year) + as.factor(Area) | as.factor(Year) * as.factor(Area)) cod.hurdle.fm3 &lt;- hurdle(formula.3, data = cod, dist = &quot;negbin&quot;) summary(cod.hurdle.fm3) ## ## Call: ## hurdle(formula = formula.3, data = cod, dist = &quot;negbin&quot;) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -0.6387 -0.3825 -0.3312 -0.1209 11.9366 ## ## Count model coefficients (truncated negbin with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.127498 0.387197 8.077 6.62e-16 *** ## Length -0.036524 0.005678 -6.432 1.26e-10 *** ## as.factor(Year)2000 0.384774 0.202231 1.903 0.0571 . ## as.factor(Year)2001 -0.153655 0.198692 -0.773 0.4393 ## as.factor(Area)2 0.404208 0.265277 1.524 0.1276 ## as.factor(Area)3 0.021424 0.231773 0.092 0.9264 ## as.factor(Area)4 1.060123 0.223756 4.738 2.16e-06 *** ## Log(theta) -1.591345 0.256348 -6.208 5.37e-10 *** ## Zero hurdle model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.4456 0.1696 2.627 0.00863 ** ## as.factor(Year)2000 0.4454 0.3419 1.303 0.19273 ## as.factor(Year)2001 -2.6428 0.4330 -6.103 1.04e-09 *** ## as.factor(Area)2 -1.3241 0.2850 -4.647 3.37e-06 *** ## as.factor(Area)3 -1.4347 0.2434 -5.893 3.78e-09 *** ## as.factor(Area)4 0.2622 0.2696 0.972 0.33085 ## as.factor(Year)2000:as.factor(Area)2 -0.1105 0.5071 -0.218 0.82758 ## as.factor(Year)2001:as.factor(Area)2 2.7711 0.5322 5.207 1.92e-07 *** ## as.factor(Year)2000:as.factor(Area)3 0.8392 0.4493 1.868 0.06182 . ## as.factor(Year)2001:as.factor(Area)3 2.7201 0.4991 5.450 5.05e-08 *** ## as.factor(Year)2000:as.factor(Area)4 0.8393 0.5918 1.418 0.15612 ## as.factor(Year)2001:as.factor(Area)4 2.5794 0.5174 4.985 6.19e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Theta: count = 0.2037 ## Number of iterations in BFGS optimization: 21 ## Log-likelihood: -2448 on 20 Df 7.5 Generalized additive models (GAMs) Recall that splines are essentially just another way of specifying the predictors in a regression model. Generalized additive models (GAMs) are to additive models what generalized linear models are to (general) linear models. In other words, GAMs use splines to create a linear predictor in a GLM that is a smooth function of a covariate. We illustrate GAMs by considering a data set that gives the size and annual survival of colonies of the stony coral Acropora millepora. (Recall that stony corals are a colonial organism, so here a coral “colony” means a colony of polyps. A colony is what most naive observers would recognize as an individual coral.) We wish to understand how the size of the colony is related to its survival. The size of the colony is measured as planar (top-down) area. The units of area are unclear; perhaps it is m\\(^2\\)? In the data file, the variable “mortality” is a binary response coded as a 0 if the colony survived, and as a 1 if the colony died. The size data have been log-transformed. These data are from Madin et al. (2014). In R, we fit the GAM with the function mgcv::gam. require(mgcv) ## Loading required package: mgcv ## Loading required package: nlme ## This is mgcv 1.9-3. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. ## ## Attaching package: &#39;mgcv&#39; ## The following object is masked from &#39;package:VGAM&#39;: ## ## s coral &lt;- read.csv(&quot;data/coral.csv&quot;, head = TRUE, stringsAsFactors = TRUE) head(coral) ## species growth_form ln_area mortality ## 1 Acropora millepora corymbose -3.427419 0 ## 2 Acropora millepora corymbose -3.964138 0 ## 3 Acropora millepora corymbose -3.682131 0 ## 4 Acropora millepora corymbose -5.144650 0 ## 5 Acropora millepora corymbose -3.814405 0 ## 6 Acropora millepora corymbose -3.661057 0 with(coral, plot(jitter(mortality, amount = 0.02) ~ ln_area, xlab = &quot;log area&quot;, ylab = &quot;mortality&quot;)) We fit a GAM with a binomial response, logit link, and use a smoothing spline to capture the relationship between log size and (the log odds of) mortality. Recall that a smoothing spline determines the degree of smoothness by generalized cross-validation. fm1 &lt;- gam(mortality ~ s(ln_area), family = binomial(link = &quot;logit&quot;), data = coral) summary(fm1) ## ## Family: binomial ## Link function: logit ## ## Formula: ## mortality ~ s(ln_area) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.6068 0.1667 -9.636 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(ln_area) 1.921 2.433 10.52 0.0089 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.0532 Deviance explained = 4.79% ## UBRE = -0.098118 Scale est. = 1 n = 270 # plot data with fit overlaid with(coral, plot(jitter(mortality, amount = 0.02) ~ ln_area, xlab = &quot;log area&quot;, ylab = &quot;mortality&quot;)) x.vals &lt;- with(coral, seq(from = min(ln_area), to = max(ln_area), length = 100)) fm1.fit &lt;- predict(fm1, newdata = data.frame(ln_area = x.vals), se = TRUE) inv.logit &lt;- function(x) exp(x) / (1 + exp(x)) lines(x.vals, inv.logit(fm1.fit$fit)) lines(x.vals, inv.logit(fm1.fit$fit + 1.96 * fm1.fit$se.fit), lty = &quot;dashed&quot;) lines(x.vals, inv.logit(fm1.fit$fit - 1.96 * fm1.fit$se.fit), lty = &quot;dashed&quot;) Note that the non-linearity in the fit is not just a consequence of converting to the probability scale. If we plot the fit on the log odds scale, we can see the non-linearity that results from using a penalized regression spline. plot(x.vals, fm1.fit$fit, type = &quot;l&quot;, xlab = &quot;log size, x&quot;, ylab = &quot;log odds of mortality, s(x)&quot;) We might also try a complementary log-log link. fm2 &lt;- gam(mortality ~ s(ln_area), family = binomial(link = &quot;cloglog&quot;), data = coral) inv.cloglog &lt;- function(x) 1 - exp(-exp(x)) with(coral, plot(jitter(mortality, amount = 0.02) ~ ln_area, xlab = &quot;log area&quot;, ylab = &quot;mortality&quot;)) fm2.fit &lt;- predict(fm2, newdata = data.frame(ln_area = x.vals), se = TRUE) lines(x.vals, inv.cloglog(fm2.fit$fit), col = &quot;red&quot;) lines(x.vals, inv.cloglog(fm2.fit$fit + 1.96 * fm2.fit$se.fit), col = &quot;red&quot;, lty = &quot;dashed&quot;) lines(x.vals, inv.cloglog(fm2.fit$fit - 1.96 * fm2.fit$se.fit), col = &quot;red&quot;, lty = &quot;dashed&quot;) AIC(fm1, fm2) ## df AIC ## fm1 2.920822 243.5082 ## fm2 2.947335 243.0539 The AIC slightly favors the complementary log-log link. Bibliography Madin, Joshua S, Andrew H Baird, Maria Dornelas, and Sean R Connolly. 2014. “Mechanical Vulnerability Explains Size-Dependent Mortality of Reef Corals.” Ecology Letters 17 (8): 1008–15. Poole, Joyce H. 1989. “Mate Guarding, Reproductive Success and Female Choice in African Elephants.” Animal Behaviour 37: 842–49. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Pacific Grove, CA: Duxbury. Vicente, Joaquı́n, Ursula Höfle, Joseba Garrido, Isabel G Fernández-De-Mera, Ramón Juste, Marta Barral, and Christian Gortazar. 2006. “Wild Boar and Red Deer Display High Prevalences of Tuberculosis-Like Lesions in Spain.” Veterinary Research 37 (1): 107–19. Wood, Simon N. 2017. Generalized Additive Models: An Introduction with r. CRC press. Zuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with R. Vol. 574. Springer. The acronym ‘GLM’ can be ambiguous in statistics. GLM can stand for ‘general linear models’ or ‘generalized linear models’. These two terms differ! General linear models encompass regression and ANOVA, while generalized linear models accommodate non-Gaussian responses. Confusingly, in SAS, PROC GLM is a procedure to fit the general linear model, while in R the glm routine fits generalized linear models. In these notes, GLM will be used as an acronym for generalized linear models.↩︎ The common mathematical scheme that is used to find the MLEs of generalized linear models is called iteratively reweighted least squares (IRLS). There is a variation of IRLS called Fisher scoring that is the fitting routine implemented in the glm routine in R.↩︎ Do not confuse the exponential family of probability distributions with the exponential distribution! The exponential distribution is one of the probability distributions in the exponential family, but it is not the only one.↩︎ "],["generalized-linear-mixed-models.html", "Chapter 8 Generalized linear mixed models 8.1 Example 1: Industrial melanism data 8.2 Example 2: Ticks on red grouse 8.3 GAMMs", " Chapter 8 Generalized linear mixed models 8.1 Example 1: Industrial melanism data We will examine several possible approaches to analyzing the industrial melanism data. The original source for these data is Bishop (1972); I obtained them from Ramsey and Schafer (2002). Recall that these data consist of paired binomial responses with two covariates: distance from Liverpool (a station-level covariate) and color morph (an observation-level covariate). In notation, the model that we seek to fit is \\[\\begin{align*} y_{ij} &amp; \\sim \\mathrm{Binom}(p_{ij}, n_{ij})\\\\ \\mathrm{logit}(p_{ij}) &amp; = \\eta_{ij} \\\\ \\eta_{ij} &amp; = a_i + b_i x_j + L_j \\\\ L_j &amp; \\sim \\mathcal{N}(0, \\sigma^2_L) \\end{align*}\\] where \\(i=1,2\\) indexes the two color morphs, \\(j = 1, \\ldots, 7\\) indexes the stations, \\(y_{ij}\\) is the number of moths removed, \\(n_{ij}\\) is the number of moths placed, and let \\(x_j\\) is the distance of the station from Liverpool. We are most interested in learning about the difference \\(b_1 - b_2\\), which quantifies how the relationship between log odds of removal and distance differs between the two color morphs, and determining whether there is evidence that this difference \\(\\neq 0\\). Alternatively, we might prefer to consider the quantity \\(e^{b_1 - b_2} = e^{b_1} / e^{b_2}\\), which tells us how the odds ratio for removal changes between the two morphs as distance increases. This odds ratio is a bit closer to something that we can mentall grasp. In terms of the odds ratio, we are interested in learning if the odds ratio \\(\\neq 1\\). Before proceeding, we note that one approach is simply to regress the difference of the empirical logits vs. distance. This reduces the problem to a simple regression. We try this approach first and use it as a benchmark. The data set used here is reformatted to include one record for each of the 7 stations. moth2 &lt;- read.table(&quot;data/moth2.txt&quot;, head = TRUE, stringsAsFactors = TRUE) head(moth2, n = 3) ## location distance morph l.placed l.removed d.placed d.removed ## 1 sp 0.0 light 56 17 56 14 ## 2 ef 7.2 light 80 28 80 20 ## 3 ha 24.1 light 52 18 52 22 elogit &lt;- function(x) log(x / (1 - x)) moth2$elogit.diff &lt;- with(moth2, elogit(d.removed / d.placed) - elogit(l.removed / l.placed)) fm1 &lt;- lm(elogit.diff ~ distance, data = moth2) with(moth2, plot(elogit.diff ~ distance, xlab = &quot;distance from city center (km)&quot;, ylab = &quot;difference in log odds of removal, dark - light&quot;)) abline(h = 0, lty = &quot;dashed&quot;) abline(fm1) summary(fm1) ## ## Call: ## lm(formula = elogit.diff ~ distance, data = moth2) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 0.10557 -0.30431 0.03501 0.26395 -0.09387 0.29714 -0.30349 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.373830 0.192503 -1.942 0.10980 ## distance 0.027579 0.005997 4.599 0.00585 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2698 on 5 degrees of freedom ## Multiple R-squared: 0.8088, Adjusted R-squared: 0.7706 ## F-statistic: 21.15 on 1 and 5 DF, p-value: 0.005846 confint(fm1) ## 2.5 % 97.5 % ## (Intercept) -0.86867531 0.12101544 ## distance 0.01216371 0.04299419 This approach tells us that the difference in log-odds slopes (defined as dark morph - light morph) is 0.0276, with a 95% confidence interval of (0.012, 0.043). This corresponds to an odds ratio of 1.028, with a 95% confidence interval of (1.012, 1.044). In other words, with every additional km from the city center, the odds ratio for a dark moth’s removal vs. a light moth’s removal increases by about 2.8%. The major disadvantage to the approach above is that it doesn’t account for the fact that differing numbers of moths were placed at the different stations. We could try to account for this with a weighted regression, but it’s not clear what the weights should be. We were also fortunate in the sense that there were no instances of either none or all of the moths being removed at a particular station, which would have led to an infinite empirical logit. 8.1.1 GEEs Next we try a GEE with a compound symmetry (“exchangable”) correlation structure imposed on the pair of measurements at each station. Because there are only two data records for each station, there is no loss of generality in assuming this correlation structure. We fit the model using geepack::geeglm. require(geepack) ## Loading required package: geepack moth &lt;- read.table(&quot;data/moth.txt&quot;, head = TRUE, stringsAsFactors = TRUE) contrasts(moth$morph) &lt;- contr.treatment(n = 2, base = 2) fm2 &lt;- geeglm(cbind(removed, placed - removed) ~ distance * morph, family = binomial(link = &quot;logit&quot;), data = moth, id = location, corstr = &quot;exchangeable&quot;) summary(fm2) ## ## Call: ## geeglm(formula = cbind(removed, placed - removed) ~ distance * ## morph, family = binomial(link = &quot;logit&quot;), data = moth, id = location, ## corstr = &quot;exchangeable&quot;) ## ## Coefficients: ## Estimate Std.err Wald Pr(&gt;|W|) ## (Intercept) -0.714717 0.129102 30.648 3.09e-08 *** ## distance -0.009385 0.003221 8.489 0.00357 ** ## morph1 -0.410238 0.163953 6.261 0.01234 * ## distance:morph1 0.027762 0.005812 22.815 1.78e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation structure = exchangeable ## Estimated Scale Parameters: ## ## Estimate Std.err ## (Intercept) 0.01313 0.006935 ## Link = identity ## ## Estimated Correlation Parameters: ## Estimate Std.err ## alpha 0.3911 0.298 ## Number of clusters: 7 Maximum cluster size: 2 The estimate of the difference between slopes on the log-odds scale is 0.0278, with an approximate 95% confidence interval of (0.0164, 0.0391). This corresponds to an odds ratio of 1.028, with an approximate 95% confidence interval of (1.017, 1.040). To visualize the model, we might plot the fitted proportion removed vs. distance for both color morphs. Bear in mind that fitted values here correspond to marginal mean removal rates. inv.logit &lt;- function(x) exp(x) / (1 + exp(x)) light.fit &lt;- function(d) inv.logit(-0.71472 - 0.00938 * d) dark.fit &lt;- function(d) inv.logit(-0.71472 - 0.41024 + (-0.00938 + 0.02776) * d) curve(dark.fit, from = min(moth$distance), to = max(moth$distance), xlab = &quot;distance from city center (km)&quot;, ylab = &quot;proportion removed&quot;, ylim = c(0.15, 0.5)) curve(light.fit, from = min(moth$distance), to = max(moth$distance), xlab = &quot;distance from city center (km)&quot;, ylab = &quot;proportion removed&quot;, add = TRUE, lty = &quot;dashed&quot;) with(subset(moth, morph == &quot;dark&quot;), points(removed / placed ~ distance, pch = 16)) with(subset(moth, morph == &quot;light&quot;), points(removed / placed ~ distance, pch = 1)) For the sake of comparing marginal means to conditional means, we will consider the predicted removal rate of dark morphs at a hypothetical location 20 km from the city center. This predicted removal rate is 0.319. dark.fit(20) ## [1] 0.3192197 8.1.2 GLMMs Next, we will fit the same model with lme4::glmer. require(lme4) ## Loading required package: lme4 ## Loading required package: Matrix fm3 &lt;- glmer(cbind(removed, placed - removed) ~ distance * morph + (1 | location), family = binomial(link = &quot;logit&quot;), data = moth) summary(fm3) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: cbind(removed, placed - removed) ~ distance * morph + (1 | location) ## Data: moth ## ## AIC BIC logLik -2*log(L) df.resid ## 85.7 88.9 -37.8 75.7 9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.73965 -0.41890 0.02967 0.66584 1.08052 ## ## Random effects: ## Groups Name Variance Std.Dev. ## location (Intercept) 0.01148 0.1072 ## Number of obs: 14, groups: location, 7 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.719786 0.205488 -3.503 0.000460 *** ## distance -0.009341 0.006270 -1.490 0.136243 ## morph1 -0.411128 0.274765 -1.496 0.134578 ## distance:morph1 0.027819 0.008094 3.437 0.000588 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) distnc morph1 ## distance -0.843 ## morph1 -0.641 0.538 ## dstnc:mrph1 0.558 -0.660 -0.859 confint(fm3, parm = c(&quot;distance:morph1&quot;)) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## distance:morph1 0.01201418 0.04377205 Nothing here is radically different. The parameter estimates are so similar to those from the GEE that a plot of the GEE and GLMM fits would be indistinguishable to the eye. Although summary.glmer doesn’t report the deviance, functions exist to obtain this information. We can use the deviance to assess overdispersion in the same way that we would in a GLM. deviance(fm3) ## [1] 9.250077 df.residual(fm3) ## [1] 9 The ratio of the deviance to the residual df is approximately 1, suggesting that the data are not overdispersed. Note that including the location random effect in the GLMM has eliminated the mild overdispersion that we detected in the GLM without the location random effect. To get a sense of how the conditional means compare to the marginal means, we will compute the conditional mean removal rate of dark morphs at a distance 20 km from the city center. dark.linpred.glmm &lt;- function(d) -0.71979 - 0.41113 + (-0.00934 + 0.02782) * d dark.fit.glmm &lt;- function(d) inv.logit(dark.linpred.glmm(d)) dark.fit.glmm(20) ## [1] 0.3183597 The conditional mean of the predicted removal rate is 0.318. Here, the difference between the marginal and conditional means is tiny. Nevertheless, we can gain a deeper understanding of the difference by taking a look at the fitted population of possible locations at 20 km distance on both the linear predictor scale and the data scale. linpred.sample &lt;- rnorm(1e6, mean = dark.linpred.glmm(20), sd = 0.1072) prob.sample &lt;- inv.logit(linpred.sample) (conditional.mean &lt;- inv.logit(dark.linpred.glmm(20))) ## [1] 0.3183597 (marginal.mean &lt;- mean(prob.sample)) ## [1] 0.3188174 par(mfrow = c(1, 2)) hist(linpred.sample, breaks = 50, xlab = &quot;linear predictor&quot;, main = &quot;&quot;) hist(prob.sample, breaks = 50, xlab = &quot;removal probability&quot;, main = &quot;&quot;) abline(v = conditional.mean, col = &quot;darkorange&quot;, lwd =2) abline(v = marginal.mean, col = &quot;blue&quot;, lwd = 2) We see that the variance of the location-level random effect is small enough that the inverse logit transformation is effectively linear. Thus, the distribution of removal probabilities across locations is nearly normal, and the conditional and marginal means nearly coincide. The estimate of the marginal mean that we have generated by simulation is not quite the same as the marginal mean generated by the GEE, which could either be due to the stochastic sampling that we have used above, and/or small numerical differences in the estimation. For the sake of illustration, we repeat these calculations by supposing that the location-to-location standard deviation was 10 times larger. linpred.sample &lt;- rnorm(1e6, mean = dark.linpred.glmm(20), sd = 10 * 0.1072) prob.sample &lt;- inv.logit(linpred.sample) (conditional.mean &lt;- inv.logit(dark.linpred.glmm(20))) ## [1] 0.3183597 (marginal.mean &lt;- mean(prob.sample)) ## [1] 0.3503541 par(mfrow = c(1, 2)) hist(linpred.sample, breaks = 50, xlab = &quot;linear predictor&quot;, main = &quot;&quot;) hist(prob.sample, breaks = 50, xlab = &quot;removal probability&quot;, main = &quot;&quot;) abline(v = conditional.mean, col = &quot;darkorange&quot;, lwd =2) abline(v = marginal.mean, col = &quot;blue&quot;, lwd = 2) 8.1.3 Bayesian fit We now fit the model using JAGS and vague priors. require(R2jags) moth.model &lt;- function() { for (j in 1:J) { # J = number of data points y[j] ~ dbin(p[j], n[j]) # data distribution p[j] &lt;- ilogit(eta[j]) # inverse link eta[j] &lt;- a[morph[j]] + b[morph[j]] * dist[j] + L[loc[j]] # linear predictor, } for (j in 1:7){ # random effects for location L[j] ~ dnorm(0, tau_L) } a[1] ~ dnorm (0.0, 1E-6) # priors for intercept a[2] ~ dnorm (0.0, 1E-6) # priors for intercept b[1] ~ dnorm (0.0, 1E-6) # prior for slope b[2] ~ dnorm (0.0, 1E-6) # prior for slope tau_L ~ dexp(1) # prior for location-level random effect sd_L &lt;- pow(tau_L, -1/2) b.diff &lt;- b[1] - b[2] } jags.data &lt;- list(y = moth$removed, n = moth$placed, dist = moth$distance, loc = as.numeric(moth$location), morph = as.numeric(moth$morph), J = nrow(moth)) jags.params &lt;- c(&quot;a[1]&quot;, &quot;a[2]&quot;, &quot;b[1]&quot;, &quot;b[2]&quot;, &quot;b.diff&quot;, &quot;sd_L&quot;) jags.inits &lt;- function(){ list(&quot;tau_L&quot; = runif(1)) } set.seed(1) jagsfit &lt;- jags(data = jags.data, inits = jags.inits, parameters.to.save = jags.params, model.file = moth.model, n.chains = 3, n.iter = 5E4, n.thin = 5) ## module glm loaded For some reason this works without specifying initial values for \\(a\\) and \\(b\\) (now both vectors). Maybe the initial values are drawn from the prior? mcmc.output &lt;- as.data.frame(jagsfit$BUGSoutput$sims.list) (post.mean &lt;- apply(mcmc.output, 2, mean)) ## a.1 a.2 b.diff b.1 b.2 deviance ## -1.14532242 -0.73593138 0.02792038 0.01844916 -0.00947122 74.26101714 ## sd_L ## 0.67703526 HPDinterval(as.mcmc(mcmc.output[&#39;b.diff&#39;])) ## lower upper ## b.diff 0.01232351 0.04384734 ## attr(,&quot;Probability&quot;) ## [1] 0.95 The posterior mean of the difference in the log-odds slopes — 0.0279 — is essentially the same value that we have seen in every analysis. We can have a look at the full posterior distribution for this difference, and calculate the posterior probability that the difference is \\(&gt;0\\). bayesplot::mcmc_areas(mcmc.output, pars = c(&quot;b.diff&quot;), prob = 0.95) table(mcmc.output$b.diff &gt; 0) ## ## FALSE TRUE ## 3 14997 Thus we would say that there is a 0.9998 posterior probability that the proportion of dark moths removed increases more rapidly with increasing distance from Liverpool than the proportion of light moths removed. We can plot the fit of the model using draws from the posterior distribution of the parameters. The heavy lines below show the fits using the posterior means of the parameters. Do these fits correspond to the marginal or conditional means? (There’s little difference here, but it’s a useful thought exercise.) subset.samples &lt;- sample(nrow(mcmc.output), size = 100) moth$prop.removed &lt;- with(moth, removed / placed) light &lt;- subset(moth, morph == &quot;light&quot;) dark &lt;- subset(moth, morph == &quot;dark&quot;) par(mfrow = c(1, 2)) #------ light morph plot(prop.removed ~ distance, data = moth, type = &quot;n&quot;, main = &quot;Light morph&quot;, ylab = &quot;proprotion removed&quot;) points(x = light$distance, y = light$prop.removed, pch = 16) for(i in subset.samples) { a &lt;- mcmc.output$a.2[i] b &lt;- mcmc.output$b.2[i] fitted.curve &lt;- function(x) inv.logit(a + b * x) curve(fitted.curve, from = min(moth$distance), to = max(moth$distance), add = TRUE, col = &quot;deepskyblue&quot;) } fitted.mean.curve &lt;- function(x) inv.logit(post.mean[&#39;a.2&#39;] + post.mean[&#39;b.2&#39;] * x) curve(fitted.mean.curve, from = min(moth$distance), to = max(moth$distance), add = TRUE, col = &quot;darkblue&quot;, lwd = 2) points(x = light$distance, y = light$prop.removed, pch = 16) #--------- dark morph plot(prop.removed ~ distance, data = moth, type = &quot;n&quot;, main = &quot;Dark morph&quot;, ylab = &quot;proprotion removed&quot;) for(i in subset.samples) { a &lt;- mcmc.output$a.1[i] b &lt;- mcmc.output$b.1[i] fitted.curve &lt;- function(x) inv.logit(a + b * x) curve(fitted.curve, from = min(moth$distance), to = max(moth$distance), add = TRUE, col = &quot;deepskyblue&quot;) } fitted.mean.curve &lt;- function(x) inv.logit(post.mean[&#39;a.1&#39;] + post.mean[&#39;b.1&#39;] * x) curve(fitted.mean.curve, from = min(moth$distance), to = max(moth$distance), add = TRUE, col = &quot;darkblue&quot;, lwd = 2) points(x = dark$distance, y = dark$prop.removed, pch = 16) 8.2 Example 2: Ticks on red grouse This example comes from Ben Bolker’s chapter in Fox, Negrete-Yankelevich, and Sosa (2015). Bolker describes the data as follows: “Elston et al. (2001) used data on numbers of ticks sampled from the heads of red grouse chicks in Scotland to explore patterns of aggregation. Ticks have potentially large fitness and demographic consequences on red grouse individuals and populations, but Elston et al.’s goal was just to decompose patterns of variation into different scales (within-brood, within-site, by altitude and year). The response is the tick count (TICKS, again Poisson or negative binomial); altitude (HEIGHT, treated as continuous) and year (YEAR, treated as categorical) are fixed predictor variables. Individual within brood (INDEX) and brood within location are nested random-effect grouping variables, with the baseline expected number of ticks (intercept) varying among groups.” An alternative analysis of these data can be found on Bolker’s Github page at https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html. The data include 3 years and 63 locations. Location and year are crossed (some locations are sampled in multiple years), but not every location is sampled every year. There are 118 broods, and each brood is nested within a year-location pair. Each observation corresponds to one of 403 individuals. Individuals are nested within broods. We also know each location’s elevation. To develop notation, let \\(i=1, \\ldots, 3\\) index the years, let \\(j = 1, \\ldots, 63\\) index the locations, let \\(k = 1, \\ldots, n_{ij}\\) index the broods sampled in each year-location pair, and let \\(l = 1, \\ldots, n_{ijk}\\) index the separate individuals in each brood. Note that almost all year-location pairs will be represented by at most 1 brood (\\(n_{ij}=0\\) or \\(=1\\) for almost all combination of years and locations) but there is at least one year-location pair with multiple broods (\\(n_{ij} &gt; 1\\)). Let \\(y_{ijkl}\\) (the response) be the number of ticks found on individual \\(l\\) of brood \\(k\\) at location \\(j\\) in year \\(i\\), and let \\(x_j\\) be the elevation of location \\(j\\). We are interested in characterizing how tick load varies among locations, among broods within locations, and among individuals within broods. We are also interested in characterizing the relationship between elevation and tick load. It seems unlikely that 3 years are enough to estimate year-to-year variation, so we will use fixed-effect parameters to capture the differences among years. We will begin by assuming that the tick load takes a Poisson distribution, and use the canonical log link. We include an observation-level random effect so that we have a variance parameter that captures individual-to-individual variation among individuals in the same brood. In notation, our GLMM is \\[\\begin{align*} y_{ijkl} &amp; \\sim \\mathrm{Pois}(\\mu_{ijkl})\\\\ \\log(\\mu_{ijkl}) &amp; = \\eta_{ijkl} \\\\ \\eta_{ijkl} &amp; = a_i + b x_j + L_j + B_{ijk} + \\varepsilon_{ijkl} \\\\ L_j &amp; \\sim \\mathcal{N}(0, \\sigma^2_L) \\\\ B_{ijk} &amp; \\sim \\mathcal{N}(0, \\sigma^2_B) \\\\ \\varepsilon_{ijkl} &amp; \\sim \\mathcal{N}(0, \\sigma^2_\\varepsilon) \\\\ \\end{align*}\\] require(lme4) require(lattice) ## Loading required package: lattice tick &lt;- read.table(&quot;data/tick.txt&quot;, head = T) names(tick) &lt;- c(&quot;index&quot;, &quot;ticks&quot;, &quot;brood&quot;, &quot;elevation&quot;, &quot;yr&quot;, &quot;loc&quot;) tick$index &lt;- as.factor(tick$index) tick$brood &lt;- as.factor(tick$brood) tick$yr &lt;- as.factor(tick$yr) tick$loc &lt;- as.factor(tick$loc) # center and scale elevation tick$elev.z &lt;- with(tick, (elevation - mean(elevation)) / sd(elevation)) Model fitting: fm1 &lt;- glmer(ticks ~ yr + elev.z + (1 | loc) + (1 | brood) + (1 | index), family = &quot;poisson&quot;, data = tick) summary(fm1) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: ticks ~ yr + elev.z + (1 | loc) + (1 | brood) + (1 | index) ## Data: tick ## ## AIC BIC logLik deviance df.resid ## 1794.5 1822.5 -890.3 1780.5 396 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6123 -0.5536 -0.1486 0.2850 2.4430 ## ## Random effects: ## Groups Name Variance Std.Dev. ## index (Intercept) 0.2932 0.5415 ## brood (Intercept) 0.5625 0.7500 ## loc (Intercept) 0.2796 0.5287 ## Number of obs: 403, groups: index, 403; brood, 118; loc, 63 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.3728 0.1964 1.898 0.057639 . ## yr96 1.1804 0.2381 4.957 7.15e-07 *** ## yr97 -0.9787 0.2628 -3.724 0.000196 *** ## elev.z -0.8543 0.1236 -6.910 4.83e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) yr96 yr97 ## yr96 -0.728 ## yr97 -0.610 0.514 ## elev.z 0.011 0.048 0.047 We can have a look at the profile confidence intervals and profile confidence regions for each of the model parameters. pp &lt;- profile(fm1) confint(pp) ## 2.5 % 97.5 % ## .sig01 0.45148400 0.6451853 ## .sig02 0.52127907 1.0569688 ## .sig03 0.00000000 0.8928761 ## (Intercept) -0.02822382 0.7485777 ## yr96 0.71308911 1.6583691 ## yr97 -1.50239867 -0.4606278 ## elev.z -1.10589101 -0.6090505 xyplot(pp, absVal = TRUE) splom(pp) We see that the standard deviation of the location-level random effect (“.sig03”) has a 95% confidence interval that includes 0. We might conclude that the location-level random effect is unnecessary. In other words, there is no evidence that different broods from the same location are more strongly (positively) correlated than two broods at different locations at the same elevation. The bivariate confidence regions show us that the estimated SD of the location-level random effect is negatively correlated with the estimated SD of the brood-level random effect (“.sig02”). Thus, the location-to-location variability (above and beyond the elevation effect) from brood-to-brood variability are confounded, which makes sense, given that most locations are represented by a small number of broods. It is something of a judgment call as to whether it would make sense at this point to drop the location-level random effect. We could retain it on the grounds that one expects some location-to-location variation beyond the effect of elevation, even if that variation is small. To test for the significance of a random effect, the usual approach is to conduct a likelihood-ratio test to compare models with and without the random effect. In this case, the null hypothesis is that the variance of the tested random effect is 0, which is on the boundary of the allowable values for a variance. Thus, the \\(p\\)-value from a LRT is conservative (too big). In simple models, the \\(p\\)-value for the LRT is twice as big as it should be, suggesting that the appropriate correction is to divide the \\(p\\)-value by 2 (Pinheiro and Bates (2000)). For more complex models, however, the divide-by-2 rule is only a rough rule of thumb. We illustrate by comparing a model with the location-level random effect to one without it. (In notation, we are testing \\(\\sigma^2_L = 0\\).) fm2 &lt;- glmer(ticks ~ yr + elev.z + (1 | brood) + (1 | index), family = &quot;poisson&quot;, data = tick) anova(fm2, fm1) ## Data: tick ## Models: ## fm2: ticks ~ yr + elev.z + (1 | brood) + (1 | index) ## fm1: ticks ~ yr + elev.z + (1 | loc) + (1 | brood) + (1 | index) ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## fm2 6 1794.0 1818.0 -891.02 1782.0 ## fm1 7 1794.5 1822.5 -890.27 1780.5 1.4973 1 0.2211 If we use the rough divide-by-2 rule, the approximate \\(p\\)-value is \\(p \\approx 0.11\\). Thus, the model with the location-level random effect does not improve significantly on the model without the location-level random effect. We might conclude that the location-level random effect is unnecessary. Although this analysis focused on how different random effects contributed to the overall variation in tick load, it is also helpful to visualize the fit of the model. The plot below shows the tick load for each individual (plotted on a log + 1 scale) vs. the centered and scaled elevation variable, along with the fitted mean line for each year. Note that the curvature in the fit appears because the response is shown as the log + 1 instead of the log. The fits would be straight lines on the log scale. par(mfrow = c(1, 3)) plot.subset &lt;- function(year, a, b) { with(tick, plot(log(ticks + 1) ~ elev.z, type = &quot;n&quot;, main = year)) with(subset(tick, yr == year), points(jitter(log(ticks + 1)) ~ elev.z)) fit &lt;- function(x) log(1 + exp(a + b * x)) curve(fit, from = min(tick$elev.z), to = max(tick$elev.z), add = TRUE, col = &quot;red&quot;) } plot.subset(&quot;95&quot;, a = 0.3728, b = -0.8543) plot.subset(&quot;96&quot;, a = 0.3728 + 1.1804, b = -0.8543) plot.subset(&quot;97&quot;, a = 0.3728 - 0.9787, b = -0.8543) 8.3 GAMMs Generalized additive mixed models (GAMMs) include just about every model feature we’ve discussed: splines to capture smooth effects of predictors, non-Gaussian responses, and correlated errors. There are two software routines available for fitting GAMMs in R: mgcv::gamm and gamm4::gamm4. The routine mgcv::gamm is based on lme, and thus provides access to the non-constant variance and correlation structures that we saw when discussing generalized least squares. The routine gamm4::gamm4 is based on lme4, and thus provides access to the same fitting syntax as lmer and glmer. We will illustrate each in turn. 8.3.1 mgcv::gamm As we have seen, serial data usually have a serial dependence structure. They are also data for which one might want to use splines to capture the underlying trend. Time series provide a prime example. Below, we will analyze daily average temperature data from RDU from January 1, 1995 to August 27, 2025.9 First, some housekeeping and exploratory analysis. rdu &lt;- read.table(&quot;data/rdu-temperature.txt&quot;, head = T) # remove NA&#39;s, coded as -99 with(rdu, table(temp == -99)) ## ## FALSE TRUE ## 11182 15 rdu &lt;- subset(rdu, temp &gt; -99) with(rdu, plot(temp ~ time, type = &quot;l&quot;, xlab = &quot;day&quot;)) We will fit a model that is the sum of two splines: a cyclic spine to capture the within-year trend in temperature, and a smoothing spline to capture the among-year trend in temperature. We also include an AR(1) structure on the errors. The AR(1) structure really should pertain to the entire time series, but the fitting takes too long if we do so. Instead, we just fit the AR(1) structure to the errors within each year, which is only a minimal modification of the model (the only consequence is that we have assumed the errors on Dec 31 and the following Jan 1 are independent), but it allows the model to be fit more quickly. require(mgcv) ## Loading required package: mgcv ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmList ## This is mgcv 1.9-3. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;. fm1 &lt;- gamm(temp ~ s(doy, bs = &quot;cc&quot;) + s(time), data = rdu, correlation = corAR1(form = ~ 1 | yr)) The output of mgcv::gamm is a list of two parts. The first part, named lme, includes the output of the model that includes most of the model fit except the smooth terms. The second part, named gam, includes any smoothing splines. For the model above, most of the interesting elements are in the gam portion. We’ll look at the lme portion, too, as this contains the estimate of the correlation parameter between consecutive days. summary(fm1$lme) ## Linear mixed-effects model fit by maximum likelihood ## Data: strip.offset(mf) ## AIC BIC logLik ## 69855.65 69899.58 -34921.83 ## ## Random effects: ## Formula: ~Xr - 1 | g ## Structure: pdIdnot ## Xr1 Xr2 Xr3 Xr4 Xr5 Xr6 Xr7 Xr8 ## StdDev: 1.856569 1.856569 1.856569 1.856569 1.856569 1.856569 1.856569 1.856569 ## ## Formula: ~Xr.0 - 1 | g.0 %in% g ## Structure: pdIdnot ## Xr.01 Xr.02 Xr.03 Xr.04 Xr.05 Xr.06 ## StdDev: 0.008279714 0.008279714 0.008279714 0.008279714 0.008279714 0.008279714 ## Xr.07 Xr.08 Residual ## StdDev: 0.008279714 0.008279714 7.523312 ## ## Correlation Structure: AR(1) ## Formula: ~1 | g/g.0/yr ## Parameter estimate(s): ## Phi ## 0.6845975 ## Fixed effects: y ~ X - 1 ## Value Std.Error DF t-value p-value ## X(Intercept) 61.09894 0.1635474 11180 373.5856 0 ## Xs(time)Fx1 1.04041 0.1634872 11180 6.3639 0 ## Correlation: ## X(Int) ## Xs(time)Fx1 0 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.135868506 -0.651586784 -0.001947138 0.594334324 3.671830628 ## ## Number of Observations: 11182 ## Number of Groups: ## g g.0 %in% g ## 1 1 summary(fm1$gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## temp ~ s(doy, bs = &quot;cc&quot;) + s(time) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 61.0989 0.1635 373.6 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(doy) 7.438 8 839.69 &lt;2e-16 *** ## s(time) 1.000 1 40.51 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.762 ## Scale est. = 56.6 n = 11182 plot(fm1$gam) Intriguingly, but not surprisingly, the fit to the within-year trend clearly shows that the spring warm-up in Raleigh is decidedly more gradual than the fall cool-down. Fall in the Piedmont is ever fleeting. Less substantially, but still interestingly, the estimate of the correlation between temperature anomalies on consecutive days is \\(\\approx\\) 0.68, which matches experience. The best-fitting smoothing spline for the among-year trend is linear. Let’s replace the smoothing spline by a linear term so that it is easier to extract the slope, which will now be contained in the lme portion. fm2 &lt;- gamm(temp ~ s(doy, bs = &quot;cc&quot;) + time, data = rdu, correlation = corAR1(form = ~ 1 | yr)) summary(fm2$lme) ## Linear mixed-effects model fit by maximum likelihood ## Data: strip.offset(mf) ## AIC BIC logLik ## 69853.65 69890.26 -34921.83 ## ## Random effects: ## Formula: ~Xr - 1 | g ## Structure: pdIdnot ## Xr1 Xr2 Xr3 Xr4 Xr5 Xr6 Xr7 Xr8 ## StdDev: 1.856608 1.856608 1.856608 1.856608 1.856608 1.856608 1.856608 1.856608 ## Residual ## StdDev: 7.523295 ## ## Correlation Structure: AR(1) ## Formula: ~1 | g/yr ## Parameter estimate(s): ## Phi ## 0.6845958 ## Fixed effects: y ~ X - 1 ## Value Std.Error DF t-value p-value ## X(Intercept) 59.29523 0.3272265 11180 181.20547 0 ## Xtime 0.00032 0.0000506 11180 6.36501 0 ## Correlation: ## X(Int) ## Xtime -0.866 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.135878917 -0.651585309 -0.001948106 0.594336225 3.671840351 ## ## Number of Observations: 11182 ## Number of Groups: 1 The temperature trend is estimated as an increase of 3.22^{-4} \\(^\\circ\\)F per day. That equates to a trend of 0.1176 \\(^\\circ\\)F per year, or 1.176 per decade. Yikes! To see the effect of the AR(1) correlation structure, let’s compare our model fit to one that doesn’t account for autocorrelated errors. fm1a &lt;- gam(temp ~ s(doy, bs = &quot;cc&quot;) + s(time), data = rdu) plot(fm1a) abline(h = 0, col = &quot;red&quot;) Without the autocorrelated errors, both smoothing splines are quite a bit wigglier. The confidence intervals around the fit are also too small. Both indicate overfitting. Accounting for the serial correlations in the errors has provided a substantially improved description of the trends in the data. The temperature increase in the RDU data is hard to believe. As a sanity check, we can plot the average recorded temperature for all (nearly) complete years from 1995–2024. (We say nearly complete because some years have occasional missing values.) yr_avg &lt;- with(subset(rdu, yr &lt; 2025), aggregate(temp, list(yr), FUN = mean)) names(yr_avg) &lt;- c(&quot;year&quot;, &quot;avg_temp&quot;) with(yr_avg, plot(avg_temp ~ year)) These data are a little odd because the source of the data changes after 2020 (and the two sources don’t completely agree for temperature data before 2020). The data for 2023–24 are so anomalous that it makes one wonder if something about the measurement device changed. Even prior to 2023, though, the trend in average annual temperature is clear. 8.3.2 gamm::gamm4 Finally, we will use gamm4::gamm4 to fit a new model to the tick data from Elston et al. (2001), this time using a smoothing spline to estimate the effect of elevation on tick abundance. Like mgcv::gamm, gamm4::gamm4 returns models with two compoments: one called mer that contains output from the portion of the model that invokes lme4::(g)lmer, and one called gam that contains the smoothing terms. require(gamm4) fm4 &lt;- gamm4(ticks ~ yr + s(elev.z), random = ~ (1 | loc) + (1 | brood) + (1 | index), family = &quot;poisson&quot;, data = tick) summary(fm4$mer) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## ## AIC BIC logLik -2*log(L) df.resid ## 1796.5 1828.5 -890.3 1780.5 395 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6123 -0.5536 -0.1486 0.2850 2.4430 ## ## Random effects: ## Groups Name Variance Std.Dev. ## index (Intercept) 2.932e-01 0.5415095 ## brood (Intercept) 5.626e-01 0.7500387 ## loc (Intercept) 2.795e-01 0.5287004 ## Xr s(elev.z) 7.426e-08 0.0002725 ## Number of obs: 403, groups: index, 403; brood, 118; loc, 63; Xr, 8 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## X(Intercept) 0.3728 0.1964 1.898 0.057632 . ## Xyr96 1.1804 0.2381 4.957 7.15e-07 *** ## Xyr97 -0.9787 0.2628 -3.724 0.000196 *** ## Xs(elev.z)Fx1 -0.8533 0.1235 -6.910 4.83e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## X(Int) Xyr96 Xyr97 ## Xyr96 -0.728 ## Xyr97 -0.610 0.514 ## Xs(lv.z)Fx1 0.011 0.048 0.047 summary(fm4$gam) ## ## Family: poisson ## Link function: log ## ## Formula: ## ticks ~ yr + s(elev.z) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.3728 0.1904 1.958 0.050222 . ## yr96 1.1804 0.2356 5.010 5.45e-07 *** ## yr97 -0.9787 0.2630 -3.722 0.000198 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(elev.z) 1 1 48.03 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.156 ## glmer.ML = 220.92 Scale est. = 1 n = 403 plot(fm4$gam) Our best fitting model continues to contain a linear association between elevation and tick abundance. Again, it is interesting to compare this fit to one without the random effects for brood or location, and to see how the absence of these random effects produces a substantially different (and presumably much over-fit) relationship between elevation and tick abundance. fm5 &lt;- gam(ticks ~ yr + s(elev.z), family = &quot;poisson&quot;, data = tick) plot(fm5) Bibliography Bishop, JA. 1972. “An Experimental Study of the Cline of Industrial Melanism in Biston Betularia (l.)(lepidoptera) Between Urban Liverpool and Rural North Wales.” The Journal of Animal Ecology, 209–43. Elston, DA, Robert Moss, T Boulinier, C Arrowsmith, and Xavier Lambin. 2001. “Analysis of Aggregation, a Worked Example: Numbers of Ticks on Red Grouse Chicks.” Parasitology 122 (5): 563–69. Fox, Gordon A, Simoneta Negrete-Yankelevich, and Vinicio J Sosa. 2015. Ecological Statistics: Contemporary Theory and Application. Oxford University Press, USA. Pinheiro, José, and Douglas Bates. 2000. Mixed-Effects Models in S and S-PLUS. Springer. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Pacific Grove, CA: Duxbury. Wood, Simon N. 2017. Generalized Additive Models: An Introduction with r. CRC press. This analysis is inspired by and modeled after a comparable analysis in section 7.7.2 of Wood (2017). The first portion of these data were downloaded from Kelly Kissock’s website at the University of Dayton, although that website no longer seems to be maintained. The more recent data were downloaded from NOAA’s Climate Data Online (CDO) portal, https://www.ncdc.noaa.gov/cdo-web/search .↩︎ "],["bibliography.html", "Bibliography", " Bibliography Bates, Douglas M. 2012+. lme4: Mixed-Effects Modeling with R. Unpublished. Bishop, JA. 1972. “An Experimental Study of the Cline of Industrial Melanism in Biston Betularia (l.)(lepidoptera) Between Urban Liverpool and Rural North Wales.” The Journal of Animal Ecology, 209–43. Bolker, Benjamin M. 2008. Ecological Models and Data in R. Princeton University Press. Elston, DA, Robert Moss, T Boulinier, C Arrowsmith, and Xavier Lambin. 2001. “Analysis of Aggregation, a Worked Example: Numbers of Ticks on Red Grouse Chicks.” Parasitology 122 (5): 563–69. Fox, Gordon A, Simoneta Negrete-Yankelevich, and Vinicio J Sosa. 2015. Ecological Statistics: Contemporary Theory and Application. Oxford University Press, USA. Gillibrand, EJV, AJ Jamieson, PM Bagley, Alain F Zuur, and IG Priede. 2007. “Seasonal Development of a Deep Pelagic Bioluminescent Layer in the Temperate NE Atlantic Ocean.” Marine Ecology Progress Series 341: 37–44. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in r. 2nd ed. Springer. Kendall, Maurice George, and Alan Stuart. 1979. The Advanced Theory of Statistics. Vol. 2: Inference and Relationship. 4th ed. London: Griffin. Loyn, RH. 1987. “Effects of Patch Area and Habitat on Bird Abundances, Species Numbers and Tree Health in Fragmented Victorian Forests.” In Nature Conservation: The Role of Remnants of Native Vegetation, edited by A. A. Burbidge DA Saunders GW Arnold and AJM Hopkins, 65–77. Chipping Norton, NSW: Surrey Beatty &amp; Sons. Madin, Joshua S, Andrew H Baird, Maria Dornelas, and Sean R Connolly. 2014. “Mechanical Vulnerability Explains Size-Dependent Mortality of Reef Corals.” Ecology Letters 17 (8): 1008–15. Pinheiro, José, and Douglas Bates. 2000. Mixed-Effects Models in S and S-PLUS. Springer. Poole, Joyce H. 1989. “Mate Guarding, Reproductive Success and Female Choice in African Elephants.” Animal Behaviour 37: 842–49. Ramsey, Fred, and Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2nd ed. Pacific Grove, CA: Duxbury. Vicente, Joaquı́n, Ursula Höfle, Joseba Garrido, Isabel G Fernández-De-Mera, Ramón Juste, Marta Barral, and Christian Gortazar. 2006. “Wild Boar and Red Deer Display High Prevalences of Tuberculosis-Like Lesions in Spain.” Veterinary Research 37 (1): 107–19. Vonesh, James R, and Benjamin M Bolker. 2005. “Compensatory Larval Responses Shift Trade-Offs Associated with Predator-Induced Hatching Plasticity.” Ecology 86 (6): 1580–91. Wood, Simon N. 2017. Generalized Additive Models: An Introduction with r. CRC press. Zuur, Alain F, Elena N Ieno, and Graham M Smith. 2007. Analysing Ecological Data. Springer. Zuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with R. Vol. 574. Springer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
