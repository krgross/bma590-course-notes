<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Bayesian computation | Applied statistical analysis of non-normal and/or correlated data</title>
  <meta name="description" content="This is a proto-textbook for BMA / ST 590, Statistical Modeling in Ecology." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Bayesian computation | Applied statistical analysis of non-normal and/or correlated data" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a proto-textbook for BMA / ST 590, Statistical Modeling in Ecology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Bayesian computation | Applied statistical analysis of non-normal and/or correlated data" />
  
  <meta name="twitter:description" content="This is a proto-textbook for BMA / ST 590, Statistical Modeling in Ecology." />
  

<meta name="author" content="Kevin Gross" />


<meta name="date" content="2025-10-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="beyondML.html"/>
<link rel="next" href="smoothing-and-gams.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Maximum likelihood estimation</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#mathematical-basics"><i class="fa fa-check"></i><b>1.1</b> Mathematical basics</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#horse"><i class="fa fa-check"></i><b>1.2</b> Horse-kick data</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#calculate-and-plot-the-log-likelihood-function"><i class="fa fa-check"></i><b>1.2.1</b> Calculate and plot the log-likelihood function</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#find-the-mle-numerically-using-optimize"><i class="fa fa-check"></i><b>1.2.2</b> Find the MLE numerically using ‘optimize’</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#pulse-rate-data"><i class="fa fa-check"></i><b>1.3</b> Pulse rate data</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#tadpole-data"><i class="fa fa-check"></i><b>1.4</b> Tadpole data</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#transformable-constraints"><i class="fa fa-check"></i><b>1.5</b> Transformable constraints</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="beyondML.html"><a href="beyondML.html"><i class="fa fa-check"></i><b>2</b> Beyond the MLE: Confidence regions and hypothesis tests using the likelihood function</a>
<ul>
<li class="chapter" data-level="2.1" data-path="beyondML.html"><a href="beyondML.html#confidence-intervals-for-single-parameters"><i class="fa fa-check"></i><b>2.1</b> Confidence intervals for single parameters</a></li>
<li class="chapter" data-level="2.2" data-path="beyondML.html"><a href="beyondML.html#two-param-mle"><i class="fa fa-check"></i><b>2.2</b> Confidence regions, profile likelihoods, and associated univariate intervals</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="beyondML.html"><a href="beyondML.html#profile-likelihoods"><i class="fa fa-check"></i><b>2.2.1</b> Profile likelihoods</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="beyondML.html"><a href="beyondML.html#quadapprox"><i class="fa fa-check"></i><b>2.3</b> Quadratic approximations to confidence intervals and regions</a></li>
<li class="chapter" data-level="2.4" data-path="beyondML.html"><a href="beyondML.html#comparing-models-likelihood-ratio-test-and-aic"><i class="fa fa-check"></i><b>2.4</b> Comparing models: Likelihood ratio test and AIC</a></li>
<li class="chapter" data-level="2.5" data-path="beyondML.html"><a href="beyondML.html#the-negative-binomial-distriution-revisited"><i class="fa fa-check"></i><b>2.5</b> The negative binomial distriution, revisited</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-computation.html"><a href="bayesian-computation.html"><i class="fa fa-check"></i><b>3</b> Bayesian computation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#computations-with-conjugate-priors"><i class="fa fa-check"></i><b>3.1</b> Computations with conjugate priors</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#mcmc-and-stochastic-approximations-of-the-posterior"><i class="fa fa-check"></i><b>3.2</b> MCMC and stochastic approximations of the posterior</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="bayesian-computation.html"><a href="bayesian-computation.html#the-horse-kick-data-once-more"><i class="fa fa-check"></i><b>3.2.1</b> The horse-kick data, once more</a></li>
<li class="chapter" data-level="3.2.2" data-path="bayesian-computation.html"><a href="bayesian-computation.html#a-simple-regression-example"><i class="fa fa-check"></i><b>3.2.2</b> A simple regression example</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayesian-computation.html"><a href="bayesian-computation.html#rstanarm"><i class="fa fa-check"></i><b>3.3</b> rstanarm</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="smoothing-and-gams.html"><a href="smoothing-and-gams.html"><i class="fa fa-check"></i><b>4</b> Smoothing and GAMs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="smoothing-and-gams.html"><a href="smoothing-and-gams.html#loess-smoothers"><i class="fa fa-check"></i><b>4.1</b> Loess smoothers</a></li>
<li class="chapter" data-level="4.2" data-path="smoothing-and-gams.html"><a href="smoothing-and-gams.html#splines"><i class="fa fa-check"></i><b>4.2</b> Splines</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="smoothing-and-gams.html"><a href="smoothing-and-gams.html#regression-splines"><i class="fa fa-check"></i><b>4.2.1</b> Regression splines</a></li>
<li class="chapter" data-level="4.2.2" data-path="smoothing-and-gams.html"><a href="smoothing-and-gams.html#natural-splines"><i class="fa fa-check"></i><b>4.2.2</b> Natural splines</a></li>
<li class="chapter" data-level="4.2.3" data-path="smoothing-and-gams.html"><a href="smoothing-and-gams.html#smoothing-splines"><i class="fa fa-check"></i><b>4.2.3</b> Smoothing splines</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="smoothing-and-gams.html"><a href="smoothing-and-gams.html#GAMs"><i class="fa fa-check"></i><b>4.3</b> Generalized additive models (GAMs)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>5</b> Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="5.1" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#heterogeneous-variance"><i class="fa fa-check"></i><b>5.1</b> Heterogeneous variance</a></li>
<li class="chapter" data-level="5.2" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#temporal-serial-correlation"><i class="fa fa-check"></i><b>5.2</b> Temporal (serial) correlation</a></li>
<li class="chapter" data-level="5.3" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html#spatial-data"><i class="fa fa-check"></i><b>5.3</b> Spatial data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html"><i class="fa fa-check"></i><b>6</b> Hierarchical (mixed) models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#one-factor-layout-dyestuff-data"><i class="fa fa-check"></i><b>6.1</b> One-factor layout: Dyestuff data</a></li>
<li class="chapter" data-level="6.2" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#bayesian-analysis"><i class="fa fa-check"></i><b>6.2</b> Bayesian analysis</a></li>
<li class="chapter" data-level="6.3" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#negative-within-group-correlations"><i class="fa fa-check"></i><b>6.3</b> Negative within-group correlations</a></li>
<li class="chapter" data-level="6.4" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#random-coefficient-models-rikz-data"><i class="fa fa-check"></i><b>6.4</b> Random coefficient models: RIKZ data</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#analysis-without-beach-level-covariate"><i class="fa fa-check"></i><b>6.4.1</b> Analysis without beach-level covariate</a></li>
<li class="chapter" data-level="6.4.2" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#adding-a-beach-level-covariate"><i class="fa fa-check"></i><b>6.4.2</b> Adding a beach-level covariate</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#nested-and-crossed-random-effects"><i class="fa fa-check"></i><b>6.5</b> Nested and crossed random effects</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#nested-random-effects"><i class="fa fa-check"></i><b>6.5.1</b> Nested random effects</a></li>
<li class="chapter" data-level="6.5.2" data-path="hierarchical-mixed-models.html"><a href="hierarchical-mixed-models.html#crossed-random-effects"><i class="fa fa-check"></i><b>6.5.2</b> Crossed random effects</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>7</b> Generalized linear models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glms-the-big-picture"><i class="fa fa-check"></i><b>7.1</b> GLMs: The big picture</a></li>
<li class="chapter" data-level="7.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>7.2</b> Poisson regression</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#horse-kick-data-revisited"><i class="fa fa-check"></i><b>7.2.1</b> Horse-kick data revisited</a></li>
<li class="chapter" data-level="7.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#elephant-matings"><i class="fa fa-check"></i><b>7.2.2</b> Elephant matings</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binary-responses"><i class="fa fa-check"></i><b>7.3</b> Binary responses</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#individual-binary-responses-tb-in-boar"><i class="fa fa-check"></i><b>7.3.1</b> Individual binary responses: TB in boar</a></li>
<li class="chapter" data-level="7.3.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#grouped-binary-data-tb-in-red-deer"><i class="fa fa-check"></i><b>7.3.2</b> Grouped binary data: TB in red deer</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#zero-adjusted-models-for-count-data"><i class="fa fa-check"></i><b>7.4</b> Zero-adjusted models for count data</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#zero-truncated-models"><i class="fa fa-check"></i><b>7.4.1</b> Zero-truncated models</a></li>
<li class="chapter" data-level="7.4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#zero-inflated-models"><i class="fa fa-check"></i><b>7.4.2</b> Zero-inflated models</a></li>
<li class="chapter" data-level="7.4.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#zero-altered-or-hurdle-models"><i class="fa fa-check"></i><b>7.4.3</b> Zero-altered, or “hurdle”, models</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#generalized-additive-models-gams"><i class="fa fa-check"></i><b>7.5</b> Generalized additive models (GAMs)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html"><i class="fa fa-check"></i><b>8</b> Generalized linear mixed models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#example-1-industrial-melanism-data"><i class="fa fa-check"></i><b>8.1</b> Example 1: Industrial melanism data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#gees"><i class="fa fa-check"></i><b>8.1.1</b> GEEs</a></li>
<li class="chapter" data-level="8.1.2" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#glmms"><i class="fa fa-check"></i><b>8.1.2</b> GLMMs</a></li>
<li class="chapter" data-level="8.1.3" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#bayesian-fit"><i class="fa fa-check"></i><b>8.1.3</b> Bayesian fit</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#example-2-ticks-on-red-grouse"><i class="fa fa-check"></i><b>8.2</b> Example 2: Ticks on red grouse</a></li>
<li class="chapter" data-level="8.3" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#GAMMs"><i class="fa fa-check"></i><b>8.3</b> GAMMs</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied statistical analysis of non-normal and/or correlated data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-computation" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Bayesian computation<a href="bayesian-computation.html#bayesian-computation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter of the computing companion will focus solely on the computing aspects of Bayesian computation in R. See the relevant sections of Bolker for the underlying theory.</p>
<p>The landscape of computing tools available to fit Bayesian models is fluid. Here, we will look at three tools currently available: <code>R2jags</code>, which is based on the JAGS (Just Another Gibbs Sampler) platform, <code>rstan</code>, which is based on the computer program Stan (itself based on Hamiltonian Monte Carlo, or HMC), and the recent <code>rstanarm</code>, which seeks to put much of the computational details in the background. (The “arm” portion of the name <code>rstanarm</code> is an acronym for applied regression modeling.)</p>
<p>Throughout, we will be working with two data sets: the horse-kick data (again), and a data set that details how the rate at which a cricket chirps depends on the air temperature. The horse-kick data are useful in this context because a Gamma distribution is a conjugate prior for Poisson data. Thus, if we use a Gamma prior, then we know the posterior exactly. Therefore, we can compare the approximations provided by stochastic sampling schemes to the known posterior. The cricket data set will be used as an example of a simple linear regression, even though the data hint that the actual relationship between temperature and the rate of chirping is nonlinear.</p>
<div id="computations-with-conjugate-priors" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Computations with conjugate priors<a href="bayesian-computation.html#computations-with-conjugate-priors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose that we observe an iid random sample <span class="math inline">\(X_1, \ldots, X_n\)</span> from a Poisson distribution with unknown parameter <span class="math inline">\(\lambda\)</span>. (This is the setting for the horse-kick data.) If we place a Gamma prior with shape parameter <span class="math inline">\(a\)</span> and rate parameter <span class="math inline">\(r\)</span> on <span class="math inline">\(\lambda\)</span>, then the posterior distribution is also Gamma with shape parameter <span class="math inline">\(a + \sum_n X_n\)</span> and rate parameter <span class="math inline">\(r + n\)</span>. In other words,
<span class="math display">\[\begin{align*}
\lambda &amp; \sim \mbox{Gamma}(a, r) \\
X_1, \ldots, X_n &amp; \sim \mbox{Pois}(\lambda) \\
\lambda | X_1, \ldots, X_n &amp; \sim \mbox{Gamma}(a + \sum_n X_n, r + n) \\
\end{align*}\]</span></p>
<p>In the horse-kick data, <span class="math inline">\(\sum_n x_n = 196\)</span> and <span class="math inline">\(n = 280\)</span>. Suppose we start with the vague Gamma prior <span class="math inline">\(a=.01\)</span>, <span class="math inline">\(r = .01\)</span> on <span class="math inline">\(\lambda\)</span>. This prior has mean <span class="math inline">\(a/r = 1\)</span> and variance <span class="math inline">\(a/r^2 = 100\)</span>. The posterior distribution for <span class="math inline">\(\lambda\)</span> is then a Gamma with shape parameter <span class="math inline">\(a = 196.01\)</span> and rate parameter <span class="math inline">\(280.01\)</span>. We can plot it:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="bayesian-computation.html#cb175-1" tabindex="-1"></a>horse <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;data/horse.txt&quot;</span>, </span>
<span id="cb175-2"><a href="bayesian-computation.html#cb175-2" tabindex="-1"></a>                    <span class="at">header =</span> <span class="cn">TRUE</span>,</span>
<span id="cb175-3"><a href="bayesian-computation.html#cb175-3" tabindex="-1"></a>                    <span class="at">stringsAsFactors =</span> <span class="cn">TRUE</span>)</span>
<span id="cb175-4"><a href="bayesian-computation.html#cb175-4" tabindex="-1"></a></span>
<span id="cb175-5"><a href="bayesian-computation.html#cb175-5" tabindex="-1"></a>l.vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">2</span>, <span class="at">length =</span> <span class="dv">200</span>)</span>
<span id="cb175-6"><a href="bayesian-computation.html#cb175-6" tabindex="-1"></a><span class="fu">plot</span>(l.vals, <span class="fu">dgamma</span>(l.vals, <span class="at">shape =</span> <span class="fl">196.01</span>, <span class="at">rate =</span> <span class="fl">280.01</span>), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb175-7"><a href="bayesian-computation.html#cb175-7" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb175-8"><a href="bayesian-computation.html#cb175-8" tabindex="-1"></a><span class="fu">lines</span>(l.vals, <span class="fu">dgamma</span>(l.vals, <span class="at">shape =</span> .<span class="dv">01</span>, <span class="at">rate =</span> .<span class="dv">01</span>), <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb175-9"><a href="bayesian-computation.html#cb175-9" tabindex="-1"></a></span>
<span id="cb175-10"><a href="bayesian-computation.html#cb175-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.7</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb175-11"><a href="bayesian-computation.html#cb175-11" tabindex="-1"></a></span>
<span id="cb175-12"><a href="bayesian-computation.html#cb175-12" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">leg =</span> <span class="fu">c</span>(<span class="st">&quot;prior&quot;</span>, <span class="st">&quot;posterior&quot;</span>), </span>
<span id="cb175-13"><a href="bayesian-computation.html#cb175-13" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="st">&quot;dashed&quot;</span>, <span class="st">&quot;solid&quot;</span>))</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The red line shows the MLE, which is displaced slightly from the posterior mode.</p>
<p>As a point estimate, we might consider any of the following. The posterior mean can be found exactly as <span class="math inline">\(a/r\)</span> = 0.70001. Alternatively, we might consider the posterior median</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="bayesian-computation.html#cb176-1" tabindex="-1"></a><span class="fu">qgamma</span>(<span class="fl">0.5</span>, <span class="at">shape =</span> <span class="fl">196.01</span>, <span class="at">rate =</span> <span class="fl">280.01</span>)</span></code></pre></div>
<pre><code>## [1] 0.6988206</code></pre>
<p>Finally, we might conisder the posterior mode:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="bayesian-computation.html#cb178-1" tabindex="-1"></a><span class="fu">optimize</span>(<span class="at">f =</span> <span class="cf">function</span>(x) <span class="fu">dgamma</span>(x, <span class="at">shape =</span> <span class="fl">196.01</span>, <span class="at">rate =</span> <span class="fl">280.01</span>), <span class="at">interval =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>), <span class="at">maximum =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## $maximum
## [1] 0.6964383
## 
## $objective
## [1] 7.995941</code></pre>
<p>To find a 95% confidence interval, we might consider the central 95% interval:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="bayesian-computation.html#cb180-1" tabindex="-1"></a><span class="fu">qgamma</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="at">shape =</span> <span class="fl">196.01</span>, <span class="at">rate =</span> <span class="fl">280.01</span>)</span></code></pre></div>
<pre><code>## [1] 0.6054387 0.8013454</code></pre>
<p>A 95% highest posterior density (HPD) interval takes a bit more work. We’ll write a function to compute the width of a 95% credible interval based on the quantile of the upper endpoint, and then find the quantile that minimizes this width.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="bayesian-computation.html#cb182-1" tabindex="-1"></a>interval.width <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb182-2"><a href="bayesian-computation.html#cb182-2" tabindex="-1"></a>  </span>
<span id="cb182-3"><a href="bayesian-computation.html#cb182-3" tabindex="-1"></a>  upper <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="at">p =</span> x, <span class="at">shape =</span> <span class="fl">196.01</span>, <span class="at">rate =</span> <span class="fl">280.01</span>)</span>
<span id="cb182-4"><a href="bayesian-computation.html#cb182-4" tabindex="-1"></a>  lower <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="at">p =</span> x <span class="sc">-</span> .<span class="dv">95</span>, <span class="at">shape =</span> <span class="fl">196.01</span>, <span class="at">rate =</span> <span class="fl">280.01</span>)</span>
<span id="cb182-5"><a href="bayesian-computation.html#cb182-5" tabindex="-1"></a>  </span>
<span id="cb182-6"><a href="bayesian-computation.html#cb182-6" tabindex="-1"></a>  upper <span class="sc">-</span> lower</span>
<span id="cb182-7"><a href="bayesian-computation.html#cb182-7" tabindex="-1"></a>}</span>
<span id="cb182-8"><a href="bayesian-computation.html#cb182-8" tabindex="-1"></a></span>
<span id="cb182-9"><a href="bayesian-computation.html#cb182-9" tabindex="-1"></a>(upper.qtile <span class="ot">&lt;-</span> <span class="fu">optimize</span>(interval.width, <span class="at">interval =</span> <span class="fu">c</span>(<span class="fl">0.95</span>, <span class="dv">1</span>))<span class="sc">$</span>minimum)</span></code></pre></div>
<pre><code>## [1] 0.9722295</code></pre>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="bayesian-computation.html#cb184-1" tabindex="-1"></a>(hpd.ci <span class="ot">&lt;-</span> <span class="fu">qgamma</span>(<span class="at">p =</span> <span class="fu">c</span>(upper.qtile <span class="sc">-</span> .<span class="dv">95</span>, upper.qtile), <span class="at">shape =</span> <span class="fl">196.01</span>, <span class="at">rate =</span> <span class="fl">280.01</span>))</span></code></pre></div>
<pre><code>## [1] 0.6031834 0.7988678</code></pre>
<p>We might also ask questions like: What is the posterior probability that <span class="math inline">\(\lambda &gt; 2/3\)</span>? These caluclations are straightforward in a Bayesian context, and they make full sense.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="bayesian-computation.html#cb186-1" tabindex="-1"></a><span class="fu">pgamma</span>(<span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>, <span class="at">shape =</span> <span class="fl">196.01</span>, <span class="at">rate =</span> <span class="fl">280.01</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.7434032</code></pre>
<p>Thus we would say that there is a 0.743 posterior probability that <span class="math inline">\(\lambda &gt; 2/3\)</span>.</p>
<p>As an illustration, note that if we had begun with a more informative prior — say, a gamma distribution with shape parameter <span class="math inline">\(a = 50\)</span> and rate parameter = <span class="math inline">\(100\)</span> — then the posterior would have been more of a compromise between the prior and the information in the data:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="bayesian-computation.html#cb188-1" tabindex="-1"></a><span class="fu">plot</span>(l.vals, <span class="fu">dgamma</span>(l.vals, <span class="at">shape =</span> <span class="dv">196</span> <span class="sc">+</span> <span class="dv">50</span>, <span class="at">rate =</span> <span class="dv">100</span> <span class="sc">+</span> <span class="dv">280</span>), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb188-2"><a href="bayesian-computation.html#cb188-2" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">expression</span>(lambda), <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb188-3"><a href="bayesian-computation.html#cb188-3" tabindex="-1"></a><span class="fu">lines</span>(l.vals, <span class="fu">dgamma</span>(l.vals, <span class="at">shape =</span> <span class="dv">50</span>, <span class="at">rate =</span> <span class="dv">100</span>), <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb188-4"><a href="bayesian-computation.html#cb188-4" tabindex="-1"></a></span>
<span id="cb188-5"><a href="bayesian-computation.html#cb188-5" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.7</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb188-6"><a href="bayesian-computation.html#cb188-6" tabindex="-1"></a></span>
<span id="cb188-7"><a href="bayesian-computation.html#cb188-7" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="at">leg =</span> <span class="fu">c</span>(<span class="st">&quot;prior&quot;</span>, <span class="st">&quot;posterior&quot;</span>), </span>
<span id="cb188-8"><a href="bayesian-computation.html#cb188-8" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="st">&quot;dashed&quot;</span>, <span class="st">&quot;solid&quot;</span>))</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="mcmc-and-stochastic-approximations-of-the-posterior" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> MCMC and stochastic approximations of the posterior<a href="bayesian-computation.html#mcmc-and-stochastic-approximations-of-the-posterior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In most settings, a full conjugate prior is not available, and determining the posterior analytically is hard. In these situations, the contemporary approach is to approximate the posterior with a pseudo-random sample. While there are several methods for generating a pseudo-random sample from a posterior distribution, the most common approaches are based on Markov chain Monte Carlo (MCMC) sampling.</p>
<p>There are multiple tools available for generating an MCMC sample from a posterior distribution, and software in this area changes rapidly. In this computing companion, we will use JAGS to approximate posterior distributions. JAGS is stand-alone software, but we can access JAGS from R using the <code>r2jags</code> package. This computing companion will largely use the default settings for the routines in <code>r2jags</code>, though in real practice the analyst will often have to do considerable work adjusting the settings to obtain a satisfactory approximation.</p>
<div id="the-horse-kick-data-once-more" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> The horse-kick data, once more<a href="bayesian-computation.html#the-horse-kick-data-once-more" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here is JAGS code to approximate the posterior to <span class="math inline">\(\lambda\)</span> for the horse-kick data, using the vague prior.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="bayesian-computation.html#cb189-1" tabindex="-1"></a><span class="fu">require</span>(R2jags)</span></code></pre></div>
<pre><code>## Loading required package: R2jags</code></pre>
<pre><code>## Loading required package: rjags</code></pre>
<pre><code>## Loading required package: coda</code></pre>
<pre><code>## Linked to JAGS 4.3.1</code></pre>
<pre><code>## Loaded modules: basemod,bugs</code></pre>
<pre><code>## 
## Attaching package: &#39;R2jags&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:coda&#39;:
## 
##     traceplot</code></pre>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="bayesian-computation.html#cb197-1" tabindex="-1"></a>horse.model <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb197-2"><a href="bayesian-computation.html#cb197-2" tabindex="-1"></a>  </span>
<span id="cb197-3"><a href="bayesian-computation.html#cb197-3" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>J) {             <span class="co"># J = 280, number of data points</span></span>
<span id="cb197-4"><a href="bayesian-computation.html#cb197-4" tabindex="-1"></a>    y[j] <span class="sc">~</span> <span class="fu">dpois</span> (lambda)      <span class="co"># data model:  the likelihood      </span></span>
<span id="cb197-5"><a href="bayesian-computation.html#cb197-5" tabindex="-1"></a>  }</span>
<span id="cb197-6"><a href="bayesian-computation.html#cb197-6" tabindex="-1"></a>  </span>
<span id="cb197-7"><a href="bayesian-computation.html#cb197-7" tabindex="-1"></a>  lambda <span class="sc">~</span> <span class="fu">dgamma</span> (<span class="fl">0.01</span>, <span class="fl">0.01</span>) <span class="co"># prior </span></span>
<span id="cb197-8"><a href="bayesian-computation.html#cb197-8" tabindex="-1"></a>                               <span class="co"># note that BUGS / JAGS parameterizes </span></span>
<span id="cb197-9"><a href="bayesian-computation.html#cb197-9" tabindex="-1"></a>                               <span class="co"># gamma by shape, rate</span></span>
<span id="cb197-10"><a href="bayesian-computation.html#cb197-10" tabindex="-1"></a>}</span>
<span id="cb197-11"><a href="bayesian-computation.html#cb197-11" tabindex="-1"></a></span>
<span id="cb197-12"><a href="bayesian-computation.html#cb197-12" tabindex="-1"></a>jags.data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y =</span> horse<span class="sc">$</span>deaths, <span class="at">J =</span> <span class="fu">length</span>(horse<span class="sc">$</span>deaths))</span>
<span id="cb197-13"><a href="bayesian-computation.html#cb197-13" tabindex="-1"></a></span>
<span id="cb197-14"><a href="bayesian-computation.html#cb197-14" tabindex="-1"></a>jags.params <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;lambda&quot;</span>)</span>
<span id="cb197-15"><a href="bayesian-computation.html#cb197-15" tabindex="-1"></a></span>
<span id="cb197-16"><a href="bayesian-computation.html#cb197-16" tabindex="-1"></a>jags.inits <span class="ot">&lt;-</span> <span class="cf">function</span>(){</span>
<span id="cb197-17"><a href="bayesian-computation.html#cb197-17" tabindex="-1"></a>  <span class="fu">list</span>(<span class="st">&quot;lambda&quot;</span> <span class="ot">=</span> <span class="fu">rgamma</span>(<span class="fl">0.01</span>, <span class="fl">0.01</span>))</span>
<span id="cb197-18"><a href="bayesian-computation.html#cb197-18" tabindex="-1"></a>}</span>
<span id="cb197-19"><a href="bayesian-computation.html#cb197-19" tabindex="-1"></a></span>
<span id="cb197-20"><a href="bayesian-computation.html#cb197-20" tabindex="-1"></a>jagsfit <span class="ot">&lt;-</span> <span class="fu">jags</span>(<span class="at">data               =</span> jags.data, </span>
<span id="cb197-21"><a href="bayesian-computation.html#cb197-21" tabindex="-1"></a>                <span class="at">inits              =</span> jags.inits, </span>
<span id="cb197-22"><a href="bayesian-computation.html#cb197-22" tabindex="-1"></a>                <span class="at">parameters.to.save =</span> jags.params,</span>
<span id="cb197-23"><a href="bayesian-computation.html#cb197-23" tabindex="-1"></a>                <span class="at">model.file         =</span> horse.model,</span>
<span id="cb197-24"><a href="bayesian-computation.html#cb197-24" tabindex="-1"></a>                <span class="at">n.chains           =</span> <span class="dv">3</span>,</span>
<span id="cb197-25"><a href="bayesian-computation.html#cb197-25" tabindex="-1"></a>                <span class="at">n.iter             =</span> <span class="dv">5000</span>)</span></code></pre></div>
<pre><code>## module glm loaded</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 280
##    Unobserved stochastic nodes: 1
##    Total graph size: 283
## 
## Initializing model</code></pre>
<p>Let’s take a look at some summary statistics of the fit</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="bayesian-computation.html#cb200-1" tabindex="-1"></a><span class="fu">print</span>(jagsfit)</span></code></pre></div>
<pre><code>## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmpo1oUaH/model59cc409730ad&quot;, fit using jags,
##  3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2
##  n.sims = 3750 iterations saved. Running time = 1 secs
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## lambda     0.701   0.050   0.605   0.667   0.700   0.733   0.804 1.001  3800
## deviance 629.318   1.499 628.310 628.406 628.739 629.604 633.345 1.002  3800
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule: pV = var(deviance)/2)
## pV = 1.1 and DIC = 630.4
## DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<p>The Rhat values suggest that our chains have converged, as we might hope for such a simple model. We can generate a trace plot using <code>traceplot</code> to inspect convergence visually, but beware that visual assessment of convergence is prone to error.</p>
<p>For an rjags object, the raw MCMC samples are stored in <code>BUGSoutput$sims.list</code>. Sometimes it is helpful to analyze these samples directly. For example, with these samples we can estimate other posterior quantities, such as the posterior median of <span class="math inline">\(\lambda\)</span>, or generate a 95% central posterior confidence interval directly:</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="bayesian-computation.html#cb202-1" tabindex="-1"></a>mcmc.output <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(jagsfit<span class="sc">$</span>BUGSoutput<span class="sc">$</span>sims.list)</span>
<span id="cb202-2"><a href="bayesian-computation.html#cb202-2" tabindex="-1"></a><span class="fu">summary</span>(mcmc.output)</span></code></pre></div>
<pre><code>##     deviance         lambda      
##  Min.   :628.3   Min.   :0.5166  
##  1st Qu.:628.4   1st Qu.:0.6671  
##  Median :628.7   Median :0.6999  
##  Mean   :629.3   Mean   :0.7008  
##  3rd Qu.:629.6   3rd Qu.:0.7325  
##  Max.   :644.7   Max.   :0.9160</code></pre>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="bayesian-computation.html#cb204-1" tabindex="-1"></a><span class="fu">median</span>(mcmc.output<span class="sc">$</span>lambda)</span></code></pre></div>
<pre><code>## [1] 0.6998957</code></pre>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="bayesian-computation.html#cb206-1" tabindex="-1"></a><span class="fu">quantile</span>(mcmc.output<span class="sc">$</span>lambda, <span class="fu">c</span>(.<span class="dv">025</span>, .<span class="dv">975</span>))</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## 0.6048446 0.8036678</code></pre>
<p>We can also use the <code>lattice</code> package to construct smoothed estimates of the posterior density:</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="bayesian-computation.html#cb208-1" tabindex="-1"></a><span class="fu">require</span>(lattice)</span></code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="bayesian-computation.html#cb210-1" tabindex="-1"></a>jagsfit.mcmc <span class="ot">&lt;-</span> <span class="fu">as.mcmc</span>(jagsfit)</span>
<span id="cb210-2"><a href="bayesian-computation.html#cb210-2" tabindex="-1"></a><span class="fu">densityplot</span>(jagsfit.mcmc)</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="a-simple-regression-example" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> A simple regression example<a href="bayesian-computation.html#a-simple-regression-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a more involved example, let’s take a look at the simple regression fit to the cricket data. First, we’ll make a plot of the data and fit a SLR model by least squares.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="bayesian-computation.html#cb211-1" tabindex="-1"></a>cricket <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;data/cricket.txt&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb211-2"><a href="bayesian-computation.html#cb211-2" tabindex="-1"></a></span>
<span id="cb211-3"><a href="bayesian-computation.html#cb211-3" tabindex="-1"></a>cricket.slr <span class="ot">&lt;-</span> <span class="fu">lm</span>(chirps <span class="sc">~</span> temperature, <span class="at">data =</span> cricket)</span>
<span id="cb211-4"><a href="bayesian-computation.html#cb211-4" tabindex="-1"></a><span class="fu">summary</span>(cricket.slr)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = chirps ~ temperature, data = cricket)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.56009 -0.57930  0.03129  0.59020  1.53259 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.30914    3.10858  -0.099 0.922299    
## temperature  0.21193    0.03871   5.475 0.000107 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9715 on 13 degrees of freedom
## Multiple R-squared:  0.6975, Adjusted R-squared:  0.6742 
## F-statistic: 29.97 on 1 and 13 DF,  p-value: 0.0001067</code></pre>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="bayesian-computation.html#cb213-1" tabindex="-1"></a><span class="fu">plot</span>(chirps <span class="sc">~</span> temperature, <span class="at">data =</span> cricket)</span>
<span id="cb213-2"><a href="bayesian-computation.html#cb213-2" tabindex="-1"></a><span class="fu">abline</span>(cricket.slr)</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Now we’ll fit the same model in JAGS, using vague priors for all model parameters</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="bayesian-computation.html#cb214-1" tabindex="-1"></a>cricket.model <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb214-2"><a href="bayesian-computation.html#cb214-2" tabindex="-1"></a>  </span>
<span id="cb214-3"><a href="bayesian-computation.html#cb214-3" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>J) {             <span class="co"># J = number of data points</span></span>
<span id="cb214-4"><a href="bayesian-computation.html#cb214-4" tabindex="-1"></a>    </span>
<span id="cb214-5"><a href="bayesian-computation.html#cb214-5" tabindex="-1"></a>    y[j] <span class="sc">~</span> <span class="fu">dnorm</span> (mu[j], tau)  <span class="co"># data model:  the likelihood </span></span>
<span id="cb214-6"><a href="bayesian-computation.html#cb214-6" tabindex="-1"></a>                               <span class="co"># note that BUGS / JAGS uses precision</span></span>
<span id="cb214-7"><a href="bayesian-computation.html#cb214-7" tabindex="-1"></a>                               <span class="co"># instead of variance</span></span>
<span id="cb214-8"><a href="bayesian-computation.html#cb214-8" tabindex="-1"></a>    </span>
<span id="cb214-9"><a href="bayesian-computation.html#cb214-9" tabindex="-1"></a>    mu[j] <span class="ot">&lt;-</span> b0 <span class="sc">+</span> b1 <span class="sc">*</span> x[j]    <span class="co"># compute the mean for each observation</span></span>
<span id="cb214-10"><a href="bayesian-computation.html#cb214-10" tabindex="-1"></a>  }</span>
<span id="cb214-11"><a href="bayesian-computation.html#cb214-11" tabindex="-1"></a>  </span>
<span id="cb214-12"><a href="bayesian-computation.html#cb214-12" tabindex="-1"></a>  b0 <span class="sc">~</span> <span class="fu">dnorm</span> (<span class="fl">0.0</span>, <span class="fl">1E-6</span>)       <span class="co"># prior for intercept</span></span>
<span id="cb214-13"><a href="bayesian-computation.html#cb214-13" tabindex="-1"></a>  b1 <span class="sc">~</span> <span class="fu">dnorm</span> (<span class="fl">0.0</span>, <span class="fl">1E-6</span>)       <span class="co"># prior for slope</span></span>
<span id="cb214-14"><a href="bayesian-computation.html#cb214-14" tabindex="-1"></a>  tau <span class="sc">~</span> <span class="fu">dgamma</span> (<span class="fl">0.01</span>, <span class="fl">0.01</span>)    <span class="co"># prior for tau</span></span>
<span id="cb214-15"><a href="bayesian-computation.html#cb214-15" tabindex="-1"></a>                               <span class="co"># note that BUGS / JAGS parameterizes </span></span>
<span id="cb214-16"><a href="bayesian-computation.html#cb214-16" tabindex="-1"></a>                               <span class="co"># gamma by shape, rate</span></span>
<span id="cb214-17"><a href="bayesian-computation.html#cb214-17" tabindex="-1"></a>  </span>
<span id="cb214-18"><a href="bayesian-computation.html#cb214-18" tabindex="-1"></a>  sigma <span class="ot">&lt;-</span> <span class="fu">pow</span>(tau, <span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>)      <span class="co"># the SD of the residaul errors</span></span>
<span id="cb214-19"><a href="bayesian-computation.html#cb214-19" tabindex="-1"></a>}</span>
<span id="cb214-20"><a href="bayesian-computation.html#cb214-20" tabindex="-1"></a></span>
<span id="cb214-21"><a href="bayesian-computation.html#cb214-21" tabindex="-1"></a>jags.data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y =</span> cricket<span class="sc">$</span>chirps, </span>
<span id="cb214-22"><a href="bayesian-computation.html#cb214-22" tabindex="-1"></a>                  <span class="at">x =</span> cricket<span class="sc">$</span>temperature,</span>
<span id="cb214-23"><a href="bayesian-computation.html#cb214-23" tabindex="-1"></a>                  <span class="at">J =</span> <span class="fu">nrow</span>(cricket))</span>
<span id="cb214-24"><a href="bayesian-computation.html#cb214-24" tabindex="-1"></a></span>
<span id="cb214-25"><a href="bayesian-computation.html#cb214-25" tabindex="-1"></a>jags.params <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;b0&quot;</span>, <span class="st">&quot;b1&quot;</span>, <span class="st">&quot;tau&quot;</span>, <span class="st">&quot;sigma&quot;</span>)</span>
<span id="cb214-26"><a href="bayesian-computation.html#cb214-26" tabindex="-1"></a></span>
<span id="cb214-27"><a href="bayesian-computation.html#cb214-27" tabindex="-1"></a>jags.inits <span class="ot">&lt;-</span> <span class="cf">function</span>(){</span>
<span id="cb214-28"><a href="bayesian-computation.html#cb214-28" tabindex="-1"></a>  <span class="fu">list</span>(<span class="st">&quot;b0&quot;</span> <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>), <span class="st">&quot;b1&quot;</span> <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>), <span class="st">&quot;tau&quot;</span> <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb214-29"><a href="bayesian-computation.html#cb214-29" tabindex="-1"></a>}</span>
<span id="cb214-30"><a href="bayesian-computation.html#cb214-30" tabindex="-1"></a></span>
<span id="cb214-31"><a href="bayesian-computation.html#cb214-31" tabindex="-1"></a>jagsfit <span class="ot">&lt;-</span> <span class="fu">jags</span>(<span class="at">data               =</span> jags.data, </span>
<span id="cb214-32"><a href="bayesian-computation.html#cb214-32" tabindex="-1"></a>                <span class="at">inits              =</span> jags.inits, </span>
<span id="cb214-33"><a href="bayesian-computation.html#cb214-33" tabindex="-1"></a>                <span class="at">parameters.to.save =</span> jags.params,</span>
<span id="cb214-34"><a href="bayesian-computation.html#cb214-34" tabindex="-1"></a>                <span class="at">model.file         =</span> cricket.model,</span>
<span id="cb214-35"><a href="bayesian-computation.html#cb214-35" tabindex="-1"></a>                <span class="at">n.chains           =</span> <span class="dv">3</span>,</span>
<span id="cb214-36"><a href="bayesian-computation.html#cb214-36" tabindex="-1"></a>                <span class="at">n.iter             =</span> <span class="dv">5000</span>)</span></code></pre></div>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 15
##    Unobserved stochastic nodes: 3
##    Total graph size: 70
## 
## Initializing model</code></pre>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="bayesian-computation.html#cb216-1" tabindex="-1"></a><span class="fu">print</span>(jagsfit)</span></code></pre></div>
<pre><code>## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmpo1oUaH/model59cc63335ee3&quot;, fit using jags,
##  3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2
##  n.sims = 3750 iterations saved. Running time = 0.06 secs
##          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
## b0        -0.303   3.350 -6.932 -2.437 -0.267  1.830  6.302 1.001  3700
## b1         0.212   0.042  0.129  0.185  0.211  0.239  0.294 1.001  3800
## sigma      1.038   0.222  0.702  0.885  1.003  1.149  1.566 1.001  3800
## tau        1.047   0.409  0.408  0.757  0.993  1.278  2.029 1.001  3800
## deviance  42.833   2.692 39.791 40.886 42.161 44.094 49.753 1.001  3800
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule: pV = var(deviance)/2)
## pV = 3.6 and DIC = 46.5
## DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="bayesian-computation.html#cb218-1" tabindex="-1"></a><span class="fu">traceplot</span>(jagsfit)</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-14-1.png" width="672" /><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-14-2.png" width="672" /><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-14-3.png" width="672" /><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-14-4.png" width="672" /><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-14-5.png" width="672" /></p>
<p>The output of the <code>print</code> function gives the quantiles that one would use to calculate a 95% central credible interval. To find a HPD credible interval, we can use the <code>HPDinterval</code> function in the <code>coda</code> library. The <code>coda</code> library contains a variety of routines for post-processing of MCMC ouput. If we simply pass the <code>jagsfit</code> object to the <code>HPDinterval</code> function, it will return an HPD interval for each of the three chains. This isn’t what we want, so we’ll extract the raw MCMC samples first, and then coerce them to a data frame.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="bayesian-computation.html#cb219-1" tabindex="-1"></a>mcmc.output <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(jagsfit<span class="sc">$</span>BUGSoutput<span class="sc">$</span>sims.list)</span>
<span id="cb219-2"><a href="bayesian-computation.html#cb219-2" tabindex="-1"></a><span class="fu">summary</span>(mcmc.output)</span></code></pre></div>
<pre><code>##        b0                 b1             deviance         sigma       
##  Min.   :-13.1658   Min.   :0.03941   Min.   :39.58   Min.   :0.5867  
##  1st Qu.: -2.4375   1st Qu.:0.18517   1st Qu.:40.89   1st Qu.:0.8846  
##  Median : -0.2665   Median :0.21129   Median :42.16   Median :1.0035  
##  Mean   : -0.3030   Mean   :0.21179   Mean   :42.83   Mean   :1.0379  
##  3rd Qu.:  1.8295   3rd Qu.:0.23881   3rd Qu.:44.09   3rd Qu.:1.1490  
##  Max.   : 14.0670   Max.   :0.37815   Max.   :68.20   Max.   :2.9931  
##       tau        
##  Min.   :0.1116  
##  1st Qu.:0.7575  
##  Median :0.9931  
##  Mean   :1.0465  
##  3rd Qu.:1.2779  
##  Max.   :2.9051</code></pre>
<p>Now we’ll coerce the data frame <code>mcmc.output</code> to an MCMC object, and pass it to <code>HPDinterval</code>:</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="bayesian-computation.html#cb221-1" tabindex="-1"></a><span class="fu">HPDinterval</span>(<span class="fu">as.mcmc</span>(mcmc.output))</span></code></pre></div>
<pre><code>##               lower      upper
## b0       -6.7360156  6.4761698
## b1        0.1325089  0.2973778
## deviance 39.6129789 47.9822687
## sigma     0.6382489  1.4481293
## tau       0.2983668  1.8350374
## attr(,&quot;Probability&quot;)
## [1] 0.9498667</code></pre>
<p>One of the merits of the Bayesian approach is that the posterior samples provide an immediate tool for propagating uncertainty to (possibly derived) quantities of interest. We can summarize the uncertainty in the regression fit graphically by randomly sampling a subset of these samples (say, 100 of them) and using them to plot a collection of regression lines:</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="bayesian-computation.html#cb223-1" tabindex="-1"></a><span class="fu">plot</span>(chirps <span class="sc">~</span> temperature, <span class="at">data =</span> cricket, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb223-2"><a href="bayesian-computation.html#cb223-2" tabindex="-1"></a><span class="co"># we&#39;ll add the points later so that they lie on top of the lines,</span></span>
<span id="cb223-3"><a href="bayesian-computation.html#cb223-3" tabindex="-1"></a><span class="co"># instead of the other way around</span></span>
<span id="cb223-4"><a href="bayesian-computation.html#cb223-4" tabindex="-1"></a></span>
<span id="cb223-5"><a href="bayesian-computation.html#cb223-5" tabindex="-1"></a>subset.samples <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(mcmc.output), <span class="at">size =</span> <span class="dv">100</span>)</span>
<span id="cb223-6"><a href="bayesian-computation.html#cb223-6" tabindex="-1"></a></span>
<span id="cb223-7"><a href="bayesian-computation.html#cb223-7" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> subset.samples) {</span>
<span id="cb223-8"><a href="bayesian-computation.html#cb223-8" tabindex="-1"></a>  </span>
<span id="cb223-9"><a href="bayesian-computation.html#cb223-9" tabindex="-1"></a>  <span class="fu">with</span>(mcmc.output, <span class="fu">abline</span>(<span class="at">a =</span> b0[i], <span class="at">b =</span> b1[i], <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="fl">0.25</span>))</span>
<span id="cb223-10"><a href="bayesian-computation.html#cb223-10" tabindex="-1"></a>}</span>
<span id="cb223-11"><a href="bayesian-computation.html#cb223-11" tabindex="-1"></a></span>
<span id="cb223-12"><a href="bayesian-computation.html#cb223-12" tabindex="-1"></a><span class="fu">with</span>(cricket, <span class="fu">points</span>(chirps <span class="sc">~</span> temperature))</span>
<span id="cb223-13"><a href="bayesian-computation.html#cb223-13" tabindex="-1"></a><span class="fu">with</span>(mcmc.output, <span class="fu">abline</span>(<span class="at">a =</span> <span class="fu">mean</span>(b0), <span class="at">b =</span> <span class="fu">mean</span>(b1), <span class="at">col =</span> <span class="st">&quot;purple3&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>))</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>We can also propagate the uncertainty to estimate, say, the posterior distribution for the value of the regression line when the temperature is 85 F. This quantifies the uncertainty in the average number of chirps at this temperature. (We can think of it as a vertical slice through the above plot.)</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="bayesian-computation.html#cb224-1" tabindex="-1"></a>avg.chirps<span class="fl">.85</span> <span class="ot">&lt;-</span> <span class="fu">with</span>(mcmc.output, b0 <span class="sc">+</span> b1 <span class="sc">*</span> <span class="dv">85</span>)</span>
<span id="cb224-2"><a href="bayesian-computation.html#cb224-2" tabindex="-1"></a><span class="fu">summary</span>(avg.chirps<span class="fl">.85</span>)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   16.10   17.48   17.70   17.70   17.92   20.10</code></pre>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="bayesian-computation.html#cb226-1" tabindex="-1"></a><span class="fu">quantile</span>(avg.chirps<span class="fl">.85</span>, <span class="at">probs =</span> <span class="fu">c</span>(.<span class="dv">025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##     2.5%    97.5% 
## 17.02454 18.37030</code></pre>
<p>We could use the <code>density</code> function to get a quick idea of the shape of the distribution:</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="bayesian-computation.html#cb228-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(avg.chirps<span class="fl">.85</span>))</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Thus, we might say that the posterior mean for the average number of chirps at 85 F is 17.7, and a central 95% credible interval is (17.02, 18.37).</p>
<p>Finally, we can use the posterior samples to estimate the uncertainty in a future observation. When we use a posterior distribution to estimate the distribution of a future observation, we refer to it as a posterior predictive distribution. The posterior predictive distribution must also include the error around the regression line. We can estimate the posterior predictive distribution as follows. Suppose we denote sample <span class="math inline">\(i\)</span> from the posterior as <span class="math inline">\(\beta_{0, i}\)</span>, <span class="math inline">\(\beta_{1, i}\)</span>, and <span class="math inline">\(\sigma_i\)</span>. Then for each posterior sample we will generate a new hypothetical observation <span class="math inline">\(y_i^\star\)</span> by sampling from a Gaussian distribution with mean equal to $<em>{0,i} + </em>{1,i} x $ and standard deviation <span class="math inline">\(\sigma_i\)</span>, where <span class="math inline">\(x = 85\)</span>. The distribution of the <span class="math inline">\(y_i^*\)</span>’s then gives the posterior predictive distribution that we seek.</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="bayesian-computation.html#cb229-1" tabindex="-1"></a>n.sims <span class="ot">&lt;-</span> <span class="fu">nrow</span>(mcmc.output)</span>
<span id="cb229-2"><a href="bayesian-computation.html#cb229-2" tabindex="-1"></a>new.errors <span class="ot">&lt;-</span> <span class="fu">with</span>(mcmc.output, <span class="fu">rnorm</span>(n.sims, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma))</span>
<span id="cb229-3"><a href="bayesian-computation.html#cb229-3" tabindex="-1"></a>new.chirps<span class="fl">.85</span> <span class="ot">&lt;-</span> <span class="fu">with</span>(mcmc.output, b0 <span class="sc">+</span> b1 <span class="sc">*</span> <span class="dv">85</span>) <span class="sc">+</span> new.errors</span>
<span id="cb229-4"><a href="bayesian-computation.html#cb229-4" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">density</span>(new.chirps<span class="fl">.85</span>))</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="bayesian-computation.html#cb230-1" tabindex="-1"></a><span class="fu">summary</span>(new.chirps<span class="fl">.85</span>)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   12.73   16.99   17.71   17.70   18.42   22.73</code></pre>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="bayesian-computation.html#cb232-1" tabindex="-1"></a><span class="fu">quantile</span>(new.chirps<span class="fl">.85</span>, <span class="at">probs =</span> <span class="fu">c</span>(.<span class="dv">025</span>, <span class="fl">0.975</span>))</span></code></pre></div>
<pre><code>##     2.5%    97.5% 
## 15.49916 19.86500</code></pre>
<p>Thus, the posterior predictive distribution has a central 95% credible interval of (15.5, 19.86).</p>
<p>Although it hasn’t caused any difficulty here, the slope and intercept are strongly negatively correlated in the posterior. We can visualize this posterior correlation:</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="bayesian-computation.html#cb234-1" tabindex="-1"></a><span class="fu">library</span>(hexbin)</span>
<span id="cb234-2"><a href="bayesian-computation.html#cb234-2" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer)</span>
<span id="cb234-3"><a href="bayesian-computation.html#cb234-3" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">colorRampPalette</span>(<span class="fu">rev</span>(<span class="fu">brewer.pal</span>(<span class="dv">11</span>, <span class="st">&#39;Spectral&#39;</span>)))</span>
<span id="cb234-4"><a href="bayesian-computation.html#cb234-4" tabindex="-1"></a><span class="fu">with</span>(jagsfit<span class="sc">$</span>BUGSoutput<span class="sc">$</span>sims.list, <span class="fu">hexbinplot</span>(b1 <span class="sc">~</span> b0, <span class="at">colramp =</span> rf))</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>We can estimate the posterior correlation between the intercept and the slope by accessing the raw MCMC samples</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="bayesian-computation.html#cb235-1" tabindex="-1"></a><span class="fu">cor</span>(mcmc.output[, <span class="sc">-</span><span class="fu">c</span>(<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>)])</span></code></pre></div>
<pre><code>##              b0          b1         tau
## b0   1.00000000 -0.99675664  0.01442628
## b1  -0.99675664  1.00000000 -0.01225101
## tau  0.01442628 -0.01225101  1.00000000</code></pre>
<p>Thus we estimate that the intercept and slope have a posterior correlation of -0.997.</p>
<p>We could make life easier on ourselves by centering the predictor and trying again:</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="bayesian-computation.html#cb237-1" tabindex="-1"></a>cricket<span class="sc">$</span>temp.ctr <span class="ot">&lt;-</span> cricket<span class="sc">$</span>temperature <span class="sc">-</span> <span class="fu">mean</span>(cricket<span class="sc">$</span>temperature)</span></code></pre></div>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="bayesian-computation.html#cb238-1" tabindex="-1"></a>jags.data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">y =</span> cricket<span class="sc">$</span>chirps, </span>
<span id="cb238-2"><a href="bayesian-computation.html#cb238-2" tabindex="-1"></a>                  <span class="at">x =</span> cricket<span class="sc">$</span>temp.ctr,</span>
<span id="cb238-3"><a href="bayesian-computation.html#cb238-3" tabindex="-1"></a>                  <span class="at">J =</span> <span class="fu">nrow</span>(cricket))</span>
<span id="cb238-4"><a href="bayesian-computation.html#cb238-4" tabindex="-1"></a></span>
<span id="cb238-5"><a href="bayesian-computation.html#cb238-5" tabindex="-1"></a>jags.params <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;b0&quot;</span>, <span class="st">&quot;b1&quot;</span>, <span class="st">&quot;tau&quot;</span>, <span class="st">&quot;sigma&quot;</span>)</span>
<span id="cb238-6"><a href="bayesian-computation.html#cb238-6" tabindex="-1"></a></span>
<span id="cb238-7"><a href="bayesian-computation.html#cb238-7" tabindex="-1"></a>jags.inits <span class="ot">&lt;-</span> <span class="cf">function</span>(){</span>
<span id="cb238-8"><a href="bayesian-computation.html#cb238-8" tabindex="-1"></a>  <span class="fu">list</span>(<span class="st">&quot;b0&quot;</span> <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>), <span class="st">&quot;b1&quot;</span> <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">1</span>), <span class="st">&quot;tau&quot;</span> <span class="ot">=</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb238-9"><a href="bayesian-computation.html#cb238-9" tabindex="-1"></a>}</span>
<span id="cb238-10"><a href="bayesian-computation.html#cb238-10" tabindex="-1"></a></span>
<span id="cb238-11"><a href="bayesian-computation.html#cb238-11" tabindex="-1"></a>jagsfit <span class="ot">&lt;-</span> <span class="fu">jags</span>(<span class="at">data               =</span> jags.data, </span>
<span id="cb238-12"><a href="bayesian-computation.html#cb238-12" tabindex="-1"></a>                <span class="at">inits              =</span> jags.inits, </span>
<span id="cb238-13"><a href="bayesian-computation.html#cb238-13" tabindex="-1"></a>                <span class="at">parameters.to.save =</span> jags.params,</span>
<span id="cb238-14"><a href="bayesian-computation.html#cb238-14" tabindex="-1"></a>                <span class="at">model.file         =</span> cricket.model,</span>
<span id="cb238-15"><a href="bayesian-computation.html#cb238-15" tabindex="-1"></a>                <span class="at">n.chains           =</span> <span class="dv">3</span>,</span>
<span id="cb238-16"><a href="bayesian-computation.html#cb238-16" tabindex="-1"></a>                <span class="at">n.iter             =</span> <span class="dv">5000</span>)</span></code></pre></div>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 15
##    Unobserved stochastic nodes: 3
##    Total graph size: 70
## 
## Initializing model</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="bayesian-computation.html#cb240-1" tabindex="-1"></a><span class="fu">print</span>(jagsfit)</span></code></pre></div>
<pre><code>## Inference for Bugs model at &quot;C:/Users/krgross/AppData/Local/Temp/Rtmpo1oUaH/model59cc1c0f6ec2&quot;, fit using jags,
##  3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2
##  n.sims = 3750 iterations saved. Running time = 0.05 secs
##          mu.vect sd.vect   2.5%    25%    50%    75%  97.5%  Rhat n.eff
## b0        16.654   0.267 16.129 16.482 16.646 16.830 17.189 1.001  3800
## b1         0.210   0.042  0.123  0.184  0.211  0.238  0.290 1.001  3800
## sigma      1.032   0.222  0.710  0.877  0.995  1.149  1.576 1.001  3800
## tau        1.059   0.409  0.403  0.758  1.010  1.299  1.983 1.001  3800
## deviance  42.857   2.657 39.775 40.864 42.176 44.189 49.580 1.001  3800
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule: pV = var(deviance)/2)
## pV = 3.5 and DIC = 46.4
## DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="bayesian-computation.html#cb242-1" tabindex="-1"></a><span class="fu">traceplot</span>(jagsfit)</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-24-1.png" width="672" /><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-24-2.png" width="672" /><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-24-3.png" width="672" /><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-24-4.png" width="672" /><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-24-5.png" width="672" /></p>
<p>The posteriors for the intercept and slope are now uncorrelated:</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="bayesian-computation.html#cb243-1" tabindex="-1"></a><span class="fu">library</span>(hexbin)</span>
<span id="cb243-2"><a href="bayesian-computation.html#cb243-2" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer)</span>
<span id="cb243-3"><a href="bayesian-computation.html#cb243-3" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">colorRampPalette</span>(<span class="fu">rev</span>(<span class="fu">brewer.pal</span>(<span class="dv">11</span>, <span class="st">&#39;Spectral&#39;</span>)))</span>
<span id="cb243-4"><a href="bayesian-computation.html#cb243-4" tabindex="-1"></a><span class="fu">with</span>(jagsfit<span class="sc">$</span>BUGSoutput<span class="sc">$</span>sims.list, <span class="fu">hexbinplot</span>(b1 <span class="sc">~</span> b0, <span class="at">colramp =</span> rf))</span></code></pre></div>
<p><img src="03-BayesianIntro_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="bayesian-computation.html#cb244-1" tabindex="-1"></a>mcmc.output <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(jagsfit<span class="sc">$</span>BUGSoutput<span class="sc">$</span>sims.list)</span>
<span id="cb244-2"><a href="bayesian-computation.html#cb244-2" tabindex="-1"></a><span class="fu">cor</span>(mcmc.output[, <span class="sc">-</span><span class="fu">c</span>(<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>)])</span></code></pre></div>
<pre><code>##               b0          b1          tau
## b0   1.000000000 -0.02813716 -0.001464623
## b1  -0.028137160  1.00000000  0.043335138
## tau -0.001464623  0.04333514  1.000000000</code></pre>
</div>
</div>
<div id="rstanarm" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> rstanarm<a href="bayesian-computation.html#rstanarm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <code>rstanarm</code> package is a recent set of routines that seeks to provide a user-friendly front end to Bayesian analysis with Stan. Specifically, <code>rstanarm</code> provides functions for fitting standard statistical models that are meant to mimic the analogous fitting functions in R. For example, the basic routine for fitting linear models in R is <code>lm</code>; <code>rstanarm</code> provides a function <code>stan_lm</code> that strives to have the same functionality and interface as <code>lm</code>, albeit using Stan “under the hood” to generate Bayesian inference. (That said, the main workhorse function in <code>rstanarm</code> for model fitting is <code>stan_glm</code>, which attempts to mimic the native R function <code>glm</code> for fitting generalized linear models. Separately, the developers of <code>rstanarm</code> have taken the not unreasonable stance that generalized linear models should supplant general linear models as the analyst’s default approach to model fitting.)</p>
<p>To provide functionality that is similar to R’s native model-fitting routines, the functions in <code>rstanarm</code> make a number of operational decisions behind the scenes. Most notably, the model fitting routines in <code>rstanarm</code> will select default priors and default HMC parameters. While these defaults can always be modified by the analyst, the implementation of software that chooses priors by default is radical. First, the developers of <code>rstanarm</code> have their own particular view about what the role of the prior should be in data analysis. While their view is a considered one, by no means does it reflect a consensus that extends beyond the developers of the software. If you use <code>rstanarm</code>’s routines out of the box, you are accepting this view as your own if you do not specify the priors yourself. Second, as best I understand, the methods by which <code>rstanarm</code> chooses default priors still appear to be in some flux. That means that future versions of <code>rstanarm</code> may supply different default priors than those that are supplied today. As a result, the behavior of <code>rstanarm</code> today may differ from its behavior tomorrow, if you use the default priors.</p>
<p>All that said, here is how you might use <code>rstanarm</code> to fit the simple regression to the cricket data:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="bayesian-computation.html#cb246-1" tabindex="-1"></a><span class="fu">require</span>(rstanarm)</span></code></pre></div>
<pre><code>## Loading required package: rstanarm</code></pre>
<pre><code>## Loading required package: Rcpp</code></pre>
<pre><code>## This is rstanarm version 2.32.1</code></pre>
<pre><code>## - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!</code></pre>
<pre><code>## - Default priors may change, so it&#39;s safest to specify priors, even if equivalent to the defaults.</code></pre>
<pre><code>## - For execution on a local, multicore CPU with excess RAM we recommend calling</code></pre>
<pre><code>##   options(mc.cores = parallel::detectCores())</code></pre>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="bayesian-computation.html#cb254-1" tabindex="-1"></a>stanarm.cricket.fit <span class="ot">&lt;-</span> <span class="fu">stan_glm</span>(chirps <span class="sc">~</span> temp.ctr, <span class="at">data =</span> cricket, <span class="at">family =</span> gaussian, <span class="at">seed =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 6.3e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.027 seconds (Warm-up)
## Chain 1:                0.036 seconds (Sampling)
## Chain 1:                0.063 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 7e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.037 seconds (Warm-up)
## Chain 2:                0.027 seconds (Sampling)
## Chain 2:                0.064 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 7e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.026 seconds (Warm-up)
## Chain 3:                0.025 seconds (Sampling)
## Chain 3:                0.051 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 6e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.027 seconds (Warm-up)
## Chain 4:                0.024 seconds (Sampling)
## Chain 4:                0.051 seconds (Total)
## Chain 4:</code></pre>
<p>We can discover the priors that <code>stan_glm</code> has selected by using the function <code>prior_summary</code>.</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="bayesian-computation.html#cb256-1" tabindex="-1"></a><span class="fu">prior_summary</span>(stanarm.cricket.fit)</span></code></pre></div>
<pre><code>## Priors for model &#39;stanarm.cricket.fit&#39; 
## ------
## Intercept (after predictors centered)
##   Specified prior:
##     ~ normal(location = 17, scale = 2.5)
##   Adjusted prior:
##     ~ normal(location = 17, scale = 4.3)
## 
## Coefficients
##   Specified prior:
##     ~ normal(location = 0, scale = 2.5)
##   Adjusted prior:
##     ~ normal(location = 0, scale = 0.63)
## 
## Auxiliary (sigma)
##   Specified prior:
##     ~ exponential(rate = 1)
##   Adjusted prior:
##     ~ exponential(rate = 0.59)
## ------
## See help(&#39;prior_summary.stanreg&#39;) for more details</code></pre>
<p>The <code>rstanarm</code> package has made a variety of decisions about how many chains to run, how long to run them, etc. We can obtain a summary of the model fit by the <code>print</code> command:</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="bayesian-computation.html#cb258-1" tabindex="-1"></a><span class="fu">print</span>(stanarm.cricket.fit, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## stan_glm
##  family:       gaussian [identity]
##  formula:      chirps ~ temp.ctr
##  observations: 15
##  predictors:   2
## ------
##             Median MAD_SD
## (Intercept) 16.648  0.257
## temp.ctr     0.210  0.041
## 
## Auxiliary parameter(s):
##       Median MAD_SD
## sigma 1.008  0.198 
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg</code></pre>
<p>There are a few parts of this output that deserve comment. First, this summary reports the posterior median of the parameters instead of the posterior mean. Second, the authors of <code>rstanarm</code> have made the curious decision to replace the posterior standard deviation (itself the Bayesian counterpart to the frequentist’s standard error) with someting they call “MAD SD”. This takes a bit of explanation. The “MAD” part stands for median absolute deviation. It is the median of the absolute deviations of the posterior samples from the posterior median. In other words, if we have a generic parameter <span class="math inline">\(\theta\)</span> and label its posterior samples as <span class="math inline">\(\theta_1, \theta_2, \ldots, \theta_n\)</span>, then the MAD of <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display">\[
\mathrm{median}_i(| \theta_i - \mathrm{median}_i(\theta_i) |)
\]</span></p>
<p>According to the authors of <code>rstanarm</code>, “Because we are so used to working with standard deviations, when we compute the median absolute deviation, we then rescale it by multiplying by 1.483, which reproduces the standard deviation in the special case of the normal distribution. We call this the mad sd.” In other words, the MAD SD is a measure of posterior uncertainty that is meant to be comparable to the posterior standard deviation. (The authors of <code>rstanarm</code> clearly must think this is a more desirable estimate of the posterior uncertainty than the posterior standard deviation; though their reasoning here is not immediately clear to me.)</p>
<!-- See p 73 of Gelman "Regression and other stories" -->
<p>If we want to compute our own summary statistics, we can extract the MCMC samples from the <code>stam_glm</code> fit using the <code>as.matrix</code> command:</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="bayesian-computation.html#cb260-1" tabindex="-1"></a>mcmc.sims <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(stanarm.cricket.fit)</span>
<span id="cb260-2"><a href="bayesian-computation.html#cb260-2" tabindex="-1"></a><span class="fu">summary</span>(mcmc.sims)</span></code></pre></div>
<pre><code>##   (Intercept)       temp.ctr           sigma       
##  Min.   :15.65   Min.   :0.02782   Min.   :0.5582  
##  1st Qu.:16.48   1st Qu.:0.18191   1st Qu.:0.8869  
##  Median :16.65   Median :0.21023   Median :1.0079  
##  Mean   :16.65   Mean   :0.20955   Mean   :1.0457  
##  3rd Qu.:16.83   3rd Qu.:0.23740   3rd Qu.:1.1627  
##  Max.   :17.68   Max.   :0.36177   Max.   :2.3400</code></pre>
<p>We might, for example, then use this output to find the posterior standard deviation of each of the parameters, or to find central 95% credible intervals:</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="bayesian-computation.html#cb262-1" tabindex="-1"></a><span class="fu">apply</span>(mcmc.sims, <span class="dv">2</span>, sd)</span></code></pre></div>
<pre><code>## (Intercept)    temp.ctr       sigma 
##  0.27003507  0.04262539  0.22262375</code></pre>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="bayesian-computation.html#cb264-1" tabindex="-1"></a><span class="fu">apply</span>(mcmc.sims, <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">quantile</span>(x, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)))</span></code></pre></div>
<pre><code>##        parameters
##         (Intercept)  temp.ctr     sigma
##   2.5%     16.10111 0.1240071 0.7171401
##   97.5%    17.17942 0.2906848 1.5816217</code></pre>
<p>Compare these values to the posterior standard deviations and 95% central credible intervals reported in the JAGS fit.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="beyondML.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="smoothing-and-gams.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "serif",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
