plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
horse.neg.ll(0.7)
qchisq(0.95, df = 1) / 2
(cutoff.ll <- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2)
abline(h = cutoff.ll, col = "red", lty = "dashed")
my.function <- function(my.lambda){
cutoff.ll - horse.neg.ll(my.lambda)
}
(lower <- uniroot(f = my.function, interval = c(0.6, 0.7)))
abline(v = lower$root, col = "red")
(upper <- uniroot(f = my.function, interval = c(0.7, 0.9)))
abline(v = upper$root, col = "red")
horse.D2 <- function(delta.l) {
(horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2
}
horse.D2(1e-04)
numDeriv::hessian(func = horse.neg.ll, x = 0.7)
(lambda.se <- sqrt(1 / horse.D2(1e-04)))
(lower.approx <- 0.7 - qnorm(.975) * lambda.se)
(upper.approx <- 0.7 + qnorm(.975) * lambda.se)
abline(v = c(lower.approx, upper.approx), col = "blue")
# clean up
rm(list = ls())
library(emdbook)
data("ReedfrogFuncresp")
frog <- ReedfrogFuncresp
rm(ReedfrogFuncresp)
frog.neg.ll <- function(params){
a <- params[1]
h <- params[2]
prob.vals <- a / (1 + a * h * frog$Initial)
ll.vals <- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE)
-1 * sum(ll.vals)
}
frog.mle <- optim(par = c(0.5, 1/80),
fn  = frog.neg.ll)
a.mle <- frog.mle$par[1]
h.mle <- frog.mle$par[2]
a.vals <- seq(from = 0.3, to = 0.75, by = 0.01)
h.vals <- seq(from = 0.001, to = 0.03, by = 0.001)
ll.vals <- matrix(nrow = length(a.vals), ncol = length(h.vals))
plot(rep(a.vals, length(h.vals)), rep(h.vals, rep(length(a.vals), length(h.vals))))
for (i.a in 1:length(a.vals)) {
for(i.h in 1:length(h.vals)) {
ll.vals[i.a, i.h] <- frog.neg.ll(c(a.vals[i.a], h.vals[i.h]))
}
}
contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100,
xlab = "a", ylab = "h")
points(x = a.mle, y = h.mle, col = "red")
(cutoff.ll <- frog.neg.ll(c(a.mle, h.mle)) +
(1 / 2) * qchisq(.95, df = 2))
contour(x = a.vals, y = h.vals, z = ll.vals,
levels = frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2),
add = TRUE, col = "red", lwd = 2)
profile.ll <- function(my.a) {
# Calculate the minimum log likelihood value for a given value of a, the attack rate
my.ll <- function(h) frog.neg.ll(c(my.a, h))
my.profile <- optimize(f = my.ll,
interval = c(0, 0.03),
maximum = FALSE)
return(my.profile$objective)
}
library(emdbook)
data("ReedfrogFuncresp")
frog <- ReedfrogFuncresp
rm(ReedfrogFuncresp)
frog.neg.ll <- function(params){
a <- params[1]
h <- params[2]
prob.vals <- a / (1 + a * h * frog$Initial)
ll.vals <- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE)
-1 * sum(ll.vals)
}
frog.mle <- optim(par = c(0.5, 1/80),
fn  = frog.neg.ll)
a.mle <- frog.mle$par[1]
h.mle <- frog.mle$par[2]
a.vals <- seq(from = 0.3, to = 0.75, by = 0.01)
h.vals <- seq(from = 0.001, to = 0.03, by = 0.001)
ll.vals <- matrix(nrow = length(a.vals), ncol = length(h.vals))
plot(rep(a.vals, length(h.vals)), rep(h.vals, rep(length(a.vals), length(h.vals))))
for (i.a in 1:length(a.vals)) {
for(i.h in 1:length(h.vals)) {
ll.vals[i.a, i.h] <- frog.neg.ll(c(a.vals[i.a], h.vals[i.h]))
}
}
for (i.a in 1:length(a.vals)) {
for(i.h in 1:length(h.vals)) {
ll.vals[i.a, i.h] <- frog.neg.ll(c(a.vals[i.a], h.vals[i.h]))
}
}
contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100,
xlab = "a", ylab = "h")
points(x = a.mle, y = h.mle, col = "red")
(cutoff.ll <- frog.neg.ll(c(a.mle, h.mle)) +
(1 / 2) * qchisq(.95, df = 2))
contour(x = a.vals, y = h.vals, z = ll.vals,
levels = frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2),
add = TRUE, col = "red", lwd = 2)
profile.ll <- function(my.a) {
# Calculate the minimum log likelihood value for a given value of a, the attack rate
my.ll <- function(h) frog.neg.ll(c(my.a, h))
my.profile <- optimize(f = my.ll,
interval = c(0, 0.03),
maximum = FALSE)
return(my.profile$objective)
}
a.values <- seq(from = 0.3, to = 0.8, by = 0.01)
a.profile <- double(length = length(a.values))
for (i in 1:length(a.values)) {
a.profile[i] <- profile.ll(a.values[i])
}
plot(x = a.values, y = a.profile, xlab = "a", ylab = "profile log-likelihood", type = "l")
cut.off <- profile.ll(a.mle) + qchisq(0.95, df = 1) / 2
abline(h = cut.off, col = "blue", lty = "dashed")
(lower <- uniroot(f = function(x) cut.off - profile.ll(x) ,
interval = c(0.3, a.mle)))
(upper <- uniroot(f = function(x) cut.off - profile.ll(x) ,
interval = c(a.mle, 0.8)))
abline(v = c(lower$root, upper$root), col = "blue")
horse.D2 <- function(delta.l) {
(horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2
}
horse.D2(1e-04)
rm(list = ls())
horse <- read.table("../data/horse.txt", header = TRUE)
horse.neg.ll <- function(my.lambda) {
ll.vals <- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE)
-1 * sum(ll.vals)
}
# create a vector of lambda values using the 'seq'uence command
lambda.vals <- seq(from = 0.5, to = 1.0, by = 0.01)
# create an empty vector to store the values of the log-likelihood
ll.vals <- double(length = length(lambda.vals))
# use a loop to find the log-likelihood for each value in lambda.vals
for (i.lambda in 1:length(lambda.vals)) {
ll.vals[i.lambda] <- horse.neg.ll(lambda.vals[i.lambda])
}
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
horse.neg.ll(0.7)
qchisq(0.95, df = 1) / 2
(cutoff.ll <- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2)
abline(h = cutoff.ll, col = "red", lty = "dashed")
my.function <- function(my.lambda){
cutoff.ll - horse.neg.ll(my.lambda)
}
(lower <- uniroot(f = my.function, interval = c(0.6, 0.7)))
abline(v = lower$root, col = "red")
(upper <- uniroot(f = my.function, interval = c(0.7, 0.9)))
abline(v = upper$root, col = "red")
horse.D2 <- function(delta.l) {
(horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2
}
horse.D2(1e-04)
numDeriv::hessian(func = horse.neg.ll, x = 0.7)
(lambda.se <- sqrt(1 / horse.D2(1e-04)))
(lower.approx <- 0.7 - qnorm(.975) * lambda.se)
(upper.approx <- 0.7 + qnorm(.975) * lambda.se)
abline(v = c(lower.approx, upper.approx), col = "blue")
# clean up
rm(list = ls())
horse <- read.table("../data/horse.txt", header = TRUE)
horse.neg.ll <- function(my.lambda) {
ll.vals <- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE)
-1 * sum(ll.vals)
}
# create a vector of lambda values using the 'seq'uence command
lambda.vals <- seq(from = 0.5, to = 1.0, by = 0.01)
# create an empty vector to store the values of the log-likelihood
ll.vals <- double(length = length(lambda.vals))
# use a loop to find the log-likelihood for each value in lambda.vals
for (i.lambda in 1:length(lambda.vals)) {
ll.vals[i.lambda] <- horse.neg.ll(lambda.vals[i.lambda])
}
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
horse.neg.ll(0.7)
qchisq(0.95, df = 1) / 2
(cutoff.ll <- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2)
abline(h = cutoff.ll, col = "red", lty = "dashed")
my.function <- function(my.lambda){
cutoff.ll - horse.neg.ll(my.lambda)
}
(lower <- uniroot(f = my.function, interval = c(0.6, 0.7)))
abline(v = lower$root, col = "red")
(upper <- uniroot(f = my.function, interval = c(0.7, 0.9)))
abline(v = upper$root, col = "red")
horse.D2 <- function(delta.l) {
(horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2
}
horse.D2(1e-04)
horse.D2(1e-05)
numDeriv::hessian(func = horse.neg.ll, x = 0.7)
?curve
approx.ll <- function(lambda) horse.neg.ll(0.7) + (1/2) * (lambda - 0.7)^2 * horse.D2(1e-04)
approx.ll(0.7)
approx.ll(0.9)
curve(approx.ll, from = min(lambda.vals), to = max(lambda.vals), add = TRUE)
curve(approx.ll, from = min(lambda.vals), to = max(lambda.vals), add = TRUE, col = "blue")
(lambda.se <- sqrt(1 / horse.D2(1e-04)))
(lower.approx <- 0.7 - qnorm(.975) * lambda.se)
(upper.approx <- 0.7 + qnorm(.975) * lambda.se)
abline(v = c(lower.approx, upper.approx), col = "blue")
curve(horse.neg.ll, from = 0.5, to = 1,
xlab = expression(lambda),
ylab = "negative log likelihood")
horse.neg.ll
horse.neg.ll()
horse.neg.ll <- function(my.lambda) -1 * sum(dpois(x = horse$deaths, lambda = my.lambda, log = TRUE))
curve(horse.neg.ll, from = 0.5, to = 1,
xlab = expression(lambda),
ylab = "negative log likelihood")
setwd("C:/Users/krgross/Documents/GitHub/bma590-course-notes/")
horse <- read.table("data/horse.txt", header = TRUE)
horse.neg.ll <- function(my.lambda) {
ll.vals <- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE)
-1 * sum(ll.vals)
}
# create a vector of lambda values using the 'seq'uence command
lambda.vals <- seq(from = 0.5, to = 1.0, by = 0.01)
# create an empty vector to store the values of the log-likelihood
ll.vals <- double(length = length(lambda.vals))
# use a loop to find the log-likelihood for each value in lambda.vals
for (i.lambda in 1:length(lambda.vals)) {
ll.vals[i.lambda] <- horse.neg.ll(lambda.vals[i.lambda])
}
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
cutoff.ll <- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2
# recreate the plot and add a line
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
abline(h = cutoff.ll, col = "red", lty = "dashed")
# use uniroot to find the confidence bounds precisely
my.function <- function(my.lambda){
horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda)
}
(lower <- uniroot(f = my.function, interval = c(0.6, 0.7)))
(upper <- uniroot(f = my.function, interval = c(0.7, 0.9)))
(lower <- uniroot(f = function(x) horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(x) ,
interval = c(0.6, 0.7)))
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
abline(h = cutoff.ll, col = "red", lty = "dashed")
abline(v = c(lower$root, upper$root), col = "red")
# clean up the workspace
rm(list = ls())
library(emdbook)
data("ReedfrogFuncresp")
# rename something shorter
frog <- ReedfrogFuncresp
rm(ReedfrogFuncresp)
frog.neg.ll <- function(params){
a <- params[1]
h <- params[2]
prob.vals <- a / (1 + a * h * frog$Initial)
ll.vals <- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE)
-1 * sum(ll.vals)
}
(frog.mle <- optim(par = c(0.5, 1/60),
fn  = frog.neg.ll))
a.mle <- frog.mle$par[1]
h.mle <- frog.mle$par[2]
# plot negative likelihood contours
a.vals <- seq(from = 0.3, to = 0.75, by = 0.01)
h.vals <- seq(from = 0.001, to = 0.03, by = 0.001)
ll.vals <- matrix(nrow = length(a.vals), ncol = length(h.vals))
for (i.a in 1:length(a.vals)) {
for(i.h in 1:length(h.vals)) {
ll.vals[i.a, i.h] <- frog.neg.ll(c(a.vals[i.a], h.vals[i.h]))
}
}
contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100,
xlab = "a", ylab = "h")
points(x = a.mle, y = h.mle, col = "red")
cut.off <- frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2)
# recreate the plot and add a line for the 95% confidence region
contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100,
xlab = "a", ylab = "h")
points(x = a.mle, y = h.mle, col = "red")
contour(x = a.vals, y = h.vals, z = ll.vals,
levels = cut.off,
add = TRUE, col = "red", lwd = 2)
# profile log-likelihood function for the attack rate a
profile.ll <- function(my.a) {
# Calculate the minimum log likelihood value for a given value of a, the attack rate
my.ll <- function(h) frog.neg.ll(c(my.a, h))
my.profile <- optimize(f = my.ll, interval = c(0, 0.03), maximum = FALSE)
my.profile$objective
}
# plot the profile likelihood vs. a
# not necessary for finding the CI, but useful for understanding
a.values <- seq(from = 0.3, to = 0.8, by = 0.01)
a.profile <- double(length = length(a.values))
for (i in 1:length(a.values)) {
a.profile[i] <- profile.ll(a.values[i])
}
plot(x = a.values, y = a.profile, xlab = "a", ylab = "negative log-likelihood", type = "l")
# Now follow the same steps as before to find the profile 95% CI
cut.off <- profile.ll(a.mle) + qchisq(0.95, df = 1) / 2
(lower <- uniroot(f = function(x) cut.off - profile.ll(x) ,
interval = c(0.3, a.mle)))
(upper <- uniroot(f = function(x) cut.off - profile.ll(x) ,
interval = c(a.mle, 0.8)))
plot(x = a.values, y = a.profile, xlab = "a", ylab = "negative log-likelihood", type = "l")
abline(v = c(lower$root, upper$root), col = "blue")
abline(h = cut.off, col = "blue", lty = "dashed")
# clean up
rm(list = ls())
# read in the data
horse <- read.table("data/horse.txt", header = TRUE)
horse.neg.ll <- function(my.lambda) {
ll.vals <- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE)
-1 * sum(ll.vals)
}
# use uniroot to find the confidence bounds precisely
my.function <- function(my.lambda){
horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda)
}
lower <- uniroot(f = my.function, interval = c(0.6, 0.7))
upper <- uniroot(f = my.function, interval = c(0.7, 0.9))
## this function finds the second derivative at the MLE by finite differences
second.deriv <- function(delta.l) {
(horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2
}
(horse.D2 <- second.deriv(1e-04))
# see how the answer changes if we change delta
second.deriv(1e-05)
numDeriv::hessian(func = horse.neg.ll, x = 0.7)
(lambda.se <- sqrt(1 / horse.D2))
(lower.approx <- 0.7 - qnorm(.975) * lambda.se)
(upper.approx <- 0.7 + qnorm(.975) * lambda.se)
lower$root
upper$root
# create a vector of lambda values using the 'seq'uence command
lambda.vals <- seq(from = 0.5, to = 1.0, by = 0.01)
# create an empty vector to store the values of the log-likelihood
ll.vals <- double(length = length(lambda.vals))
# use a loop to find the log-likelihood for each value in lambda.vals
for (i.lambda in 1:length(lambda.vals)) {
ll.vals[i.lambda] <- horse.neg.ll(lambda.vals[i.lambda])
}
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
approx.ll <- function(lambda) horse.neg.ll(0.7) + (1/2) * (lambda - 0.7)^2 * horse.D2
curve(approx.ll, from = min(lambda.vals), to = max(lambda.vals), add = TRUE, col = "blue")
###################################
## Now find the confidence interval, and plot it
####################################
cutoff.ll <- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2
# add a line to the plot
abline(h = cutoff.ll, col = "red", lty = "dashed")
abline(v = c(lower$root, upper$root), col = "red")
abline(v = c(lower.approx, upper.approx), col = "blue")
legend(x = 0.65, y = 326,
leg = c("exact", "approximate"),
pch = 16,
col = c("red", "blue"),
bty = "n")
# clean up
rm(list = ls())
# create a vector of lambda values using the 'seq'uence command
lambda.vals <- seq(from = 0.5, to = 1.0, by = 0.01)
# create an empty vector to store the values of the log-likelihood
ll.vals <- double(length = length(lambda.vals))
# use a loop to find the log-likelihood for each value in lambda.vals
for (i.lambda in 1:length(lambda.vals)) {
ll.vals[i.lambda] <- horse.neg.ll(lambda.vals[i.lambda])
}
horse <- read.table("data/horse.txt", header = TRUE)
horse.neg.ll <- function(my.lambda) {
ll.vals <- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE)
-1 * sum(ll.vals)
}
# create a vector of lambda values using the 'seq'uence command
lambda.vals <- seq(from = 0.5, to = 1.0, by = 0.01)
# create an empty vector to store the values of the log-likelihood
ll.vals <- double(length = length(lambda.vals))
# use a loop to find the log-likelihood for each value in lambda.vals
for (i.lambda in 1:length(lambda.vals)) {
ll.vals[i.lambda] <- horse.neg.ll(lambda.vals[i.lambda])
}
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
cutoff.ll <- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2
# recreate the plot and add a line
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
abline(h = cutoff.ll, col = "red", lty = "dashed")
# use uniroot to find the confidence bounds precisely
my.function <- function(my.lambda){
horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda)
}
(lower <- uniroot(f = my.function, interval = c(0.6, 0.7)))
(upper <- uniroot(f = my.function, interval = c(0.7, 0.9)))
(lower <- uniroot(f = function(x) horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(x) ,
interval = c(0.6, 0.7)))
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l")
abline(h = cutoff.ll, col = "red", lty = "dashed")
abline(v = c(lower$root, upper$root), col = "red")
# clean up the workspace
rm(list = ls())
library(emdbook)
data("ReedfrogFuncresp")
# rename something shorter
frog <- ReedfrogFuncresp
rm(ReedfrogFuncresp)
frog.neg.ll <- function(params){
a <- params[1]
h <- params[2]
prob.vals <- a / (1 + a * h * frog$Initial)
ll.vals <- dbinom(frog$Killed, size = frog$Initial, prob = prob.vals, log = TRUE)
-1 * sum(ll.vals)
}
(frog.mle <- optim(par = c(0.5, 1/60),
fn  = frog.neg.ll))
a.mle <- frog.mle$par[1]
h.mle <- frog.mle$par[2]
# plot negative likelihood contours
a.vals <- seq(from = 0.3, to = 0.75, by = 0.01)
h.vals <- seq(from = 0.001, to = 0.03, by = 0.001)
ll.vals <- matrix(nrow = length(a.vals), ncol = length(h.vals))
for (i.a in 1:length(a.vals)) {
for(i.h in 1:length(h.vals)) {
ll.vals[i.a, i.h] <- frog.neg.ll(c(a.vals[i.a], h.vals[i.h]))
}
}
contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100,
xlab = "a", ylab = "h")
points(x = a.mle, y = h.mle, col = "red")
cut.off <- frog.neg.ll(c(a.mle, h.mle)) + (1 / 2) * qchisq(.95, df = 2)
# recreate the plot and add a line for the 95% confidence region
contour(x = a.vals, y = h.vals, z = ll.vals, nlevels = 100,
xlab = "a", ylab = "h")
points(x = a.mle, y = h.mle, col = "red")
contour(x = a.vals, y = h.vals, z = ll.vals,
levels = cut.off,
add = TRUE, col = "red", lwd = 2)
# profile log-likelihood function for the attack rate a
profile.ll <- function(my.a) {
# Calculate the minimum log likelihood value for a given value of a, the attack rate
my.ll <- function(h) frog.neg.ll(c(my.a, h))
my.profile <- optimize(f = my.ll, interval = c(0, 0.03), maximum = FALSE)
my.profile$objective
}
# plot the profile likelihood vs. a
# not necessary for finding the CI, but useful for understanding
a.values <- seq(from = 0.3, to = 0.8, by = 0.01)
a.profile <- double(length = length(a.values))
for (i in 1:length(a.values)) {
a.profile[i] <- profile.ll(a.values[i])
}
plot(x = a.values, y = a.profile, xlab = "a", ylab = "negative log-likelihood", type = "l")
# Now follow the same steps as before to find the profile 95% CI
cut.off <- profile.ll(a.mle) + qchisq(0.95, df = 1) / 2
(lower <- uniroot(f = function(x) cut.off - profile.ll(x) ,
interval = c(0.3, a.mle)))
(upper <- uniroot(f = function(x) cut.off - profile.ll(x) ,
interval = c(a.mle, 0.8)))
plot(x = a.values, y = a.profile, xlab = "a", ylab = "negative log-likelihood", type = "l")
abline(v = c(lower$root, upper$root), col = "blue")
abline(h = cut.off, col = "blue", lty = "dashed")
# clean up
rm(list = ls())
# read in the data
horse <- read.table("data/horse.txt", header = TRUE)
horse.neg.ll <- function(my.lambda) {
ll.vals <- dpois(x = horse$deaths, lambda = my.lambda, log = TRUE)
-1 * sum(ll.vals)
}
# use uniroot to find the confidence bounds precisely
my.function <- function(my.lambda){
horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2 - horse.neg.ll(my.lambda)
}
lower <- uniroot(f = my.function, interval = c(0.6, 0.7))
upper <- uniroot(f = my.function, interval = c(0.7, 0.9))
## this function finds the second derivative at the MLE by finite differences
second.deriv <- function(delta.l) {
(horse.neg.ll(0.7 + delta.l) - 2 * horse.neg.ll(0.7) + horse.neg.ll(0.7 - delta.l)) / delta.l ^ 2
}
(horse.D2 <- second.deriv(1e-04))
# see how the answer changes if we change delta
second.deriv(1e-05)
numDeriv::hessian(func = horse.neg.ll, x = 0.7)
(lambda.se <- sqrt(1 / horse.D2))
(lower.approx <- 0.7 - qnorm(.975) * lambda.se)
(upper.approx <- 0.7 + qnorm(.975) * lambda.se)
lower$root
upper$root
# create a vector of lambda values using the 'seq'uence command
lambda.vals <- seq(from = 0.5, to = 1.0, by = 0.01)
# create an empty vector to store the values of the log-likelihood
ll.vals <- double(length = length(lambda.vals))
# use a loop to find the log-likelihood for each value in lambda.vals
for (i.lambda in 1:length(lambda.vals)) {
ll.vals[i.lambda] <- horse.neg.ll(lambda.vals[i.lambda])
}
plot(ll.vals ~ lambda.vals, xlab = "lambda", ylab = "negative log likelihood", type = "l", col = "red")
approx.ll <- function(lambda) horse.neg.ll(0.7) + (1/2) * (lambda - 0.7)^2 * horse.D2
curve(approx.ll, from = min(lambda.vals), to = max(lambda.vals), add = TRUE, col = "blue")
###################################
## Now find the confidence interval, and plot it
####################################
cutoff.ll <- horse.neg.ll(0.7) + qchisq(0.95, df = 1) / 2
# add a line to the plot
abline(h = cutoff.ll, col = "red", lty = "dashed")
abline(v = c(lower$root, upper$root), col = "red")
abline(v = c(lower.approx, upper.approx), col = "blue")
legend(x = 0.65, y = 326,
leg = c("exact", "approximate"),
pch = 16,
col = c("red", "blue"),
bty = "n")
# clean up
rm(list = ls())
getwd()
bookdown::render_book("index.Rmd", bookdown::gitbook)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
